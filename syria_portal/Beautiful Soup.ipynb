{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_report(uri):\n",
    "    result = requests.get(uri)\n",
    "    \n",
    "    # Make sure the server returned a good status code\n",
    "    if result.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    content = result.content\n",
    "    soup = BeautifulSoup(content,\"lxml\")\n",
    "    \n",
    "    # Make sure we are not on a Page Not Found page\n",
    "    if soup.find(\"h1\").text == 'Page not found':\n",
    "        return None\n",
    "\n",
    "    # Establish the structure of the report\n",
    "    report = {\"title\":\"\",\n",
    "              \"description\":\"\", \n",
    "              \"about\":{},\n",
    "              \"link\":uri}\n",
    "\n",
    "    # Build the title of the resource\n",
    "    report_title = soup.find(\"h1\",\"restrict\").text\n",
    "\n",
    "    try:\n",
    "        doc_extra_title = soup.find(\"h2\",\"green\").text\n",
    "        report_title += \" - \" + doc_extra_title\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    report['title'] = report_title\n",
    "\n",
    "    # Get data for \"About\" this resource\n",
    "    resource_data = soup.find(\"table\",\"resourcedata\")\n",
    "    table_cells = resource_data.find_all(\"td\")\n",
    "    for idx in range(0,len(table_cells)):\n",
    "        if idx % 2 == 0:\n",
    "            sec = table_cells[idx].text.replace(\"\\xa0\",\" \").strip(\":\")\n",
    "            val = table_cells[idx+1].text.replace(\"\\xa0\",\" \")\n",
    "            report[\"about\"][sec] = val\n",
    "\n",
    "    # Add the description\n",
    "    resource_description = soup.find_all(\"p\")[0].text.replace(\"\\xa0\",\" \")\n",
    "    report[\"description\"] = resource_description\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Scraping search page results\n",
    "\n",
    "Everything added to the site after Jan of 2000. 142 pages of results.\n",
    "\n",
    "`http://www.syrialearning.org/resources.aspx?page=1&date=2010m1t2017m6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harvest the report numbers from the search result pages\n",
    "pages = 143\n",
    "articles = []\n",
    "\n",
    "for n in range(1,pages):\n",
    "    uri = \"http://www.syrialearning.org/resources.aspx?page=\"\n",
    "    uri += str(n)\n",
    "    uri += \"&date=2000m1t2017m6\"\n",
    "    \n",
    "    # Request the contents of the page\n",
    "    result = requests.get(uri)\n",
    "    \n",
    "    # Get the raw content of the page from the result request\n",
    "    content = result.content\n",
    "    \n",
    "    # Create a BeautifulSoup object to parse the contents\n",
    "    soup = BeautifulSoup(content,\"lxml\")\n",
    "    \n",
    "    # Look for all \"Header 4\" elements on the page - used for \n",
    "    # report titles in the search results\n",
    "    headers = soup.find_all(\"h4\")\n",
    "    \n",
    "    # Iterate the Header 4 elements and find those with links\n",
    "    # and then extract the links and append them to a list\n",
    "    for h in headers:\n",
    "        try:\n",
    "            num = h.find(\"a\")['href'].split(\"/\")[-1]\n",
    "            title = h.find(\"a\").text\n",
    "            articles.append((num,title))\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    # Wait a few seconds between each request in an attempt to not\n",
    "    # overload the Syria Learning server.\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('24760', 'Palliative Health Care in Jordan for Syrian Refugees')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(articles[0])\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrape the report pages to gather the data we need to insert into Discourse\n",
    "reports = []\n",
    "\n",
    "for article in articles:\n",
    "    uri_num = article[0]\n",
    "    uri = \"http://www.syrialearning.org/resource/\" + uri_num\n",
    "    \n",
    "    try:\n",
    "        rep = scrape_report(uri)\n",
    "        if rep:\n",
    "            reports.append(rep)\n",
    "    except Exception:\n",
    "        print(\"Failed\",uri)\n",
    "        \n",
    "    sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about': {'Agency': 'The Humanitarian Health Ethics Network',\n",
       "  'Author(s)': 'McDonald, M. ',\n",
       "  'Countries': 'Jordan, Syria',\n",
       "  'Date published': '24 May 2017',\n",
       "  'Keywords': 'Conflict, violence & peace, Health, Refugees/IDPs',\n",
       "  'Language': 'English',\n",
       "  'Pages': '35pp',\n",
       "  'Resource type': 'Research, reports and studies'},\n",
       " 'description': 'The Hashemite Kingdom of Jordan shares its northern border with the Syrian Arab Republic, and has been one of the main receiving countries of fleeing refugees since the beginning of the Syrian conflict in 2011.',\n",
       " 'link': 'http://www.syrialearning.org/resource/24760',\n",
       " 'title': 'Palliative Health Care in Jordan for Syrian Refugees'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///article_metadata.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_articles = db['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agency': 'Agency',\n",
       " 'authors': 'Author(s)',\n",
       " 'countries': 'Countries',\n",
       " 'date_published': 'Date published',\n",
       " 'keywords': 'Keywords',\n",
       " 'language': 'Language',\n",
       " 'pages': 'Pages',\n",
       " 'publisher': 'Publisher',\n",
       " 'resource_type': 'Resource type'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = set()\n",
    "for report in reports:\n",
    "    about_keys = report[\"about\"].keys()\n",
    "    for key in about_keys:\n",
    "        fields.add(key)\n",
    "        \n",
    "fields_lookup = {}\n",
    "\n",
    "for field in fields:\n",
    "    db_col = field.replace(\" \",\"_\").replace(\"(\",\"\").replace(\")\",\"\").lower()\n",
    "    fields_lookup[db_col] = field\n",
    "\n",
    "fields_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for report in reports:\n",
    "    try:\n",
    "        title = report[\"title\"]\n",
    "    except:\n",
    "        title = \"\"\n",
    "        \n",
    "    try:\n",
    "        link = report[\"link\"]\n",
    "    except:\n",
    "        link = \"\"\n",
    "        \n",
    "    try:\n",
    "        description = report[\"description\"]\n",
    "    except:\n",
    "        description = \"\"\n",
    "        \n",
    "    try:\n",
    "        about = json.dumps(report[\"about\"])\n",
    "    except:\n",
    "        about = \"\"\n",
    "        \n",
    "    record = {\n",
    "        \"title\":title,\n",
    "        \"link\":link,\n",
    "        \"description\":description,\n",
    "        \"about\":about\n",
    "    }\n",
    "    \n",
    "    for db_col in fields_lookup.keys():\n",
    "        try:\n",
    "            record[db_col] = report[\"about\"][fields_lookup[db_col]]\n",
    "        except:\n",
    "            record[db_col] = \"\"\n",
    "    \n",
    "    tab_articles.insert(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,10):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
