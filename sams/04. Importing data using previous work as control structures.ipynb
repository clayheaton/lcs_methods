{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Setup Note\n",
    "\n",
    "Prior to this notebook, the variables were normalized to provide a structure for importing data from all of the sheets. The consequence of that is that if the variable reduction mapping has improperly combined variables or the reduction is incomplete, then the template for this level will need to be changed to accommodate the new variable mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulate the file system\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Copy dictionaries\n",
    "import copy\n",
    "\n",
    "# work with dates\n",
    "import datetime\n",
    "import arrow\n",
    "\n",
    "# Convert stored string representation of a list to a list\n",
    "import ast\n",
    "\n",
    "# Recurse through a directory tree and return file names with glob\n",
    "import glob\n",
    "\n",
    "# Decode and re-encode mangled Arabic file names\n",
    "import codecs\n",
    "\n",
    "# Connect to a SQLite database in a lazy manner.\n",
    "import sqlalchemy\n",
    "import dataset\n",
    "\n",
    "# Enables opening and reading of Excel files\n",
    "import openpyxl\n",
    "\n",
    "# Translating variables, sheet names, and workbook names from Arabic\n",
    "# This is NOT free to use.\n",
    "from google.cloud import translate\n",
    "\n",
    "# Set the environment variable for the Google Service Account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\clay\\\\Documents\\\\fxb-lcs-2b24f4f8a73a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed template clone sams_data_phase04.sqlite\n",
      "Created database from template: sams_data_phase04.sqlite\n"
     ]
    }
   ],
   "source": [
    "#If there's an existing db for this sheet, delete it\n",
    "#so that we can copy from the template for a fresh start\n",
    "\n",
    "try:\n",
    "    os.remove(\"sams_data_phase04.sqlite\")\n",
    "    print(\"Removed template clone sams_data_phase04.sqlite\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Try to preserve a copy in case there is a problem and it has to be restored\n",
    "    shutil.copy2(\"sams_data_phase04_template.sqlite\",\"sams_data_phase04.sqlite\")\n",
    "    \n",
    "    print(\"Created database from template: sams_data_phase04.sqlite\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///sams_data_phase04.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first attempt to import the data will be into a single table. If it becomes necessary to break the data out into multiple tables, then that work will be done after evaluating the original import.\n",
    "\n",
    "To create the table without causing problems with the insert -- since the bulk insert inevitably is orders of magnitude faster, then we'll create the fields manually by iterating through all of the normalized names in the variables table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_all = db['full_raw_data']\n",
    "tab_vars = db['variables']\n",
    "tab_files_sheets = db['files_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of variables\n",
    "recs = db.query(\"SELECT DISTINCT(normalized) AS var FROM variables;\")\n",
    "variables = sorted([r['var'] for r in recs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a few common variables\n",
    "tab_all.create_column(\"a_file_id\",sqlalchemy.Integer)\n",
    "tab_all.create_column(\"a_files_sheets_id\",sqlalchemy.Integer)\n",
    "tab_all.create_column(\"a_sheet_id\",sqlalchemy.Integer)\n",
    "\n",
    "for v in variables:\n",
    "    tab_all.create_column(v,sqlalchemy.String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table is ready for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an in-memory lookup table for variables\n",
    "var_lookup = {}\n",
    "for r in tab_vars.find():\n",
    "    var_lookup[r['orig']] = r['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheets_to_process = db.query(\"\"\"\n",
    "SELECT files_sheets.id AS files_sheets_id, \n",
    "       files_sheets.file_id, \n",
    "       files_sheets.sheet_id, \n",
    "       files_sheets.header_start, \n",
    "       files_sheets.header_end,\n",
    "       files_sheets.header_values,\n",
    "       files.path AS file_path,\n",
    "       sheets.name AS sheet_name\n",
    "FROM files_sheets\n",
    "JOIN files ON files_sheets.file_id = files.id\n",
    "JOIN sheets ON files_sheets.sheet_id = sheets.id\n",
    "WHERE sheets.skip = 0\n",
    "\n",
    "AND files.ignore = 0\n",
    "AND files_sheets.header_start IS NOT NULL\n",
    "AND files_sheets.header_start <> 'PROBLEM'\n",
    "AND files_sheets.header_end NOT LIKE '%PROBLEM%'\n",
    "ORDER BY file_id, files_sheets_id;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Only reopen files when necessary\n",
    "working_file_id = -1\n",
    "active_file_path = None\n",
    "active_workbook = None\n",
    "\n",
    "\n",
    "for rec in sheets_to_process:\n",
    "    import_status = \"\"\n",
    "    \n",
    "    if rec['file_id'] > working_file_id:\n",
    "        working_file_id = rec['file_id']\n",
    "        active_file_path = rec['file_path']\n",
    "        try:\n",
    "            active_workbook = openpyxl.load_workbook(active_file_path,read_only=True,guess_types=False,data_only=True)\n",
    "        except:\n",
    "            import_status = \"Unable to open file\"\n",
    "            import_status = \"imported\"\n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])\n",
    "            print(\"Unable to open\",active_file_path)\n",
    "            \n",
    "            active_workbook = None\n",
    "            active_file_path = None\n",
    "            working_file_id = -1\n",
    "            continue\n",
    "            \n",
    "    # Process the active file\n",
    "    sheet_name = rec['sheet_name']\n",
    "    header_start = rec['header_start']\n",
    "    header_end = rec['header_end']\n",
    "    \n",
    "    # Unable to find a header in this sheet. Mark the record\n",
    "    if header_start is None or header_end is None or \"PROBLEM\" in header_start or \"PROBLEM\" in header_end:\n",
    "        import_status = \"skipped\"\n",
    "        continue\n",
    "    else:\n",
    "        worksheet = active_workbook.get_sheet_by_name(sheet_name)\n",
    "        last_row = worksheet.max_row\n",
    "        \n",
    "        # Sometimes worksheet.max_row doesn't return a value\n",
    "        if last_row == None:\n",
    "            import_status = \"imported: last row None\"\n",
    "            last_row = 10000\n",
    "        \n",
    "        data_start = header_start[0] + str(int(header_start[1:])+1)\n",
    "        data_end = header_end[0] + str(last_row)\n",
    "        data_range_string = data_start + \":\" + data_end\n",
    "        \n",
    "        # These are stored as a list converted to a string. Convert back to a list for enumeration\n",
    "        header_values = ast.literal_eval(rec['header_values'])\n",
    "        sheet_data = []\n",
    "        \n",
    "        try:\n",
    "            for datarow in worksheet[data_range_string]:\n",
    "                record = {}\n",
    "                for idx,cell in enumerate(datarow):\n",
    "                    header_val = header_values[idx]\n",
    "\n",
    "                    # Get the normalized value\n",
    "                    header_val = var_lookup[header_val]\n",
    "\n",
    "                    cell_value = cell.value                \n",
    "\n",
    "                    try:\n",
    "                        cell_value = cell_value.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Cannot write datetime objects to the database\n",
    "                    # Unless they are first converted to strings\n",
    "\n",
    "                    if isinstance(cell_value,datetime.datetime):\n",
    "                        cell_value = openpyxl.utils.datetime.datetime_to_W3CDTF(cell_value)\n",
    "                    elif isinstance(cell_value,datetime.time):\n",
    "                        cell_value = str(cell_value)\n",
    "                    elif cell_value is not None:\n",
    "                        cell_value = str(cell_value)\n",
    "\n",
    "                    # There's already a value in the field and it should be a string.\n",
    "                    # If it is our representation of none, replace it\n",
    "                    if header_val in record.keys():\n",
    "                        if record[header_val] == '.' or record[header_val] is None:\n",
    "                            record[header_val] = cell_value\n",
    "                        elif cell_value is not None:\n",
    "                            record[header_val] = record[header_val] + \", \" + cell_value\n",
    "                    else:\n",
    "\n",
    "                        # Blank strings instead of NULL will help us know which fields were available for the record\n",
    "                        if cell_value is None:\n",
    "                            record[header_val] = '.'\n",
    "                        else:\n",
    "                            record[header_val] = cell_value\n",
    "\n",
    "                sheet_data.append(record)\n",
    "\n",
    "            # Remove from sheet_data blank rows\n",
    "            rich_sheet_data = []\n",
    "            for staged in sheet_data:\n",
    "                working_copy = copy.deepcopy(staged)\n",
    "                try:\n",
    "                    # Put \"passover columns\" here. They will be removed from the record\n",
    "                    # before it is evaluated as \"empty.\" Number is a good example because there are sheets\n",
    "                    # where somebody dragged numbers down a column in preparation for a lot of data but \n",
    "                    # never actually used all of the numbered rows\n",
    "                    del working_copy['number']\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                if all((x == None or x == '.' or x == '') for x in list(working_copy.values())):\n",
    "                    continue\n",
    "                else:\n",
    "                    staged['a_file_id'] = rec['file_id']\n",
    "                    staged['a_files_sheets_id'] = rec['files_sheets_id']\n",
    "                    staged['a_sheet_id'] = rec['sheet_id']\n",
    "\n",
    "                    rich_sheet_data.append(staged)\n",
    "\n",
    "            # Try to perform a bulk insert\n",
    "            tab_all.insert_many(rich_sheet_data)\n",
    "            \n",
    "            # Update the status of the worksheet\n",
    "            import_status = \"imported\"\n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])\n",
    "            \n",
    "        except:\n",
    "            print(\"\\n--------------------------------------------------------------------------\")\n",
    "            print(\"Failure\")\n",
    "            print(\"file_id\",rec['file_id'],\"files_sheets_id\",rec['files_sheets_id'],\"sheet_id\",rec['sheet_id'])\n",
    "            print(active_file_path,sheet_name)\n",
    "            print(\"Header range:\",header_start,header_end)\n",
    "            print(\"Data range:\",data_start,data_end)\n",
    "            print(\"data_range_string\",data_range_string)\n",
    "            \n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At this point, the first ETL pass is complete. Save the database out as a template and move to notebook 5 for additional processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sams_data_phase05_template.sqlite'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not rerun this cell!\n",
    "# shutil.copy2('sams_data_phase04.sqlite','sams_data_phase05_template.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
