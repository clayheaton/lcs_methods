{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulate the file system\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import arrow\n",
    "\n",
    "# Convert stored string representation of a list to a list\n",
    "import ast\n",
    "\n",
    "# Recurse through a directory tree and return file names with glob\n",
    "import glob\n",
    "\n",
    "# Decode and re-encode mangled Arabic file names\n",
    "import codecs\n",
    "\n",
    "# Connect to a SQLite database in a lazy manner.\n",
    "import dataset\n",
    "\n",
    "# Enables opening and reading of Excel files\n",
    "import openpyxl\n",
    "\n",
    "# Translating variables, sheet names, and workbook names from Arabic\n",
    "# This is NOT free to use.\n",
    "from google.cloud import translate\n",
    "\n",
    "# Set the environment variable for the Google Service Account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\clay\\\\Documents\\\\fxb-lcs-2b24f4f8a73a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If there's an existing db for this sheet, delete it\n",
    "# so that we can copy from the template for a fresh start\n",
    "\n",
    "# try:\n",
    "#     os.remove(\"sams_data_phase03.sqlite\")\n",
    "#     print(\"Removed template clone sams_data_phase03.sqlite\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# try:\n",
    "#     # Try to preserve a copy in case there is a problem and it has to be restored\n",
    "#     shutil.copy2(\"sams_data_phase03_template.sqlite\",\"sams_data_phase03.sqlite\")\n",
    "    \n",
    "#     print(\"Created database from template: sams_data_phase03.sqlite\")\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///sams_data_phase03.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database created from the phase03 template contains a lot of manual changes to fix incorrect header references across a variety of files. The first task with this copy of the database continues the manual work, using SQLite Manager in Firefox. \n",
    "\n",
    "----\n",
    "\n",
    "The procedure here is to fix header references or mark files as `ignore` in the `files` table. Delete all of the variables, recreate by iterating through the files, then check again for problems.\n",
    "\n",
    "The code to delete and recheck is all in the following cell. Note that it may take a while to run, but you should be able to run it as many times as possible as the database it updates to fix header references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files = db['files']\n",
    "tab_files_sheets = db['files_sheets']\n",
    "tab_vars = db['variables']\n",
    "tab_files_sheets_vars = db['files_sheets_vars']\n",
    "tab_files_vars = db['files_variables']\n",
    "tab_sheets_vars = db['sheets_variables']\n",
    "\n",
    "tab_vars.drop()\n",
    "tab_files_sheets_vars.drop()\n",
    "tab_files_vars.drop()\n",
    "tab_sheets_vars.drop()\n",
    "\n",
    "recs_to_process = db.query(\"\"\"\n",
    "SELECT files_sheets.id AS files_sheets_id, \n",
    "       files_sheets.file_id, \n",
    "       files_sheets.sheet_id, \n",
    "       files_sheets.header_start, \n",
    "       files_sheets.header_end,\n",
    "       files.path AS file_path,\n",
    "       sheets.name AS sheet_name\n",
    "FROM files_sheets\n",
    "JOIN files ON files_sheets.file_id = files.id\n",
    "JOIN sheets ON files_sheets.sheet_id = sheets.id\n",
    "WHERE sheets.skip = 0\n",
    "AND files.ignore = 0\n",
    "ORDER BY file_id, sheet_id;\n",
    "\"\"\")\n",
    "\n",
    "letter_lookup = ['A','B','C','D','E','F','G','H','I','J','K',\n",
    "                 'L','M','N','O','P','Q','R','S','T','U','V',\n",
    "                 'W','X','Y','Z']\n",
    "\n",
    "variables = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_file_id = -1\n",
    "active_file_path = None\n",
    "active_workbook = None\n",
    "\n",
    "for rec in recs_to_process:\n",
    "    \n",
    "    # This only fires with a new file_id\n",
    "    if rec['file_id'] > working_file_id:\n",
    "        working_file_id = rec['file_id']\n",
    "        active_file_path = rec['file_path']\n",
    "        try:\n",
    "            active_workbook = openpyxl.load_workbook(active_file_path,read_only=True,guess_types=False,data_only=True)\n",
    "        except:\n",
    "            print(\"Unable to open\",active_file_path)\n",
    "            active_workbook = None\n",
    "            active_file_path = None\n",
    "            working_file_id = -1\n",
    "            continue\n",
    "            \n",
    "    # Process the active file\n",
    "    sheet_name = rec['sheet_name']\n",
    "    # header_range, header_data = headers_from_worksheet(active_workbook,sheet_name)\n",
    "    header_start = rec['header_start']\n",
    "    header_end = rec['header_end']\n",
    "    \n",
    "    # Unable to find a header in this sheet. Mark the record\n",
    "    if header_start is None or header_end is None or \"PROBLEM\" in header_start or \"PROBLEM\" in header_end:\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        worksheet = active_workbook.get_sheet_by_name(sheet_name)\n",
    "\n",
    "        header_cells = worksheet[header_start:header_end]\n",
    "        header_data = [c.value for c in header_cells[0]]\n",
    "        \n",
    "        fixed_header_data = []\n",
    "        for value in header_data:\n",
    "            if isinstance(value,datetime.datetime):\n",
    "                fixed_value = arrow.get(value).format(\"YYYY-MM-DD\")\n",
    "                fixed_header_data.append(fixed_value)\n",
    "                variables.add(fixed_value)\n",
    "            else:\n",
    "                fixed_header_data.append(value)\n",
    "                variables.add(value)\n",
    "        \n",
    "        update_rec = {\n",
    "            \"id\":rec['files_sheets_id'],\n",
    "            \"header_start\":header_start,\n",
    "            \"header_end\":header_end,\n",
    "            \"header_values\":str(fixed_header_data)\n",
    "        }\n",
    "        \n",
    "        tab_files_sheets.update(update_rec,['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "translate_client = translate.Client()\n",
    "target_lang = 'en'\n",
    "\n",
    "try:\n",
    "    tab_vars.drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tab_vars = db['variables']\n",
    "\n",
    "for v in variables:\n",
    "    try:\n",
    "        v_str = str(v)\n",
    "        clean = v_str.replace(\"\\n\",\" \").replace(\"\\\\\",\"\").replace(\"\\t\",\" \").strip()\n",
    "    except:\n",
    "        print(\"Could not process\",v)\n",
    "        continue\n",
    "        \n",
    "    translation = translate_client.translate(v_str,target_language=target_lang)\n",
    "        \n",
    "    rec = {\n",
    "        \"orig\":v,\n",
    "        \"clean\":clean,\n",
    "        \"translation\":translation['translatedText'],\n",
    "        \"normalized\":\"\"\n",
    "    }\n",
    "    try:\n",
    "        tab_vars.insert(rec)\n",
    "    except:\n",
    "        print(\"\\nFailure to insert\")\n",
    "        print(rec)\n",
    "        \n",
    "try:\n",
    "    tab_files_vars.drop()\n",
    "    tab_sheets_vars.drop()\n",
    "    tab_files_sheets_vars.drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tab_files_vars = db['files_variables']\n",
    "tab_sheets_vars = db['sheets_variables']\n",
    "tab_files_sheets_vars = db['files_sheets_vars']\n",
    "\n",
    "var_lookup = {}\n",
    "for rec in tab_vars.find():\n",
    "    var_lookup[str(rec['orig'])] = rec['id'] # This might cause a problem casting to str\n",
    "    \n",
    "files_vars_set = set()\n",
    "sheets_vars_set = set()\n",
    "files_sheets_vars_set = set()\n",
    "\n",
    "\n",
    "# Below need to check if file is being skipped before processing header\n",
    "\n",
    "for rec in tab_files_sheets.find():\n",
    "    header_start = rec['header_start']\n",
    "    header_end = rec['header_end']\n",
    "    \n",
    "    file_id = rec['file_id']\n",
    "    \n",
    "    file_rec = tab_files.find_one(id=file_id)\n",
    "    if file_rec['skipped'] is True or file_rec['ignore'] is True:\n",
    "        continue\n",
    "    \n",
    "    if header_start is None or header_end is None:\n",
    "        continue\n",
    "    \n",
    "    if '(PROBLEM)' in header_start or '(PROBLEM)' in header_end:\n",
    "        continue\n",
    "    \n",
    "    header_values = rec['header_values']\n",
    "    if header_values is None:\n",
    "        continue\n",
    "    \n",
    "    header_values = ast.literal_eval(rec['header_values'])\n",
    "    \n",
    "    for header in header_values:\n",
    "        if header is None:\n",
    "            continue\n",
    "            \n",
    "        file_id = rec['file_id']\n",
    "        sheet_id = rec['sheet_id']\n",
    "        files_sheets_id = rec['id']\n",
    "        \n",
    "        try:\n",
    "            lu = str(header)\n",
    "            var_id = var_lookup[lu]\n",
    "        except:\n",
    "            try:\n",
    "                var_id = var_lookup[header]\n",
    "            except:\n",
    "                print(\"Unable to find\",header)\n",
    "                continue\n",
    "        \n",
    "        files_vars_set.add((file_id,var_id))\n",
    "        files_sheets_vars_set.add((files_sheets_id,var_id))\n",
    "        sheets_vars_set.add((sheet_id,var_id))\n",
    "        \n",
    "tab_files_vars_recs = []\n",
    "\n",
    "for rec_tuple in files_vars_set:\n",
    "    rec = {\"file_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_files_vars_recs.append(rec)\n",
    "    \n",
    "tab_files_vars.insert_many(tab_files_vars_recs)\n",
    "\n",
    "tab_sheets_vars_recs = []\n",
    "\n",
    "for rec_tuple in sheets_vars_set:\n",
    "    rec = {\"sheet_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_sheets_vars_recs.append(rec)\n",
    "    \n",
    "tab_sheets_vars.insert_many(tab_sheets_vars_recs)\n",
    "\n",
    "tab_files_sheets_vars_recs = []\n",
    "\n",
    "for rec_tuple in files_sheets_vars_set:\n",
    "    rec = {\"files_sheets_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_files_sheets_vars_recs.append(rec)\n",
    "    \n",
    "tab_files_sheets_vars.insert_many(tab_files_sheets_vars_recs)\n",
    "\n",
    "# There are sheets that have header references but are blank.\n",
    "# blank them out in the database.\n",
    "recs = tab_files_sheets.find()\n",
    "\n",
    "update_recs = []\n",
    "\n",
    "for rec in recs:\n",
    "    if rec['header_values'] == '[]':\n",
    "        ur = {\n",
    "            \"id\":rec['id'],\n",
    "            \"header_start\":None,\n",
    "            \"header_end\":None,\n",
    "            \"header_values\":None\n",
    "        }\n",
    "        update_recs.append(ur)\n",
    "        \n",
    "for rec in update_recs:\n",
    "    tab_files_sheets.update(rec,['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Clear `header_values` from `files_sheets` before running the block above.\n",
    "\n",
    "\n",
    "AT THIS POINT... I'm ready to do manual variable consolidation. That's the final step prior to exporting the data from sheets into the database, into the consolidated variables. There are currently 355 variables identified across the files. Need to consolidate them into as few as possible. Note that it's important to consider whether they all are \"patient log\" variables or how many of them are \"pharmacy log\" variables, etc. If there is a variable that has presence in multiple tables, should it be prefixed with `pl_` for patient log or something like that? How to handle the case when two variables are consolidated into the same field but appear in the same record? Probably best to just comma separate the values and put them in the same column. It will be important to track where those appear because sometimes they likely will need to be broken out into multiple columns.\n",
    "\n",
    "Now that the manual work is done to try to give sane names to the variables, it's time to save the database out as a template for the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sams_data_phase04_template.sqlite'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not rerun this cell!\n",
    "# shutil.copy2('sams_data_phase03.sqlite','sams_data_phase04_template.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
