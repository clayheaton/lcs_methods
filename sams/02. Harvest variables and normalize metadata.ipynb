{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional metadata support\n",
    "\n",
    "This notebook takes the output from notebook 1. It iterates through all of the worksheets in all of the Excel files and attempts to pull out all of the unique variable names while storing data about the location of the header rows and data in the database. The unique variables are stores in a table and a join table links them to the unique sheet types. \n",
    "\n",
    "The idea is that eventually, each unique sheet type will have a table and the data from the tables that matches those types will be stored in them, regardless of the source. \n",
    "\n",
    "After the variables are stored, a manual pass is made through the data to create the normalized column names that will appear in the data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulate the file system\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import arrow\n",
    "\n",
    "# Convert stored string representation of a list to a list\n",
    "import ast\n",
    "\n",
    "# Recurse through a directory tree and return file names with glob\n",
    "import glob\n",
    "\n",
    "# Decode and re-encode mangled Arabic file names\n",
    "import codecs\n",
    "\n",
    "# Connect to a SQLite database in a lazy manner.\n",
    "import dataset\n",
    "\n",
    "# Enables opening and reading of Excel files\n",
    "import openpyxl\n",
    "\n",
    "# Translating variables, sheet names, and workbook names from Arabic\n",
    "# This is NOT free to use.\n",
    "from google.cloud import translate\n",
    "\n",
    "# Set the environment variable for the Google Service Account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\clay\\\\Documents\\\\fxb-lcs-2b24f4f8a73a.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sheet names were manually normalized in the database template prior to this step. The template normalization preserves the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed template clone sams_data_phase02.sqlite\n",
      "Created database from template: sams_data_phase02.sqlite\n"
     ]
    }
   ],
   "source": [
    "# If there's an existing db for this sheet, delete it\n",
    "# so that we can copy from the template for a fresh start\n",
    "\n",
    "try:\n",
    "    os.remove(\"sams_data_phase02.sqlite\")\n",
    "    print(\"Removed template clone sams_data_phase02.sqlite\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Try to preserve a copy in case there is a problem and it has to be restored\n",
    "    shutil.copy2(\"sams_data_phase02_template.sqlite\",\"sams_data_phase02.sqlite\")\n",
    "    \n",
    "    print(\"Created database from template: sams_data_phase02.sqlite\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior notebook ends with copying the database to a template file that will again be copied to create the active file for this notebook. That means that you can run all of the cells in this notebook without destroying the database that is required to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///sams_data_phase02.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run a query that pulls out a file path and sheet name for every sheet that we are going to try to parse variables from (that are not marked skip in the sheets table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recs_to_process = db.query(\"\"\"\n",
    "SELECT files_sheets.id AS files_sheets_id, files_sheets.file_id, files_sheets.sheet_id, files.path AS file_path,sheets.name AS sheet_name\n",
    "FROM files_sheets\n",
    "JOIN files ON files_sheets.file_id = files.id\n",
    "JOIN sheets ON files_sheets.sheet_id = sheets.id\n",
    "WHERE sheets.skip = 0\n",
    "ORDER BY file_id, sheet_id;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will attempt to extract the table headers from the sheets in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def headers_from_worksheet(workbook,worksheet_name):\n",
    "    worksheet = workbook.get_sheet_by_name(worksheet_name)\n",
    "    winning_row_values = 0\n",
    "    winning_row_number = None\n",
    "    \n",
    "    for row in range(1,21):\n",
    "        start_range = 'A' + str(row)\n",
    "        end_range = 'Z' + str(row)\n",
    "        cells = worksheet[start_range:end_range]\n",
    "        try:\n",
    "            values = [c.value for c in cells[0]]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        filled_cells = 0\n",
    "        for v in values:\n",
    "            if v is not None:\n",
    "                filled_cells += 1\n",
    "        \n",
    "        if filled_cells > winning_row_values:\n",
    "            winning_row_values = filled_cells\n",
    "            winning_row_number = row\n",
    "            \n",
    "    # Some sheets may be blank\n",
    "    if winning_row_number is None:\n",
    "        return None,None\n",
    "            \n",
    "    winning_start = 'A' + str(winning_row_number)\n",
    "    winning_end = 'Z' + str(winning_row_number)\n",
    "\n",
    "    header_cells = worksheet[winning_start:winning_end]\n",
    "    header_data = [c.value for c in header_cells[0]]\n",
    "    \n",
    "    # If we detect a datetime.datetime.object, then we probably\n",
    "    # want the previous row. Might be a better way to check this\n",
    "    # TODO: This is a bad idea?\n",
    "    \n",
    "    # What's the actual start column of the header?\n",
    "    start_idx = 0\n",
    "    determined_start = False\n",
    "    \n",
    "    for val in header_data:\n",
    "        if not determined_start and val is None:\n",
    "            start_idx += 1\n",
    "        elif val is not None:\n",
    "            determined_start = True\n",
    "            \n",
    "        if isinstance(val,datetime.datetime):\n",
    "            winning_row_number -= 1\n",
    "            \n",
    "    header_start_letter = letter_lookup[start_idx]\n",
    "            \n",
    "    winning_start = header_start_letter + str(winning_row_number)\n",
    "    winning_end = 'Z' + str(winning_row_number)\n",
    "    try:\n",
    "        header_cells = worksheet[winning_start:winning_end]\n",
    "        header_data = [c.value for c in header_cells[0]]\n",
    "    except:\n",
    "        return None,None\n",
    "    \n",
    "    end_idx = len(header_data) - 1\n",
    "    problem = ''\n",
    "#     print(end_idx)\n",
    "#     print(header_data)\n",
    "#     print(header_data[end_idx],\"\\n\")\n",
    "    \n",
    "    while header_data[end_idx] is None:\n",
    "        end_idx -= 1\n",
    "        if end_idx <= start_idx:\n",
    "            problem = ' (PROBLEM)'\n",
    "            break\n",
    "            \n",
    "    # Lookup assumes that the header starts with col A, so offset the lookup on the\n",
    "    # end letter by the start letter index and it will assign the proper letter to the\n",
    "    # end letter.\n",
    "    end_letter = letter_lookup[end_idx+start_idx]  \n",
    "    header_end = end_letter + winning_start[1:] + problem\n",
    "    header_range = (winning_start,header_end)\n",
    "    \n",
    "    # Prune the header_data to get rid of trailing None values\n",
    "    prune_by = 0\n",
    "    \n",
    "    while header_data[prune_by-1] is None:\n",
    "        prune_by -= 1\n",
    "        \n",
    "    try:\n",
    "        header_data = header_data[:prune_by]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return header_range, header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_sheets = db['files_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letter_lookup = ['A','B','C','D','E','F','G','H','I','J','K',\n",
    "                 'L','M','N','O','P','Q','R','S','T','U','V',\n",
    "                 'W','X','Y','Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907 Problem workbook data\\jordan\\2016\\April\\π∩ƒºƒó δφε ¼∞⌐ 4-2016.xlsx --> sheet --> وصفات شهرية\n",
      "1677 Problem workbook data\\jordan\\2016\\Feb\\½ñΘ Ω⌐Φ¬ π∩ƒºƒó ñƒ½Ω Θ¼∞⌐ 2-2016.xlsx --> sheet --> الفيزيائي\n",
      "5390 Problem workbook data\\turkey\\2015\\Feb\\Jisr Al shgur\\Dental and Dermatological\\Patient log 1.xlsx --> sheet --> ك\n"
     ]
    }
   ],
   "source": [
    "variables = set()\n",
    "\n",
    "working_file_id = -1\n",
    "active_file_path = None\n",
    "active_workbook = None\n",
    "\n",
    "for rec in recs_to_process:\n",
    "    \n",
    "    # This only fires with a new file_id\n",
    "    if rec['file_id'] > working_file_id:\n",
    "        working_file_id = rec['file_id']\n",
    "        active_file_path = rec['file_path']\n",
    "        try:\n",
    "            active_workbook = openpyxl.load_workbook(active_file_path,read_only=True,guess_types=False,data_only=True)\n",
    "        except:\n",
    "            print(\"Unable to open\",active_file_path)\n",
    "            active_workbook = None\n",
    "            active_file_path = None\n",
    "            working_file_id = -1\n",
    "            \n",
    "    # Process the active file\n",
    "    sheet_name = rec['sheet_name']\n",
    "    header_range, header_data = headers_from_worksheet(active_workbook,sheet_name)\n",
    "    \n",
    "    # Unable to find a header in this sheet. Mark the record\n",
    "    if header_range is None:\n",
    "        update_rec = {\"id\":rec['files_sheets_id'],\"header_start\":\"PROBLEM\"}\n",
    "        tab_files_sheets.update(update_rec,['id'])\n",
    "        print(rec['files_sheets_id'],\"Problem workbook\",active_file_path,\"--> sheet -->\",sheet_name)\n",
    "        continue\n",
    "    else:\n",
    "        header_start = header_range[0]\n",
    "        header_end = header_range[1]\n",
    "        \n",
    "        fixed_header_data = []\n",
    "        for value in header_data:\n",
    "            if isinstance(value,datetime.datetime):\n",
    "                fixed_value = arrow.get(value).format(\"YYYY-MM-DD\")\n",
    "                fixed_header_data.append(fixed_value)\n",
    "                variables.add(fixed_value)\n",
    "            else:\n",
    "                fixed_header_data.append(value)\n",
    "                variables.add(value)\n",
    "        \n",
    "        update_rec = {\n",
    "            \"id\":rec['files_sheets_id'],\n",
    "            \"header_start\":header_start,\n",
    "            \"header_end\":header_end,\n",
    "            \"header_values\":str(fixed_header_data)\n",
    "        }\n",
    "        \n",
    "        tab_files_sheets.update(update_rec,['id'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table to hold the unique variables collected and translate them with the Google Translate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate_client = translate.Client()\n",
    "target_lang = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tab_vars.drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tab_vars = db['variables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in variables:\n",
    "    try:\n",
    "        v_str = str(v)\n",
    "        clean = v_str.replace(\"\\n\",\" \").replace(\"\\\\\",\"\").replace(\"\\t\",\" \").strip()\n",
    "    except:\n",
    "        print(\"Could not process\",v)\n",
    "        continue\n",
    "        \n",
    "    translation = translate_client.translate(v_str,target_language=target_lang)\n",
    "        \n",
    "    rec = {\n",
    "        \"orig\":v,\n",
    "        \"clean\":clean,\n",
    "        \"translation\":translation['translatedText'],\n",
    "        \"normalized\":\"\"\n",
    "    }\n",
    "    try:\n",
    "        tab_vars.insert(rec)\n",
    "    except:\n",
    "        print(\"\\nFailure to insert\")\n",
    "        print(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIRST PASS AT VARIABLES IS IN THE DB\n",
    "# - Find names and figure out which sheets are importing wrong.\n",
    "# - Manually set the proper header ranges?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the files_sheets, and create reference tables that connect the sheets table and the files_sheets table to the variables. This is mainly for diagnostic purposes and for debugging which sheets and files have header reference problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tab_files_vars.drop()\n",
    "    tab_sheets_vars.drop()\n",
    "    tab_files_sheets_vars.drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tab_files_vars = db['files_variables']\n",
    "tab_sheets_vars = db['sheets_variables']\n",
    "tab_files_sheets_vars = db['files_sheets_vars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold a variable reference in memory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_lookup = {}\n",
    "for rec in tab_vars.find():\n",
    "    var_lookup[rec['orig']] = rec['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_vars_set = set()\n",
    "sheets_vars_set = set()\n",
    "files_sheets_vars_set = set()\n",
    "\n",
    "for rec in tab_files_sheets.find():\n",
    "    header_values = rec['header_values']\n",
    "    if header_values is None:\n",
    "        continue\n",
    "    \n",
    "    header_values = ast.literal_eval(rec['header_values'])\n",
    "    \n",
    "    for header in header_values:\n",
    "        if header is None:\n",
    "            continue\n",
    "        try:\n",
    "            var_id = var_lookup[str(header)]\n",
    "            file_id = rec['file_id']\n",
    "            sheet_id = rec['sheet_id']\n",
    "            files_sheets_id = rec['id']\n",
    "        except:\n",
    "            print(\"problem with\",header)\n",
    "            continue\n",
    "        \n",
    "        files_vars_set.add((file_id,var_id))\n",
    "        files_sheets_vars_set.add((files_sheets_id,var_id))\n",
    "        sheets_vars_set.add((sheet_id,var_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the join tables that we'll use for research into problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_vars_recs = []\n",
    "\n",
    "for rec_tuple in files_vars_set:\n",
    "    rec = {\"file_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_files_vars_recs.append(rec)\n",
    "    \n",
    "tab_files_vars.insert_many(tab_files_vars_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_sheets_vars_recs = []\n",
    "\n",
    "for rec_tuple in sheets_vars_set:\n",
    "    rec = {\"sheet_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_sheets_vars_recs.append(rec)\n",
    "    \n",
    "tab_sheets_vars.insert_many(tab_sheets_vars_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_sheets_vars_recs = []\n",
    "\n",
    "for rec_tuple in files_sheets_vars_set:\n",
    "    rec = {\"files_sheets_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_files_sheets_vars_recs.append(rec)\n",
    "    \n",
    "tab_files_sheets_vars.insert_many(tab_files_sheets_vars_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we've tried to guess what the header row is for each of the sheets in each of the Excel files. There are errors, however, and they have to be fixed manually.\n",
    "\n",
    "The process is to identify the most problematic header indexing errors through querying unusual variable names and tracing them back to the `files_sheets` entry that corresponds. The process of identifying the problems can be handled in two ways: the first is that some of the problems are systematic and the header extraction code can be updated to pull those schema in a more appropriate manner and the second is manual, where for any number of reasons, we either have columns that do not belong, columns that are not named properly or have a generic name, or sheets that were unparseable. \n",
    "\n",
    "Some of the unparseable sheets are simply empty and should be skipped, some of them contain aggregate data, which we currently are not importing, some come from corrupt files, and some are a problem with the header extraction algorithm.\n",
    "\n",
    "This step seeks to manually repair the largest header range issues in the `files_sheets` page and the to re-extract the header values from those sheets using the references instead of the generic algorithm. That will trigger another variable review. Iterate this process until the error is tolerable and then this step of the process is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are sheets that have header references but are blank.\n",
    "# blank them out in the database.\n",
    "recs = tab_files_sheets.find()\n",
    "\n",
    "update_recs = []\n",
    "\n",
    "for rec in recs:\n",
    "    if rec['header_values'] == '[]':\n",
    "        ur = {\n",
    "            \"id\":rec['id'],\n",
    "            \"header_start\":None,\n",
    "            \"header_end\":None,\n",
    "            \"header_values\":None\n",
    "        }\n",
    "        update_recs.append(ur)\n",
    "        \n",
    "for rec in update_recs:\n",
    "    tab_files_sheets.update(rec,['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the manual effort that has gone into cleaning files_sheets, this notebook will conclude by saving out the current database as the template for the next database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sams_data_phase03_template.sqlite'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not rerun this cell!\n",
    "shutil.copy2('sams_data_phase02.sqlite','sams_data_phase03_template.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
