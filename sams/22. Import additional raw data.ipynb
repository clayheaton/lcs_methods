{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulate the file system\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import arrow\n",
    "\n",
    "# Copy dictionaries\n",
    "import copy\n",
    "\n",
    "# Convert stored string representation of a list to a list\n",
    "import ast\n",
    "\n",
    "# Recurse through a directory tree and return file names with glob\n",
    "import glob\n",
    "\n",
    "# Decode and re-encode mangled Arabic file names\n",
    "import codecs\n",
    "\n",
    "# Connect to a SQLite database in a lazy manner.\n",
    "import dataset\n",
    "import sqlalchemy\n",
    "\n",
    "# Enables opening and reading of Excel files\n",
    "import openpyxl\n",
    "\n",
    "# Translating variables, sheet names, and workbook names from Arabic\n",
    "# This is NOT free to use.\n",
    "from google.cloud import translate\n",
    "\n",
    "# Set the environment variable for the Google Service Account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\clay\\\\Documents\\\\fxb-lcs-2b24f4f8a73a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed template clone sams_data_phase22.sqlite\n",
      "Created database from template: sams_data_phase22.sqlite\n"
     ]
    }
   ],
   "source": [
    "#If there's an existing db for this sheet, delete it\n",
    "#so that we can copy from the template for a fresh start\n",
    "\n",
    "try:\n",
    "    os.remove(\"sams_data_phase22.sqlite\")\n",
    "    print(\"Removed template clone sams_data_phase22.sqlite\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Try to preserve a copy in case there is a problem and it has to be restored\n",
    "    shutil.copy2(\"sams_data_phase22_template.sqlite\",\"sams_data_phase22.sqlite\")\n",
    "    \n",
    "    print(\"Created database from template: sams_data_phase22.sqlite\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///sams_data_phase22.sqlite\")\n",
    "tab_files = db['files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of the files to process\n",
    "file_list = glob.glob(\"SAMSData_Missing/*.xls*\",recursive=True)\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we can process the files, we need to have the latest facilities in the database an in a list.\n",
    "\n",
    "After review with Ranya, most facilities should be accounted for in the current `facilities` table. Edits were made to an export of it and can be found at the following link, but will not be imported for now.\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1UoRB3AGCP1Domphie1rWbU2Uq5vSEb9A1JeGFbdGhh8/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a reference to the facilities\n",
    "tab_facilities = db['facilities']\n",
    "\n",
    "facility_lookup = {}\n",
    "\n",
    "for rec in tab_facilities.find():\n",
    "    facility_lookup[rec['facility_code']] = rec['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in file_list:\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(f,read_only=True)\n",
    "        problem = False\n",
    "        sheets = wb.sheetnames\n",
    "        num_sheets = len(sheets)\n",
    "    except:\n",
    "        print(\"Unable to load\",f)\n",
    "        problem = True\n",
    "        sheets = []\n",
    "        num_sheets = 0\n",
    "        \n",
    "    path = f\n",
    "    filename = f.split(\"\\\\\")[-1]\n",
    "    \n",
    "    # Stub this in later\n",
    "    country = None\n",
    "    \n",
    "    year = filename.split(\"-\")[1]\n",
    "    month = filename.split(\"-\")[2].split(\".\")[0]\n",
    "    facility_code = filename.split(\"-\")[0]\n",
    "    facility_id = None\n",
    "    \n",
    "    try:\n",
    "        facility_id = facility_lookup[facility_code]\n",
    "    except:\n",
    "        pass    \n",
    "    \n",
    "    skipped = False\n",
    "    ignore = False\n",
    "    \n",
    "    ungarbled = None\n",
    "    translation = None\n",
    "    \n",
    "    file_rec = {\n",
    "        \"file_name\":filename,\n",
    "        \"ungarbled\":ungarbled,\n",
    "        \"translation\":translation,\n",
    "        \"path\":path,\n",
    "        \"country\":country,\n",
    "        \"year\":year,\n",
    "        \"month\":month,\n",
    "        \"num_sheets\":num_sheets,\n",
    "        \"sheet_names\":str(sheets),\n",
    "        \"info\":\"\",\n",
    "        \"problem_opening\":problem,\n",
    "        \"skipped\":skipped,\n",
    "        \"ignore\":ignore,\n",
    "        \"facility_id\":facility_id,\n",
    "        \"added\":\"2018-04-03\",\n",
    "        \"processed\":0\n",
    "    }\n",
    "    \n",
    "    # Insert the file record into the database\n",
    "    tab_files.insert(file_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All good to this point - now need to break out the files and try to import the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files from May 2015\n",
    "\n",
    "Process May 2015 first and mark the files as processed when complete. These files all share a similar schema and should be importable with old code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_files = [rec for rec in db.query(\"SELECT * FROM files WHERE added = '2018-04-03' AND sheet_names NOT LIKE '%M1%' AND sheet_names NOT LIKE '%M3%';\")]\n",
    "\n",
    "tab_sheets = db['sheets']\n",
    "tab_files_sheets_join = db['files_sheets']\n",
    "\n",
    "sheets_lookup = {rec['name']:rec['id'] for rec in tab_sheets.find()}\n",
    "\n",
    "sheet_names_to_skip = [\n",
    "    \"TOTAL\",\n",
    "    \"Name\",\n",
    "    \"Code\",\n",
    "    \"Sheet\",\n",
    "    \"Monthly\",\n",
    "    \"Injured Info\",\n",
    "    \"statstics_weekly\", # intentional misspelling\n",
    "    \"Import SheetHIS\",\n",
    "    \"statstics_DailyHIS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheet_set = set()\n",
    "\n",
    "for rec in new_files:\n",
    "    sheets = ast.literal_eval(rec['sheet_names'])\n",
    "    for s in sheets:\n",
    "        sheet_set.add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Google Translate client\n",
    "translate_client = translate.Client()\n",
    "target_lang = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'statstics_weekly', 'added': '2018-04-03', 'translation': 'statstics_weekly', 'normalized': '', 'skip': True}\n",
      "{'name': 'lists', 'added': '2018-04-03', 'translation': 'lists', 'normalized': '', 'skip': False}\n",
      "{'name': 'register', 'added': '2018-04-03', 'translation': 'register', 'normalized': '', 'skip': False}\n",
      "{'name': 'Import SheetHIS', 'added': '2018-04-03', 'translation': 'Import SheetHIS', 'normalized': '', 'skip': True}\n",
      "{'name': 'لشمانيا', 'added': '2018-04-03', 'translation': 'Leishmania', 'normalized': '', 'skip': False}\n",
      "{'name': 'statstics_DailyHIS', 'added': '2018-04-03', 'translation': 'statstics_DailyHIS', 'normalized': '', 'skip': True}\n",
      "{'name': 'السنية', 'added': '2018-04-03', 'translation': 'Sunni conflict', 'normalized': '', 'skip': False}\n"
     ]
    }
   ],
   "source": [
    "for sheet_name in sheet_set:\n",
    "    if sheet_name not in sheets_lookup.keys():\n",
    "        new_rec = {\"name\":sheet_name,\"added\":\"2018-04-03\"}\n",
    "        translation = translate_client.translate(sheet_name, target_language=target_lang)\n",
    "        translation = translation['translatedText']\n",
    "        new_rec[\"translation\"] = translation\n",
    "        new_rec[\"normalized\"] = \"\"\n",
    "        skip = False\n",
    "\n",
    "        if any(skipname in sheet_name for skipname in sheet_names_to_skip):\n",
    "            skip = True\n",
    "        new_rec[\"skip\"] = skip\n",
    "        tab_sheets.insert(new_rec)\n",
    "        print(new_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created `sheet` records for sheets that did not previously appear in the table.\n",
    "\n",
    "Next, create the join records so that we can know what we've processed. These are sheet instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheets_lookup = {rec['name']:rec['id'] for rec in tab_sheets.find()}\n",
    "\n",
    "join_records = []\n",
    "\n",
    "for rec in new_files:\n",
    "    for sheet in ast.literal_eval(rec['sheet_names']):\n",
    "        sheet_id = sheets_lookup[sheet]\n",
    "        join_rec = {\n",
    "            \"file_id\":rec['id'],\n",
    "            \"sheet_id\":sheet_id,\n",
    "            \"header_start\":None,\n",
    "            \"header_end\":None,\n",
    "            \"header_values\":None,\n",
    "            \"added\":\"2018-04-03\"\n",
    "        }\n",
    "        join_records.append(join_rec)\n",
    "\n",
    "# Bulk inserts are faster than individual inserts\n",
    "tab_files_sheets_join.insert_many(join_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, locate the headers in the sheets in question. Note that it skips sheets we can't work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recs_to_process = db.query(\"\"\"\n",
    "SELECT files_sheets.id AS files_sheets_id, files_sheets.file_id, files_sheets.sheet_id, files.path AS file_path,sheets.name AS sheet_name\n",
    "FROM files_sheets\n",
    "JOIN files ON files_sheets.file_id = files.id\n",
    "JOIN sheets ON files_sheets.sheet_id = sheets.id\n",
    "WHERE sheets.skip = 0\n",
    "  AND files_sheets.added = '2018-04-03'\n",
    "ORDER BY file_id, sheet_id;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def headers_from_worksheet(workbook,worksheet_name):\n",
    "    worksheet = workbook.get_sheet_by_name(worksheet_name)\n",
    "    winning_row_values = 0\n",
    "    winning_row_number = None\n",
    "    \n",
    "    for row in range(1,21):\n",
    "        start_range = 'A' + str(row)\n",
    "        end_range = 'Z' + str(row)\n",
    "        cells = worksheet[start_range:end_range]\n",
    "        try:\n",
    "            values = [c.value for c in cells[0]]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        filled_cells = 0\n",
    "        for v in values:\n",
    "            if v is not None:\n",
    "                filled_cells += 1\n",
    "        \n",
    "        if filled_cells > winning_row_values:\n",
    "            winning_row_values = filled_cells\n",
    "            winning_row_number = row\n",
    "            \n",
    "    # Some sheets may be blank\n",
    "    if winning_row_number is None:\n",
    "        return None,None\n",
    "            \n",
    "    winning_start = 'A' + str(winning_row_number)\n",
    "    winning_end = 'Z' + str(winning_row_number)\n",
    "\n",
    "    header_cells = worksheet[winning_start:winning_end]\n",
    "    header_data = [c.value for c in header_cells[0]]\n",
    "    \n",
    "    # If we detect a datetime.datetime.object, then we probably\n",
    "    # want the previous row. Might be a better way to check this\n",
    "    # TODO: This is a bad idea?\n",
    "    \n",
    "    # What's the actual start column of the header?\n",
    "    start_idx = 0\n",
    "    determined_start = False\n",
    "    \n",
    "    for val in header_data:\n",
    "        if not determined_start and val is None:\n",
    "            start_idx += 1\n",
    "        elif val is not None:\n",
    "            determined_start = True\n",
    "            \n",
    "        if isinstance(val,datetime.datetime):\n",
    "            winning_row_number -= 1\n",
    "            \n",
    "    header_start_letter = letter_lookup[start_idx]\n",
    "            \n",
    "    winning_start = header_start_letter + str(winning_row_number)\n",
    "    winning_end = 'Z' + str(winning_row_number)\n",
    "    try:\n",
    "        header_cells = worksheet[winning_start:winning_end]\n",
    "        header_data = [c.value for c in header_cells[0]]\n",
    "    except:\n",
    "        return None,None\n",
    "    \n",
    "    end_idx = len(header_data) - 1\n",
    "    problem = ''\n",
    "#     print(end_idx)\n",
    "#     print(header_data)\n",
    "#     print(header_data[end_idx],\"\\n\")\n",
    "    \n",
    "    while header_data[end_idx] is None:\n",
    "        end_idx -= 1\n",
    "        if end_idx <= start_idx:\n",
    "            problem = ' (PROBLEM)'\n",
    "            break\n",
    "            \n",
    "    # Lookup assumes that the header starts with col A, so offset the lookup on the\n",
    "    # end letter by the start letter index and it will assign the proper letter to the\n",
    "    # end letter.\n",
    "    end_letter = letter_lookup[end_idx+start_idx]  \n",
    "    header_end = end_letter + winning_start[1:] + problem\n",
    "    header_range = (winning_start,header_end)\n",
    "    \n",
    "    # Prune the header_data to get rid of trailing None values\n",
    "    prune_by = 0\n",
    "    \n",
    "    while header_data[prune_by-1] is None:\n",
    "        prune_by -= 1\n",
    "        \n",
    "    try:\n",
    "        header_data = header_data[:prune_by]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return header_range, header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letter_lookup = ['A','B','C','D','E','F','G','H','I','J','K',\n",
    "                 'L','M','N','O','P','Q','R','S','T','U','V',\n",
    "                 'W','X','Y','Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename this ref b/c the old code did\n",
    "tab_files_sheets = db['files_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7599 Problem workbook SAMSData_Missing\\SAMS101-2015-09-1.xlsx --> sheet --> week4\n"
     ]
    }
   ],
   "source": [
    "variables = set()\n",
    "\n",
    "working_file_id = -1\n",
    "active_file_path = None\n",
    "active_workbook = None\n",
    "\n",
    "for rec in recs_to_process:\n",
    "    \n",
    "    # This only fires with a new file_id\n",
    "    if rec['file_id'] > working_file_id:\n",
    "        working_file_id = rec['file_id']\n",
    "        active_file_path = rec['file_path']\n",
    "        try:\n",
    "            active_workbook = openpyxl.load_workbook(active_file_path,read_only=True,guess_types=False,data_only=True)\n",
    "        except:\n",
    "            print(\"Unable to open\",active_file_path)\n",
    "            active_workbook = None\n",
    "            active_file_path = None\n",
    "            working_file_id = -1\n",
    "            \n",
    "    # Process the active file\n",
    "    sheet_name = rec['sheet_name']\n",
    "    header_range, header_data = headers_from_worksheet(active_workbook,sheet_name)\n",
    "    \n",
    "    # Unable to find a header in this sheet. Mark the record\n",
    "    if header_range is None:\n",
    "        update_rec = {\"id\":rec['files_sheets_id'],\"header_start\":\"PROBLEM\"}\n",
    "        tab_files_sheets.update(update_rec,['id'])\n",
    "        print(rec['files_sheets_id'],\"Problem workbook\",active_file_path,\"--> sheet -->\",sheet_name)\n",
    "        continue\n",
    "    else:\n",
    "        header_start = header_range[0]\n",
    "        header_end = header_range[1]\n",
    "        \n",
    "        fixed_header_data = []\n",
    "        for value in header_data:\n",
    "            if isinstance(value,datetime.datetime):\n",
    "                fixed_value = arrow.get(value).format(\"YYYY-MM-DD\")\n",
    "                fixed_header_data.append(fixed_value)\n",
    "                variables.add(fixed_value)\n",
    "            else:\n",
    "                fixed_header_data.append(value)\n",
    "                variables.add(value)\n",
    "        \n",
    "        update_rec = {\n",
    "            \"id\":rec['files_sheets_id'],\n",
    "            \"header_start\":header_start,\n",
    "            \"header_end\":header_end,\n",
    "            \"header_values\":str(fixed_header_data)\n",
    "        }\n",
    "        \n",
    "        tab_files_sheets.update(update_rec,['id'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix # SAMSData_Missing\\SAMS183-2015-02.xlsx register --> A3:AA755\n",
    "# file_id 1180 files_sheets_id 7703 sheet_id 205\n",
    "\n",
    "# File edited to address the issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to look at the variables that we already have in the variables table and see if any of the ones that we have from these new files are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_vars = db['variables']\n",
    "var_lookup = {rec['orig']:rec['id'] for rec in tab_vars.find()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weekbottom weekbottom\n",
      "رمز المريض Patient Code\n",
      "agemonthcat agemonthcat\n",
      "نازح/ لاجىء/ غيره Displaced / Refugee / Other\n",
      "جديد/مراجع New / References\n",
      "Followup Followup\n",
      "الاحالة Assignment\n",
      "Sex Sex\n",
      "Disposition Disposition\n",
      "التلقيح ضد الحصبة خلال الزيارة Measles vaccination during the visit\n",
      "التلقيح باللقاح الثلاثي خلال الزيارة Vaccine vaccination during the visit\n",
      "عمليات Operations\n",
      "death death\n",
      "مكان الاقامة Place of residence\n",
      "ملاريا ( شك ) Malaria (doubt)\n",
      "classification classification\n",
      "agebottom agebottom\n",
      "Agecat Agecat\n",
      "الوزن the weight\n",
      "التصنيف الطبي  Medical Classification\n",
      "عناية Attention\n",
      "Profile Profile\n",
      "agemonthb agemonthb\n",
      "درجة الحرارة temperature\n",
      "exit exit\n",
      "التلقيح ضد شلل الاطفال خلال الزيارة Polio vaccination during the visit\n",
      "الاجراء الجراحي Surgical procedure\n",
      "yesno yesno\n",
      "العمر بالاشهر(حديث الولادة) New Age (months of birth)\n",
      "حالة جديدة/مراجعة New status / revision\n",
      "العمر بالسنوات New years\n",
      "intervention intervention\n"
     ]
    }
   ],
   "source": [
    "# Note that we create the set() variables two cells above, when looking for headers\n",
    "\n",
    "for v in variables:\n",
    "    if v not in var_lookup.keys():\n",
    "        translation = translate_client.translate(v, target_language=target_lang)\n",
    "        translation = translation['translatedText']\n",
    "        new_var_rec = {'orig':v, 'translation':translation, 'normalized': 'new_or_followup', 'added':'2018-04-03' }\n",
    "        tab_vars.insert(new_var_rec)\n",
    "        print(v,translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fix_vars = {\n",
    "    \"weekbottom\": \"info_age\",\n",
    "    \"Profile\": \"profile\",\n",
    "    \"agemonthcat\": \"info_age\",\n",
    "    \"Disposition\": \"disposition\",\n",
    "    \"intervention\": \"treatment\",\n",
    "    \"agemonthb\": \"info_age\",\n",
    "    \"death\": \"death\",\n",
    "    \"exit\": \"discharge\",\n",
    "    \"yesno\": \"yes_no\",\n",
    "    \"New / References\": \"new_or_followup\",\n",
    "    \"Attention\": \"treatment\",\n",
    "    \"agebottom\": \"info_age\",\n",
    "    \"Sex\": \"info_sex\",\n",
    "    \"Malaria (doubt)\": \"malaria\",\n",
    "    \"classification\": \"status\",\n",
    "    \"Agecat\": \"info_age\",\n",
    "    \"Operations\": \"surgery_type\",\n",
    "    \"Followup\": \"new_or_followup\"\n",
    "}\n",
    "\n",
    "for k,v in fix_vars.items():\n",
    "    db.query(\"UPDATE variables SET normalized = '\" + v + \"' WHERE translation = '\" + k + \"';\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual DB editing done to help consolidate variables.\n",
    "\n",
    "Note from last running:\n",
    "\n",
    "Adding جديد/مراجع as id 63 because it's similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_lookup = {rec['orig']:rec['id'] for rec in tab_vars.find()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_vars = db['files_variables']\n",
    "tab_sheets_vars = db['sheets_variables']\n",
    "tab_files_sheets_vars = db['files_sheets_vars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_vars_set = set()\n",
    "sheets_vars_set = set()\n",
    "files_sheets_vars_set = set()\n",
    "\n",
    "recs_to_process = [rec for rec in db.query(\"SELECT * from files_sheets WHERE added = '2018-04-03';\")]\n",
    "\n",
    "for rec in recs_to_process:\n",
    "    header_values = rec['header_values']\n",
    "    if header_values is None:\n",
    "        continue\n",
    "    \n",
    "    header_values = ast.literal_eval(rec['header_values'])\n",
    "    \n",
    "    for header in header_values:\n",
    "        if header is None:\n",
    "            continue\n",
    "        try:\n",
    "            var_id = var_lookup[str(header)]    \n",
    "            file_id = rec['file_id']\n",
    "            sheet_id = rec['sheet_id']\n",
    "            files_sheets_id = rec['id']\n",
    "        except:\n",
    "            print(\"problem with\",header)\n",
    "            continue\n",
    "        \n",
    "        files_vars_set.add((file_id,var_id))\n",
    "        files_sheets_vars_set.add((files_sheets_id,var_id))\n",
    "        sheets_vars_set.add((sheet_id,var_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_vars_recs = []\n",
    "\n",
    "for rec_tuple in files_vars_set:\n",
    "    rec = {\"file_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_files_vars_recs.append(rec)\n",
    "    \n",
    "tab_files_vars.insert_many(tab_files_vars_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_sheets_vars_recs = []\n",
    "\n",
    "for rec_tuple in sheets_vars_set:\n",
    "    rec = {\"sheet_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_sheets_vars_recs.append(rec)\n",
    "    \n",
    "tab_sheets_vars.insert_many(tab_sheets_vars_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_sheets_vars_recs = []\n",
    "\n",
    "for rec_tuple in files_sheets_vars_set:\n",
    "    rec = {\"files_sheets_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_files_sheets_vars_recs.append(rec)\n",
    "    \n",
    "tab_files_sheets_vars.insert_many(tab_files_sheets_vars_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to importing the raw data\n",
    "\n",
    "This is coming from workbook 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheets_to_process = db.query(\"\"\"\n",
    "SELECT files_sheets.id AS files_sheets_id, \n",
    "       files_sheets.file_id, \n",
    "       files_sheets.sheet_id, \n",
    "       files_sheets.header_start, \n",
    "       files_sheets.header_end,\n",
    "       files_sheets.header_values,\n",
    "       files.path AS file_path,\n",
    "       sheets.name AS sheet_name\n",
    "FROM files_sheets\n",
    "JOIN files ON files_sheets.file_id = files.id\n",
    "JOIN sheets ON files_sheets.sheet_id = sheets.id\n",
    "WHERE sheets.skip = 0\n",
    "  AND files_sheets.added = '2018-04-03'\n",
    "\n",
    "AND files.ignore = 0\n",
    "AND files_sheets.header_start IS NOT NULL\n",
    "AND files_sheets.header_start <> 'PROBLEM'\n",
    "AND files_sheets.header_end NOT LIKE '%PROBLEM%'\n",
    "ORDER BY file_id, files_sheets_id;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_all = db['full_raw_scrubbed']\n",
    "tab_vars = db['variables']\n",
    "tab_files_sheets = db['files_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to create some columns...\n",
    "raw_columns = [k for k in tab_all.find_one().keys()]\n",
    "flag_cols = [c for c in raw_columns if 'flag_' in c]\n",
    "col_names_we_need = sorted(list(set([r['normalized'] for r in tab_vars.find()])))\n",
    "missing_col_names = set(col_names_we_need) - set(raw_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the new columns\n",
    "tab_all.create_column(\"added\", sqlalchemy.String)\n",
    "\n",
    "for c in missing_col_names:\n",
    "    tab_all.create_column(c, sqlalchemy.String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename var_lookup to work with code below\n",
    "\n",
    "# Create an in-memory lookup table for variables\n",
    "var_lookup = {}\n",
    "for r in tab_vars.find():\n",
    "    var_lookup[r['orig']] = r['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only reopen files when necessary\n",
    "working_file_id = -1\n",
    "active_file_path = None\n",
    "active_workbook = None\n",
    "\n",
    "\n",
    "for rec in sheets_to_process:\n",
    "    import_status = \"\"\n",
    "    \n",
    "    if rec['file_id'] > working_file_id:\n",
    "        working_file_id = rec['file_id']\n",
    "        active_file_path = rec['file_path']\n",
    "        try:\n",
    "            active_workbook = openpyxl.load_workbook(active_file_path,read_only=True,guess_types=False,data_only=True)\n",
    "        except:\n",
    "            import_status = \"Unable to open file\"\n",
    "            import_status = \"imported\"\n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])\n",
    "            print(\"Unable to open\",active_file_path)\n",
    "            \n",
    "            active_workbook = None\n",
    "            active_file_path = None\n",
    "            working_file_id = -1\n",
    "            continue\n",
    "            \n",
    "    # Process the active file\n",
    "    sheet_name = rec['sheet_name']\n",
    "    header_start = rec['header_start']\n",
    "    header_end = rec['header_end']\n",
    "    \n",
    "    # Unable to find a header in this sheet. Mark the record\n",
    "    if header_start is None or header_end is None or \"PROBLEM\" in header_start or \"PROBLEM\" in header_end:\n",
    "        import_status = \"skipped\"\n",
    "        continue\n",
    "    else:\n",
    "        worksheet = active_workbook.get_sheet_by_name(sheet_name)\n",
    "        last_row = worksheet.max_row\n",
    "        \n",
    "        # Sometimes worksheet.max_row doesn't return a value\n",
    "        if last_row == None:\n",
    "            import_status = \"imported: last row None\"\n",
    "            last_row = 10000\n",
    "        \n",
    "        data_start = header_start[0] + str(int(header_start[1:])+1)\n",
    "        data_end = header_end[0] + str(last_row)\n",
    "        data_range_string = data_start + \":\" + data_end\n",
    "        \n",
    "        # These are stored as a list converted to a string. Convert back to a list for enumeration\n",
    "        header_values = ast.literal_eval(rec['header_values'])\n",
    "        sheet_data = []\n",
    "        \n",
    "        try:\n",
    "            for datarow in worksheet[data_range_string]:\n",
    "                record = {}\n",
    "                for idx,cell in enumerate(datarow):\n",
    "                    header_val = header_values[idx]\n",
    "\n",
    "                    # Get the normalized value\n",
    "                    header_val = var_lookup[header_val]\n",
    "\n",
    "                    cell_value = cell.value                \n",
    "\n",
    "                    try:\n",
    "                        cell_value = cell_value.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Cannot write datetime objects to the database\n",
    "                    # Unless they are first converted to strings\n",
    "\n",
    "                    if isinstance(cell_value,datetime.datetime):\n",
    "                        cell_value = openpyxl.utils.datetime.datetime_to_W3CDTF(cell_value)\n",
    "                    elif isinstance(cell_value,datetime.time):\n",
    "                        cell_value = str(cell_value)\n",
    "                    elif cell_value is not None:\n",
    "                        cell_value = str(cell_value)\n",
    "\n",
    "                    # There's already a value in the field and it should be a string.\n",
    "                    # If it is our representation of none, replace it\n",
    "                    if header_val in record.keys():\n",
    "                        if record[header_val] == '.' or record[header_val] is None:\n",
    "                            record[header_val] = cell_value\n",
    "                        elif cell_value is not None:\n",
    "                            record[header_val] = record[header_val] + \", \" + cell_value\n",
    "                    else:\n",
    "\n",
    "                        # Blank strings instead of NULL will help us know which fields were available for the record\n",
    "                        if cell_value is None:\n",
    "                            record[header_val] = '.'\n",
    "                        else:\n",
    "                            record[header_val] = cell_value\n",
    "\n",
    "                sheet_data.append(record)\n",
    "\n",
    "            # Remove from sheet_data blank rows\n",
    "            rich_sheet_data = []\n",
    "            for staged in sheet_data:\n",
    "                working_copy = copy.deepcopy(staged)\n",
    "                try:\n",
    "                    # Put \"passover columns\" here. They will be removed from the record\n",
    "                    # before it is evaluated as \"empty.\" Number is a good example because there are sheets\n",
    "                    # where somebody dragged numbers down a column in preparation for a lot of data but \n",
    "                    # never actually used all of the numbered rows\n",
    "                    del working_copy['number']\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                if all((x == None or x == '.' or x == '') for x in list(working_copy.values())):\n",
    "                    continue\n",
    "                else:\n",
    "                    staged['a_file_id'] = rec['file_id']\n",
    "                    staged['a_files_sheets_id'] = rec['files_sheets_id']\n",
    "                    staged['a_sheet_id'] = rec['sheet_id']\n",
    "                    staged['added'] = '2018-04-03'\n",
    "\n",
    "                    rich_sheet_data.append(staged)\n",
    "\n",
    "            # Try to perform a bulk insert\n",
    "            tab_all.insert_many(rich_sheet_data)\n",
    "\n",
    "            # Update the status of the worksheet\n",
    "            import_status = \"imported\"\n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])\n",
    "            \n",
    "        except Exception as ex:\n",
    "            \n",
    "            print(\"\\n--------------------------------------------------------------------------\")\n",
    "            print(ex)\n",
    "            print(\"Failure\")\n",
    "            print(\"file_id\",rec['file_id'],\"files_sheets_id\",rec['files_sheets_id'],\"sheet_id\",rec['sheet_id'])\n",
    "            print(active_file_path,sheet_name)\n",
    "            print(\"Header range:\",header_start,header_end)\n",
    "            print(\"Data range:\",data_start,data_end)\n",
    "            print(\"data_range_string\",data_range_string)\n",
    "            \n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "- scrub PII in situ\n",
    "- export to arabic_values\n",
    "- translate\n",
    "- flag\n",
    "- generate flagged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Scrub PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_update = {}\n",
    "\n",
    "# Do not save this value in a source code repository!\n",
    "salt = 'REDACTED'.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"info_name\",\n",
    "    \"info_name_author\",\n",
    "    \"info_name_caregiver\",\n",
    "    \"info_name_facility\",\n",
    "    \"info_name_group\",\n",
    "    \"info_name_of_coach\",\n",
    "    \"info_name_processor\",\n",
    "    \"info_name_surgeon\",\n",
    "    \"info_phone_skype\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rec in tab_all.find(added='2018-04-03'):\n",
    "    for pii_field in fields:\n",
    "        if rec[pii_field] is None or rec[pii_field] == '.':\n",
    "            continue\n",
    "        else:\n",
    "            # Hash the value in the field\n",
    "            h = hashlib.sha256()\n",
    "            h.update(rec[pii_field].encode())\n",
    "            h.update(salt)\n",
    "            \n",
    "            if rec['id'] not in to_update.keys():\n",
    "                to_update[rec['id']] = {'id':rec['id']}\n",
    "\n",
    "            to_update[rec['id']][pii_field] = h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in to_update.keys():\n",
    "    tab_all.update(to_update[k],['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to arabic_values and translate\n",
    "\n",
    "Be careful here because arabic_values no longer auto-increments the id, so it has to be set manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_raw = db['full_raw_scrubbed']\n",
    "tab_arabic = db['arabic_values']\n",
    "tab_vars = db['variables']\n",
    "\n",
    "column_names = db.query(\"SELECT DISTINCT(normalized) FROM variables;\")\n",
    "column_names = sorted([r['normalized'] for r in column_names])\n",
    "\n",
    "# We don't want to work with the values in the fields that have been hashed,\n",
    "# so remove them from the list of variables to query.\n",
    "fields = [\n",
    "    \"info_name\",\n",
    "    \"info_name_author\",\n",
    "    \"info_name_caregiver\",\n",
    "    \"info_name_facility\",\n",
    "    \"info_name_group\",\n",
    "    \"info_name_of_coach\",\n",
    "    \"info_name_processor\",\n",
    "    \"info_name_surgeon\",\n",
    "    \"info_phone_skype\",\n",
    "    \"date\",\n",
    "    \"date_first_exam\",\n",
    "    \"death_date\"\n",
    "]\n",
    "column_names = [e for e in column_names if e not in fields]\n",
    "\n",
    "arabic_lookup = set([r['arabic'] for r in tab_arabic.find()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_id_results = db.query(\"SELECT max(id) FROM arabic_values;\")\n",
    "for r in max_id_results:\n",
    "    max_id = int(r['max(id)'])\n",
    "\n",
    "current_id = max_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = []\n",
    "for col in column_names:\n",
    "    col_values = db.query(\"\"\"\n",
    "        SELECT DISTINCT([\"\"\" + col + \"\"\"]) \n",
    "        FROM full_raw_scrubbed \n",
    "        WHERE added = '2018-04-03'\n",
    "        AND [\"\"\" + col + \"\"\"] IS NOT NULL\n",
    "        AND [\"\"\" + col + \"\"\"] <> '.'\n",
    "        AND [\"\"\" + col + \"\"\"] <> '';\n",
    "        \"\"\")\n",
    "    col_values = [r[col] for r in col_values]\n",
    "\n",
    "    # Create a table of unique Arabic values\n",
    "    for v in col_values:\n",
    "        if v in arabic_lookup:\n",
    "            continue\n",
    "        # Skip numbers\n",
    "        if v.replace(\",\",\".\").replace('.','',1).isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            r = {\"id\":current_id,\"arabic\":v,\"added\":'2018-04-03'}\n",
    "            current_id += 1\n",
    "            buffer.append(r)\n",
    "            arabic_lookup.add(v)\n",
    "            \n",
    "# 367621\n",
    "\n",
    "tab_arabic.insert_many(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display errors in realtime\n",
    "import ipywidgets as widgets\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "record_counter = widgets.HTML(value=\"Records: 0\",continuous_update=True)\n",
    "character_counter = widgets.HTML(value=\"Characters: 0\",continuous_update=True)\n",
    "error_counter = widgets.HTML(value=\"Errors: 0\",continuous_update=True)\n",
    "latest_translation = widgets.HTML(value=\"Latest: -\", continuous_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11174d051ecb411ea5a22ad69ceed217"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "record_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241fe665dc07444194b63043327cf661"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "character_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6c8f9b54bc48978ea8532bc7495b7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc1208c3306416f94619a05a4800ca3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latest_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recordset length 3562\n",
      "--------------\n",
      "Done\n",
      "Record Count 3562\n",
      "Character Count 86171\n"
     ]
    }
   ],
   "source": [
    "character_count = 0\n",
    "record_count = 0\n",
    "error_count = 0\n",
    "\n",
    "translate_client = translate.Client()\n",
    "target_lang = 'en'\n",
    "\n",
    "recordset = [r for r in tab_arabic.find(added='2018-04-03')]\n",
    "print(\"recordset length\",len(recordset))\n",
    "\n",
    "update_records = []\n",
    "\n",
    "for row in recordset:\n",
    "    arabic_string = row['arabic']\n",
    "    arabic_string.replace(\"_\",\" \").replace(\"\\n\",\" \").replace(\"  \",\" \").replace(\"\\t\",\" \").strip()\n",
    "    try:\n",
    "        english = translate_client.translate(arabic_string, target_language=target_lang)\n",
    "        english = english['translatedText']\n",
    "        new_rec = {\n",
    "            \"id\":row['id'],\n",
    "            \"google_translate_feb\":english\n",
    "        }\n",
    "\n",
    "        update_records.append(new_rec)\n",
    "\n",
    "        character_count += len(row['arabic'])\n",
    "        character_counter.value = \"Characters: \" + str(character_count)\n",
    "        record_count += 1\n",
    "        record_counter.value = \"Records: \" + str(record_count)\n",
    "        latest_translation.value = \"Latest: \" + english\n",
    "        \n",
    "        del new_rec\n",
    "\n",
    "    except:\n",
    "        error_count += 1\n",
    "        error_counter.value = \"Errors: \" + str(error_count)\n",
    "        time.sleep(10)\n",
    "        \n",
    "print(\"--------------\\nDone\")\n",
    "print(\"Record Count\",record_count)\n",
    "print(\"Character Count\",character_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rec in update_records:\n",
    "    tab_arabic.update(rec,['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize & translate missing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_tokens = db['arabic_tokens']\n",
    "tab_arabic_values_tokens = db['arabic_values_tokens']\n",
    "\n",
    "good_char = 'ا'\n",
    "bad_char1 = 'أ'\n",
    "bad_char2 = 'إ'\n",
    "bad_char3 = 'آ'\n",
    "\n",
    "delimiters = \"(\", \")\", \",\", \"/\", \".\", \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\"+\",\"_\",\"-\",\"\\\\\",\"=\",\"ـ\"\n",
    "regexpattern = \"|\".join(map(re.escape,delimiters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lookup = {}\n",
    "\n",
    "recordset = [r for r in tab_arabic.find(added='2018-04-03')]\n",
    "\n",
    "for rec in recordset:\n",
    "    tokens = rec['arabic']\n",
    "    tokens = tokens.replace(bad_char1,good_char)\n",
    "    tokens = tokens.replace(bad_char2,good_char)\n",
    "    tokens = tokens.replace(bad_char3,good_char)\n",
    "    \n",
    "    tokens = [t.strip() for t in re.split(regexpattern,tokens) if t.strip() != '']\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in token_lookup.keys():\n",
    "            token_lookup[token] = {\"token_id\":None, \"arabic_values_id\":[]}\n",
    "        token_lookup[token][\"arabic_values_id\"].append(rec['id'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2976"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "existing_token_lookup = {}\n",
    "\n",
    "for rec in tab_tokens.find():\n",
    "    existing_token_lookup[rec['token']] = rec['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "existing_toks = list(existing_token_lookup.keys())\n",
    "\n",
    "for tok in token_lookup.keys():\n",
    "    if tok in existing_toks:\n",
    "        token_lookup[tok]['token_id'] = existing_token_lookup[tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tokens = []\n",
    "\n",
    "for k,v in token_lookup.items():\n",
    "    if v['token_id'] is None:\n",
    "        new_tok = {\"token\":k, \"added\":'2018-04-03'}\n",
    "        new_tokens.append(new_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_tokens.insert_many(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "existing_token_lookup = {}\n",
    "\n",
    "for rec in tab_tokens.find():\n",
    "    existing_token_lookup[rec['token']] = rec['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tok in token_lookup.keys():\n",
    "    token_lookup[tok]['token_id'] = existing_token_lookup[tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arabic_values_id': [396790, 396791, 396792, 396793, 396794, 396795],\n",
       " 'token_id': 153469}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lookup['susp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_records = []\n",
    "for token in token_lookup.keys():\n",
    "    for orig_id in token_lookup[token][\"arabic_values_id\"]:\n",
    "        join_records.append({\"token_id\":token_lookup[token][\"token_id\"],\"arabic_values_id\":orig_id, \"added\":'2018-04-03'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in join_records:\n",
    "    tab_arabic_values_tokens.insert(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now translate the new tokens\n",
    "\n",
    "translate_client = translate.Client()\n",
    "target_lang = 'en'\n",
    "\n",
    "updates = []\n",
    "\n",
    "tokens_to_translate = list(tab_tokens.find(added='2018-04-03'))\n",
    "\n",
    "for rec in tokens_to_translate:\n",
    "    arabic = rec['token']\n",
    "    english = translate_client.translate(arabic, target_language=target_lang)\n",
    "    english = english['translatedText']\n",
    "    update_rec = {\n",
    "        \"id\":rec['id'],\n",
    "        \"google_translate_feb\":english,\n",
    "        \"translation\":english\n",
    "    }\n",
    "\n",
    "    updates.append(update_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rec in updates:\n",
    "    tab_tokens.update(rec,['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now join the translated tokens back to the arabic_values table for flag generation\n",
    "\n",
    "join_lookup = {}\n",
    "\n",
    "for record in tab_arabic_values_tokens.find():\n",
    "    if record['arabic_values_id'] not in join_lookup.keys():\n",
    "        join_lookup[record['arabic_values_id']] = set()\n",
    "    join_lookup[record['arabic_values_id']].add(record['token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lookup = {}\n",
    "\n",
    "# Not new, actually...\n",
    "new_tokens = list(tab_tokens.find())\n",
    "\n",
    "for record in new_tokens:\n",
    "    translation = record['translation']\n",
    "    \n",
    "    # Nothing to join back\n",
    "    if translation is None:\n",
    "        continue\n",
    "    else:\n",
    "        translation = [x.lower().strip() for x in translation.split(\",\") if x.strip() != '']\n",
    "        \n",
    "    token_lookup[record['id']] = translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "397106",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-8dbadf7e03a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjoin_lookup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m397106\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 397106"
     ]
    }
   ],
   "source": [
    "join_lookup[397106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_arabic_values = db['arabic_values']\n",
    "\n",
    "ar_values_to_update = []\n",
    "\n",
    "new_ar_val_ids = []\n",
    "\n",
    "for rec in tab_arabic_values.find(added='2018-04-03'):\n",
    "    new_ar_val_ids.append(rec['id'])\n",
    "\n",
    "errors = 0\n",
    "    \n",
    "for key in new_ar_val_ids:\n",
    "    update_rec = {'id':key}\n",
    "    all_vals = []\n",
    "    try:\n",
    "        for token_rec in sorted(list(join_lookup[key])):\n",
    "            try:\n",
    "                all_vals += token_lookup[token_rec]\n",
    "            except:\n",
    "                pass\n",
    "        if len(all_vals) == 0:\n",
    "            continue\n",
    "\n",
    "        hum_trans = \", \".join(all_vals)\n",
    "        update_rec['google_tokens_joined'] = hum_trans\n",
    "        ar_values_to_update.append(update_rec)\n",
    "    except Exception:\n",
    "        errors += 1\n",
    "        pass\n",
    "    \n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3560"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ar_values_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'google_tokens_joined': 'another reason', 'id': 396835},\n",
       " {'google_tokens_joined': 'unknown reason', 'id': 396836},\n",
       " {'google_tokens_joined': 'injuries اذيات ورضوض', 'id': 396837},\n",
       " {'google_tokens_joined': 'hypertension', 'id': 396838},\n",
       " {'google_tokens_joined': 'watery diarrhoea water desalination', 'id': 396839},\n",
       " {'google_tokens_joined': 'lrti respiratory infection', 'id': 396840},\n",
       " {'google_tokens_joined': 'امراض جلدية, باستثناء الليشمانيا, skin disease, not leishmaniasis',\n",
       "  'id': 396841},\n",
       " {'google_tokens_joined': 'peptic ulcer', 'id': 396842},\n",
       " {'google_tokens_joined': 'other diseases', 'id': 396843},\n",
       " {'google_tokens_joined': 'urinary tract infection, urinary tract infection, dws',\n",
       "  'id': 396844}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_values_to_update[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rec in ar_values_to_update:\n",
    "    tab_arabic_values.update(rec,['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flag generation\n",
    "\n",
    "Only need to generate flags for the new arabic_values records & then generate the new flags for the flag dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a reference to the arabic_values table\n",
    "tab_arabic_values = db['arabic_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped full_raw_flags\n",
      "Dropped full_raw_flags_reduced\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    db['full_raw_flags'].drop()\n",
    "    print(\"Dropped full_raw_flags\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    db['full_raw_flags_reduced'].drop()\n",
    "    print(\"Dropped full_raw_flags_reduced\").drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# One SQLite limitation is you cannot drop columns, so you have to create a new table and then rename it.\n",
    "preserve_fields = [k for k in tab_arabic_values.find_one().keys() if 'flag_' not in k]\n",
    "\n",
    "# We don't use result but assigning it skips printing some garbage below\n",
    "result = db.query(\"\"\"\n",
    "CREATE TABLE new_arabic_values AS \n",
    "    SELECT \"\"\" + \",\".join(preserve_fields) + \"\"\" \n",
    "    FROM arabic_values;\n",
    "\"\"\")\n",
    "\n",
    "# Drop the original arabic_values table\n",
    "tab_arabic_values.drop()\n",
    "\n",
    "# Rename new_arabic_values to arabic_values & now we have a table with no flag columns\n",
    "result = db.query(\"\"\"\n",
    "ALTER TABLE new_arabic_values RENAME TO arabic_values;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now because we futzed with the arabic_values table, we have to create a new reference to the database\n",
    "# and to our arabic_values table. The db object stores some schema information that isn't updated with\n",
    "# our direct query calls above.\n",
    "\n",
    "del db\n",
    "del tab_arabic_values\n",
    "\n",
    "db = dataset.connect(\"sqlite:///sams_data_phase22.sqlite\")\n",
    "tab_arabic_values = db['arabic_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now create an in-memory representation of the arabic_values table\n",
    "# and store it in variable `data`\n",
    "data = [x for x in tab_arabic_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update this if you want to change what flags you are making on the dataset.\n",
    "# The logic for creating them is in the following cell.\n",
    "\n",
    "# Require and flag term\n",
    "flag_terms = [\n",
    "    \"blunt\",\n",
    "    \"explosive\",\n",
    "    \"blast\",\n",
    "    \"stab\",\n",
    "    \"upper extremity\",\n",
    "    \"lower extremity\",\n",
    "    \"neck\",\n",
    "    \"chest\",\n",
    "    \"back\",\n",
    "    \"spinal\",\n",
    "    \"neurologic\",\n",
    "    \"nerve\",\n",
    "    \"vascular\",\n",
    "    \"orthopedic\",\n",
    "    \"fracture\",\n",
    "    \"suspected\",\n",
    "    \"follow-up\",\n",
    "    \"complication\",\n",
    "    \"history of\",\n",
    "    \"traffic accident\"\n",
    "]\n",
    "\n",
    "# require all terms - not in use at the moment\n",
    "multiple_flag_terms = [\n",
    "#     (\"burn\",\"fracture\")\n",
    "]\n",
    "\n",
    "# require any of the terms but name the flag after the first\n",
    "synonym_flag_terms = [\n",
    "    (\"allergy\", \"allergic\"),\n",
    "    (\"anemia\", \"thalassemia\"),\n",
    "    (\"cancer\", \"bcc\", \"leukemia\", \"lymphoma\", \"malignancy\", \"malignant\", \"scc\"),\n",
    "    (\"cardiovascular\",\" asd \",\" vsd \",\"cholesterol\",\"hypercholesterolemia\",\"hyperlipidemia\",\"hypertriglyceridemia\",\"triglycerides\",\"blood pressure\",\" bp \",\"high blood pressure\",\"hypertension\",\"acute coronary syndrome\",\"angina\",\"arrhythmia\",\"atrial fibrillation\",\" avr \",\"cardiac ischemia\",\"chest pain\",\"clot\",\"clotting\",\"coronary atery\",\"coronary heart disease\",\"coronary ischemia\",\"dvt\",\"endocarditis\",\"heart attack\",\"heart disease\",\"heart failure\",\"heart valve\",\"hf\",\"hypotension\",\"ihd\",\" mi \",\"mitral valve prolapse\",\"mvr\",\"myocardial hypoperfusion\",\"myocardial infarction\",\"palpitations\",\"pericarditis\",\"pulmonary embolism\",\"pvd\",\"svt\",\"thromboembolism\",\"thrombophlebitis\",\"thrombosis\",\"vasculitis\"),\n",
    "    (\"congenital\", \"asd\", \"vsd\"),\n",
    "    (\"dehydration\", \"dehydration\", \"hypovolemic shock\"),\n",
    "    (\"dental complaint\", \"dental\", \"gingivitis\", \" gum \", \"odonitis\", \"teeth\", \"tooth\", \"toothache\"),\n",
    "    (\"derm\", \"acne\",\"alopecia\",\"blisters\",\"cellulitis\",\"dermatitis\",\"dermoid\",\"dry skin\",\"eczema\",\"folliculitis\",\"hair loss\",\"inflammatory papules\",\"intertrigo\",\"itch\",\"lice\",\"pruritis\",\"psoriasis\",\"rash\",\"ringworm\",\"scabies\",\"skin disease\",\"skin disorder\",\"skin eruption\",\"skin infection\",\"skin lesion\",\"tinea\",\"warts\"),\n",
    "    (\"diabetes\",\"diabetic\",\"DKA\",\"glucose\",\"hyperglycemia\",\"hypoglycemia\",\"sugar\"),\n",
    "    (\"endocrine\",\"hyperthyroid\",\"hyperthyroidism\",\"hypocalcemia\",\"hypothyroid\",\"hypothyroidism\",\"parathyroid\",\"thyroid\",\" TSH \"),\n",
    "    (\"infection\",\"conjunctivitis\",\"eye discharge\",\"eye infection\",\"keratoconjunctivitis\",\"ophthalmic infection\"),\n",
    "    (\"pain\", \"corneal inflammation\", \"eye sensitivity\", \"keratitis\", \"pain in the eye\"),\n",
    "    (\"fatigue\", \"exhaustion\", \"tired\", \"tiredness\"),\n",
    "    (\"fever\", \"hyperthermia\", \"temperature\"),\n",
    "    (\"constipation\", \"intestinal stasis\"),\n",
    "    (\"shrapnel\", \"fragments\",\"sliver\",\"splinter\"),\n",
    "    (\"musculoskeletal pain\",\"ankylosing spondylitis\",\"arthralgia\",\"Arthritis\",\"back pain\",\"bruise\",\"bruising\",\"chondritis\",\"contusion\",\"costochondritis\",\"disc herniation\",\"disc herniation\",\"discitis\",\"elbow pain\",\"extremity pain\",\"gout\",\"inflammation of the shoulder\",\"joint\",\"knee degeneration\",\"knee inflammation\",\"knee pain\",\"loin pain\",\"low back pain\",\"lumbar pain\",\"musclar pain\",\"Muscle spasm\",\"muscular pain\",\"myalgia\",\"myositis\",\"neck pain\",\"osteoarthritis\",\"osteomyelitis\",\"osteomylitis\",\"plantar fasciitis\",\"polyarthritis\",\"rheumatism\",\"sacroiliitis\",\"spine degeneration\",\"sprain\",\"strain\",\"synovitis\",\"tendinitis\",\"tendonitis\",\"tendonopathy\",\"tmj\"),\n",
    "    (\"headache\", \"head pain\"),\n",
    "    (\"stroke\",\"cerebral accident\",\"cerebral hemorrhage\",\"cerebral infarction\",\"cerebral ischemia\",\"cerebrovascular accident\",\" cva \"),\n",
    "    (\"gunshot\", \" shot \"),\n",
    "    \n",
    "    # Prior flags, preserved\n",
    "    (\"facial\",\"face\"),\n",
    "    (\"pelvic\",\"pelvis\"),\n",
    "    (\"head\",\"eye\",\"ear\",\"face\",\"brain\",\"scalp\",\"mouth\",\"nose\"),\n",
    "    (\"spine\",\"spinal\"),\n",
    "    (\"abdomen\",\"abdominal\")\n",
    "]\n",
    "\n",
    "# require the first term and the absence of the remaining terms\n",
    "# name the flag after the first term.\n",
    "complex_flag_terms = [\n",
    "    (\"urologic\",\"neurologic\"),\n",
    "    (\"burn\",\"heartburn\"),\n",
    "    (\"trauma\", \"psychological trauma\")\n",
    "]\n",
    "\n",
    "# Look for any of the terms in terms_to_find but only apply if terms in terms_to_avoid are absent.\n",
    "# Check human or google translation (ht, gt)\n",
    "\n",
    "complex_set_flag_terms = [\n",
    "    {\n",
    "        \"flag_name\": \"hyperlipidemia\",\n",
    "        \"terms_to_find\": [\"blood pressure\", \"bp\", \"high blood pressure\", \"hypertension\"],\n",
    "        \"terms_to_avoid\": [\"hypotension\"],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"ENT\",\n",
    "        \"terms_to_find\": [\"adenoiditis\",\"ear congestion\",\"ear discharge\",\"ear infection\",\"ear inflammation\",\"eustachian tube infection\",\"mucositis\",\"mumps\",\"nasal congestion\",\"nose congestion\",\"otitis\",\"otorrhea\",\"pharyngitis\",\"throat ache\",\"tonsillitis\",\"tonsils enlargement\",\"cerumen impaction\",\"dysphagia\",\"earache\",\"epistaxis\",\"hearing impairment\",\"hearing loss\",\"nasal obstruction\",\"pain in the ear\",\"pharyngeal pain\",\"pharynx pain\",\"swallowing pain\",\"vestibulitis\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"infection\",\n",
    "        \"terms_to_find\": [\"adenoiditis\",\"ear congestion\",\"ear discharge\",\"ear infection\",\"ear inflammation\",\"eustachian tube infection\",\"laryngitis\",\"mucositis\",\"mumps\",\"nasal congestion\",\"nose congestion\",\"otitis\",\"otorrhea\",\"pharyngitis\",\"rhinitis\",\"sinusitis\",\"throat ache\",\"tonsillitis\",\"tonsils enlargement\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"eye\",\n",
    "        \"terms_to_find\": [\"conjunctivitis\",\"eye discharge\",\"eye infection\",\"keratoconjunctivitis\",\"ophthalmic infection\",\"corneal inflammation\",\"eye sensitivity\",\"keratitis\",\"pain in the eye\",\"blepharitis\",\"cataract\",\"eye redness\",\"eyelid\",\"eye-redness\",\"glaucoma\",\"left eye\",\"my eye\",\"npdr\",\"pterygium\",\"pupil\",\"redness of the eye\",\"retinal\",\"retinopathy\",\"right eye\",\"swelling of the eye\",\"uveitis\",\"vision\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"gi_complaint\",\n",
    "        \"terms_to_find\": [\"abdominal injury\",\"apendicitis\",\"appendicitis\",\"belly pain\",\"bile duct obstruction\",\"bile stones\",\"cholecystitis\",\"colic\",\"colitis\",\"colon spasm\",\"Crohn\",\"duodenal ulcer\",\"enteritis\",\"epigastric pain\",\"flank pain\",\"gallbladder inflammation\",\"gastric pain\",\"gastric ulcer\",\"gastritis\",\"gastroenteritis\",\"gastrointestinal infection\",\"hiatal hernia\",\"ibd\",\"ibs\",\"indigestion\",\"inflammation of the stomach\",\"intestinal pain\",\"intestinal ulcer\",\"pain in the stomach\",\"pancreatitis\",\"peptic ulcer\",\"peritoneal inflammation\",\"peritonitis\",\"sore stomach\",\"stomach hurts\",\"stomach pain\",\"Digestive bleed\",\"Gastric bleeding\",\"Gastric hemorrhage\",\"Gastrointestinal bleeding\",\"hemorrhoids\",\"Ulcer of the colon\",\"Constipation\",\"intestinal stasis\",\"diarrhea\",\"dysentery\",\"food poisoning\",\"giardia\",\"typhoid\",\"cirrhosis\",\"hapatitis\",\"hep a\",\"hep b\",\"hep c\",\"hepatic\",\"jaundice\",\"nausea\",\"vomiting\",\"vomitting\",\"anal fissure\",\"bloating\",\"celiac disease\",\"esophageal reflux\",\"gastroesophageal reflux\",\"gerd\",\"heartburn\",\"inguinal hernia\",\"malabsorption\",\"umbilical fistula\",\"umbilical hernia\"],\n",
    "        \"terms_to_avoid\": [\"renal colic\"],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"abdominal_pain\",\n",
    "        \"terms_to_find\": [\"abdominal injury\",\"apendicitis\",\"appendicitis\",\"belly pain\",\"bile duct obstruction\",\"bile stones\",\"cholecystitis\",\"colic\",\"colitis\",\"colon spasm\",\"Crohn\",\"duodenal ulcer\",\"enteritis\",\"epigastric pain\",\"flank pain\",\"gallbladder inflammation\",\"gastric pain\",\"gastric ulcer\",\"gastritis\",\"gastroenteritis\",\"gastrointestinal infection\",\"hiatal hernia\",\"ibd\",\"ibs\",\"indigestion\",\"inflammation of the stomach\",\"intestinal pain\",\"intestinal ulcer\",\"pain in the stomach\",\"pancreatitis\",\"peptic ulcer\",\"peritoneal inflammation\",\"peritonitis\",\"sore stomach\",\"stomach hurts\",\"stomach pain\"],\n",
    "        \"terms_to_avoid\": [\"renal colic\"],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"bleed\",\n",
    "        \"terms_to_find\": [\"Digestive bleed\",\"Gastric bleeding\",\"Gastric hemorrhage\",\"Gastrointestinal bleeding\",\"hemorrhoids\",\"Ulcer of the colon\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"diarrhea_dysentery\",\n",
    "        \"terms_to_find\": [\"diarrhea\",\"dysentery\",\"food poisoning\",\"giardia\",\"typhoid\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"liver_dysfunction\",\n",
    "        \"terms_to_find\": [\"cirrhosis\",\"hapatitis\",\"hep a\",\"hep b\",\"hep c\",\"hepatic\",\"jaundice\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"nausea_vomiting\",\n",
    "        \"terms_to_find\": [\"nausea\",\"vomiting\",\"vomitting\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"gu\",\n",
    "        \"terms_to_find\": [\"cystitis\",\"dysuria\",\"epididymitis\",\"genital infection\",\"herpes\",\"orchitis\",\"sexually transmitted infection\",\"urethritis\",\"urinary infection\",\"Urinary tract infection\",\"urogenital infection\",\"UTI\",\"bladder\",\"hematuria\",\"incontinence\",\"pelvic mass\",\"urinary disorder\",\"urinary retention\",\"urinary symptoms\",\"varicocele\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"infection\",\n",
    "        \"terms_to_find\": [\"cystitis\",\"dysuria\",\"epididymitis\",\"genital infection\",\"herpes\",\"orchitis\",\"sexually transmitted infection\",\"urethritis\",\"urinary infection\",\"Urinary tract infection\",\"urogenital infection\",\"UTI\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"gyn_women\",\n",
    "        \"terms_to_find\": [\"breast\",\"endometriosis\",\"fibroids\",\"gynecological\",\"hot flashes\",\"irregular cycle\",\"mastitis\",\"menopause\",\"menstrual\",\"ovarian\",\"ovary\",\"ovulation\",\"reproductive health\",\"uterine\",\"uterus\",\"vagina\",\"vaginal\",\"vaginitis\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"injury\",\n",
    "        \"terms_to_find\": [\"bite\",\"sting\",\"stinging\",\"cut\",\"wound\",\"injury\",\"blast\",\"burn\",\"fracture\",\"gunshot\",\"shot\",\"hemiplegia\",\"paralysis\",\"paraplegia\",\"quadriplegia\",\"fragments\",\"shrapnel\",\"sliver\",\"splinter\",\"traffic accident\",\"abrasion\",\"bruise\",\"bruising\",\"Concussion\",\"contusion\",\"falling\",\"knee rupture\",\"splint\",\"trauma\"],\n",
    "        \"terms_to_avoid\": [\"psychological trauma\",\"heartburn\"],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"injury\",\n",
    "        \"terms_to_find\": [\"ulcer\"],\n",
    "        \"terms_to_avoid\": [\"gastric\", \"stomach\", \"peptic\", \"intestinal\", \"duodenal\"],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"injury_neuro\",\n",
    "        \"terms_to_find\": [\"hemiplegia\",\"paralysis\",\"paraplegia\",\"quadriplegia\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"malnutrition\",\n",
    "        \"terms_to_find\": [\"delayed growth\",\"growth delay\",\"growth retardation\",\"short stature\",\"malnutrition\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"growth_delay\",\n",
    "        \"terms_to_find\": [\"delayed growth\",\"growth delay\",\"growth retardation\",\"short stature\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"mental_health\",\n",
    "        \"terms_to_find\": [\"anxiety\",\"bipolar\",\"mental illness\",\"personality disorder\",\"post traumatic syndrome\",\"post-traumatic syndrome\",\"psychiatric\",\"psychological\",\"ptsd\",\"schizophrenia\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"mental_health\",\n",
    "        \"terms_to_find\": [\"depression\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"neuro_complaint\",\n",
    "        \"terms_to_find\": [\"head pain\",\"headache\",\"cerebral accident\",\"cerebral hemorrhage\",\"cerebral infarction\",\"cerebral ischemia\",\"cerebrovascular accident\",\"cva\",\"stroke\",\"benign paroxysmal postitional vertigo\",\"brachial plexus\",\"brain infection\",\"brain tumor\",\"cauda equina\",\"cerebral palsy\",\"cervical root\",\"convulsion\",\"convulsions\",\"dementia\",\"dizziness\",\"encephalitis\",\"epilepsy\",\"epileptic\",\"foot drop\",\"hand drop\",\"meningitis\",\"meningocele\",\"migraine\",\"nerve\",\"neuritis\",\"neurodegenerative\",\"neurological\",\"neuropathy\",\"numbness\",\"nystagmus\",\"polyneuritis\",\"sciatica\",\"seizure\",\"subarachnoid hemorrhage\",\"TIA\",\"tinnitus\",\"Vertigo\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"other_infection\",\n",
    "        \"terms_to_find\": [\"mediterranean fever\",\"mf\",\"abcess\",\"abscess\",\"sepsis\",\"septic shock\",\"bacteremia\",\"brucellosis\",\"chickenpox\",\"diphtheria\",\"finger infection\",\"foot infection\",\"fungal\",\"hand foot\",\"hand infection\",\"hand mouth\",\"hand-foot\",\"hookworm\",\"infection of blood\",\"intestinal worms\",\"leprosy\",\"lymphadenitis\",\"lymphadenopathy\",\"measles\",\"nemotodes\",\"omphalitis\",\"parasite\",\"pinworm\",\"rheumatic fever\",\"rubella\",\"scarlet fever\",\"thrush\",\"toe infection\",\"worms\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"other_infection\",\n",
    "        \"terms_to_find\": [\"leishmania\",\"leishmaniasis\"],\n",
    "        \"terms_to_avoid\": [\"excluding leishmaniasis\", \"excluding leishmania\", \"except leishmaniasis\"],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"pregnancy\",\n",
    "        \"terms_to_find\": [\"abortion\",\"antenatal\",\"birth\",\"caesarean section\",\"csection\",\"delivery\",\"gestation\",\"miscarriage\",\"placenta\",\"postnatal\",\"postpartum\",\"pregnancy\",\"pregnant\",\"prenatal\"],\n",
    "        \"terms_to_avoid\": [\"not pregnant\"],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"renal\",\n",
    "        \"terms_to_find\": [\"hydronephrosis\",\"kidney cysts\",\"kidney failure\",\"kidney stone\",\"nephritis\",\"nephrolithiasis\",\"nephropathy\",\"pyelonephritis\",\"renal calculi\",\"renal calculus\",\"renal failure\",\"renal impairment\",\"renal insufficiency\",\"renal stones\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"respiratory\",\n",
    "        \"terms_to_find\": [\"laryngitis\",\"rhinitis\",\"sinusitis\",\"bronchiolitis\",\"bronchitis\",\"cold\",\"congestion\",\"cough\",\"croup\",\"flu\",\"grippe\",\"influenza\",\"penumonia\",\"pneumonia\",\"pneumonitis\",\"pulmonary infection\",\"respiratory infection\",\"respiratory tract infection\",\"rhinorrhea\",\"running nose\",\"runny nose\",\"tuberculosis\",\"urti\",\"asthma\",\"bronchospasm\",\"COPD\",\"difficulty breathing\",\"dyspnea\",\"emphysema\",\"hemoptysis\",\"lung disease\",\"nebulization\",\"nebulizing\",\"pulmonary disease\",\"pulmonary fibrosis\",\"shortness of breath\",\"sneezing\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"infection\",\n",
    "        \"terms_to_find\": [\"bronchiolitis\",\"bronchitis\",\"cold\",\"congestion\",\"cough\",\"croup\",\"flu\",\"grippe\",\"influenza\",\"penumonia\",\"pneumonia\",\"pneumonitis\",\"pulmonary infection\",\"respiratory infection\",\"respiratory tract infection\",\"rhinorrhea\",\"running nose\",\"runny nose\",\"tuberculosis\",\"urti\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"wound\",\n",
    "        \"terms_to_find\": [\"dressing change\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"animal_insect_bite\",\n",
    "        \"terms_to_find\": [\"bite\",\"sting\",\"stinging\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the rows we change here\n",
    "# so that we can update the table\n",
    "\n",
    "update_data = []\n",
    "\n",
    "# Iterate through the in-memory representation\n",
    "for rec in data:\n",
    "    # Create a placeholder update record\n",
    "    update_rec = {'id':rec['id']}\n",
    "    \n",
    "    # A flag we'll use to determine whether the record needs to be updated\n",
    "    update_record = False\n",
    "    \n",
    "    # Get the human_translate value from the record\n",
    "    ht = rec['human_translate']\n",
    "    \n",
    "    # If it is not None, then convert it to lowercase\n",
    "    if ht:\n",
    "        ht = ht.lower()\n",
    "    \n",
    "    # Get the google_translate value and convert it to lowercase\n",
    "    # We are not currently using this to generate flags so it is commented out\n",
    "    # but you could substitute it in below or write additional code if you want\n",
    "    # to use it for flag generation\n",
    "    gt = rec['google_translate_feb']\n",
    "    if gt:\n",
    "        gt = gt.lower()\n",
    "    \n",
    "    # Look at google_tokens_joined field\n",
    "    gtj = rec['google_tokens_joined']\n",
    "    if gtj:\n",
    "        gtj = gtj.lower()\n",
    "    \n",
    "    \n",
    "    # Walk through the different flag types from above and check whether the \n",
    "    # human_translate value matches for that flag. If so, create the update record\n",
    "    # for that flag and then mark our update boolean indicator true so that we know\n",
    "    # to update the appropriate record in the database. All records that will be updated\n",
    "    # have their update record put into the update_data list.\n",
    "    for term in flag_terms:\n",
    "        if (ht and term in ht) or (gt and term in gt) or (gtj and term in gtj):\n",
    "            update_rec[\"flag_\" + \"_\".join(term.replace(\"-\",\"_\").split())] = 1\n",
    "            update_record = True\n",
    "\n",
    "    for tup in multiple_flag_terms:\n",
    "        if (ht and all(x in ht for x in tup)) or (gt and all(x in gt for x in tup)) or (gtj and all(x in gtj for x in tup)):\n",
    "            update_rec[\"flag_\" + \"_and_\".join(tup)] = 1\n",
    "            update_record = True\n",
    "\n",
    "    for tup in synonym_flag_terms:\n",
    "        if (ht and any(x in ht for x in tup)) or (gt and any(x in gt for x in tup)) or (gtj and any(x in gtj for x in tup)):\n",
    "            update_rec[\"flag_\" + \"_\".join(tup[0].split())] = 1\n",
    "            update_record = True\n",
    "\n",
    "    for tup in complex_flag_terms:\n",
    "        if (ht and tup[0] in ht and not any(x in ht for x in tup[1:])) or (gt and tup[0] in gt and not any(x in gt for x in tup[1:])) or (gtj and tup[0] in gtj and not any(x in gtj for x in tup[1:])):\n",
    "            update_rec[\"flag_\" + tup[0].replace(\" \",\"_\").replace(\"-\",\"_\")] = 1\n",
    "            update_record = True\n",
    "\n",
    "    # complex_set_flag_terms\n",
    "    for rule in complex_set_flag_terms:\n",
    "        flag_name = \"flag_\" + \"_\".join(rule['flag_name'].split())\n",
    "\n",
    "        # Continue because we already set this flag\n",
    "        if flag_name in update_rec.keys():\n",
    "            if update_rec[flag_name] == 1:\n",
    "                continue\n",
    "\n",
    "        if \"ht\" in rule['check']:\n",
    "            if ht and any(x in ht for x in rule[\"terms_to_find\"]) and not any(x in ht for x in rule[\"terms_to_avoid\"]):\n",
    "                update_rec[flag_name] = 1\n",
    "                update_record = True\n",
    "                # We set the flag so stop searching\n",
    "                continue\n",
    "\n",
    "        if \"gt\" in rule['check']:\n",
    "            if gt and any(x in gt for x in rule[\"terms_to_find\"]) and not any(x in gt for x in rule[\"terms_to_avoid\"]):\n",
    "                update_rec[flag_name] = 1\n",
    "                update_record = True\n",
    "                # We set the flag so stop searching\n",
    "                continue\n",
    "\n",
    "            if gtj and any(x in gtj for x in rule[\"terms_to_find\"]) and not any(x in gtj for x in rule[\"terms_to_avoid\"]):\n",
    "                update_rec[flag_name] = 1\n",
    "                update_record = True\n",
    "                # We set the flag so stop searching\n",
    "                continue\n",
    "            \n",
    "    # Handle war-related separately. This very likely can be improved upon\n",
    "    if ht and 'war-related injury' in ht and 'not war-related injury' not in ht:\n",
    "        update_rec['flag_conflict_related'] = 1\n",
    "        update_record = True\n",
    "    \n",
    "    # If we created any flags, update_record is true so put this record in the list \n",
    "    # of records to update.\n",
    "    if update_record:\n",
    "        # Create comprehensive injury flag per Ranya's request\n",
    "        keys = update_rec.keys()\n",
    "        if ('flag_injury' in keys and update_rec['flag_injury'] == 1) or ('flag_wound' in keys and update_rec['flag_wound'] == 1):\n",
    "            update_rec['flag_comprehensive_injury'] = 1\n",
    "        else:\n",
    "            update_rec['flag_comprehensive_injury'] = 0\n",
    "                \n",
    "        update_data.append(update_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119891"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many records are we going to update in the arabic_values table?\n",
    "len(update_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'flag_comprehensive_injury': 0, 'flag_head': 1, 'id': 400108},\n",
       " {'flag_comprehensive_injury': 0, 'flag_pregnancy': 1, 'id': 400115},\n",
       " {'flag_comprehensive_injury': 0, 'flag_follow_up': 1, 'id': 400117},\n",
       " {'flag_comprehensive_injury': 0, 'flag_follow_up': 1, 'id': 400118},\n",
       " {'flag_comprehensive_injury': 0, 'flag_follow_up': 1, 'id': 400119},\n",
       " {'flag_comprehensive_injury': 0, 'flag_derm': 1, 'id': 400169},\n",
       " {'flag_comprehensive_injury': 0, 'flag_derm': 1, 'id': 400171},\n",
       " {'flag_comprehensive_injury': 0, 'flag_neck': 1, 'id': 400229},\n",
       " {'flag_comprehensive_injury': 0,\n",
       "  'flag_infection': 1,\n",
       "  'flag_respiratory': 1,\n",
       "  'id': 400269},\n",
       " {'flag_comprehensive_injury': 0, 'flag_derm': 1, 'id': 400307}]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What do the update records look like? \n",
    "update_data[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the arabic_values table with the update_records' data\n",
    "# 1. Create the columns we need\n",
    "# 2. Bulk update for each column\n",
    "\n",
    "flag_cols = set()\n",
    "for rec in update_data:\n",
    "    for k in rec.keys():\n",
    "        if k != 'id':\n",
    "            flag_cols.add(k)\n",
    "flag_cols = sorted(list(flag_cols))\n",
    "\n",
    "# The trick here is to get the id from a record in arabic values and update that\n",
    "# record with a None value for each of these flags - that will cause dataset to generate the columns\n",
    "ref_rec = tab_arabic_values.find_one()\n",
    "ref_rec_update = {'id':ref_rec['id']}\n",
    "for col in flag_cols:\n",
    "    ref_rec_update[col] = None\n",
    "tab_arabic_values.update(ref_rec_update, ['id'])\n",
    "\n",
    "# At this point maybe open DB Browser for SQLite to make sure the columns were created.\n",
    "# The 1 that prints below is the number of records updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now iterate through the flag cols and create a list of each record that needs to set the value for each\n",
    "# flag column and then bulk update. It is orders of magnitude faster to do it this way than one by one.\n",
    "\n",
    "# Note - this is generating and executing some super gnarly long SQL queries with tons of ID numbers\n",
    "\n",
    "for col in flag_cols:\n",
    "    recs_to_update = []\n",
    "    for rec in update_data:\n",
    "        if col in rec.keys():\n",
    "            recs_to_update.append(rec['id'])\n",
    "    recs_to_update = sorted(recs_to_update)\n",
    "\n",
    "    db.query(\"\"\"\n",
    "    UPDATE arabic_values\n",
    "    SET \"\"\" + col + \"\"\" = 1 \n",
    "    WHERE id IN (\"\"\" + \",\".join([str(a) for a in recs_to_update]) +\"\"\");\n",
    "    \"\"\")\n",
    "    \n",
    "# After this runs, check in the database against to make sure the flags were properly applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a new db connection again in case the schema has changed.\n",
    "# This probably isn't necessary but is a safety measure.\n",
    "\n",
    "try:\n",
    "    del db\n",
    "    del tab_arabic_values\n",
    "except:\n",
    "    pass\n",
    "\n",
    "db = dataset.connect(\"sqlite:///sams_data_phase22.sqlite\")\n",
    "tab_arabic_values = db['arabic_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a reference to the raw Arabic data table\n",
    "tab_raw_ar = db['full_raw_scrubbed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,a_file_id,a_files_sheets_id,a_sheet_id,acceptance_pattern,analysis,analysis_request,analysis_type,anesthesia_type,assign_method,case,category,center,clinic,clinical_case,col_1,col_2,col_3,col_4,col_5,col_6,col_misc,col_moawak,col_none,col_null,col_to,conflict_related,consultations,daily_number,data_validation,date,date_first_exam,death,death_cause,death_certificate,death_date,death_location,death_time,department,diagnosis,diagnosis_confirmed,discharge,discharge_date,discharge_status,discharged_to,disclaimers,disease,displaced,displacement_duration,dose,drug_class,er,events,exam_type,examination_1,examination_2,examination_3,examination_4,examination_type,facility_type,housing,housing_persons_number,image,image_request,image_type,import_status,info_age,info_card_number,info_card_type,info_care_type,info_geo_address,info_geo_area,info_geo_community,info_geo_country_of_origin,info_geo_district,info_geo_governorate,info_geo_injury_city,info_geo_injury_site,info_geo_injury_state,info_geo_jurisdiction,info_geo_living,info_geo_location,info_geo_region,info_geo_sub_district,info_name,info_name_author,info_name_caregiver,info_name_facility,info_name_group,info_name_of_coach,info_name_processor,info_name_surgeon,info_origin,info_phone_skype,info_residence,info_residence_of_relative,info_residence_original,info_sex,injury_type,link_type,medication,medication_discharge,medicine,month,months,muslim,new_listings,new_or_followup,notes,num_family_members,num_sessions,number,number_of_sessions,other,outcome,pathology,patient_number,patient_status,patient_type,period,post_op_results,procedure,radiation_type,rating,ratio,referral,referral_notes,referral_reason,referred_to,related_events,residence_status,sequence,session_number,sessions_patient_week,son_daughter,special_case,status,surgery_length,surgery_type,symptoms,time,total,transferred_from,treatment,treatment_code,type,units_of_blood,visit_reason,visit_review,visit_status,year,yes_no,flag_abdomen,flag_back,flag_blast,flag_blunt,flag_burn,flag_chest,flag_complication,flag_conflict_related,flag_explosive,flag_extremity,flag_facial,flag_follow_up,flag_fracture,flag_gunshot,flag_head,flag_history_of,flag_injury,flag_lower_extremity,flag_neck,flag_nerve,flag_neurologic,flag_orthopedic,flag_pelvic,flag_shrapnel,flag_spinal,flag_spine,flag_stab,flag_suspected,flag_traffic,flag_trauma,flag_upper_extremity,flag_urologic,flag_vascular,flag_wound,added,profile,malaria,disposition\n"
     ]
    }
   ],
   "source": [
    "# Get the list of variables used in full_raw_scrubbed and full_raw_english\n",
    "rec_raw = tab_raw_ar.find_one()\n",
    "variables = list(rec_raw.keys())\n",
    "print(\",\".join(variables))\n",
    "\n",
    "# Due to previous work, there are flag columns in the full_raw_scrubbed table, but we will ignore them\n",
    "# because they aren't used in this flag-generation methodology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the in-memory arabic_values lookup\n",
    "# This time, since we created the flags, they'll be in the records\n",
    "\n",
    "arabic_lookup = {}\n",
    "arabic_values = [x for x in tab_arabic_values.find()]\n",
    "\n",
    "for v in arabic_values:\n",
    "    arabic_lookup[v['arabic']] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "التهاب مجاري تنفسية سفلى, التهاب قصبات\n",
      "---------------------- Lookup result below\n",
      "OrderedDict([('id', 3863), ('arabic', 'التهاب مجاري تنفسية سفلى, التهاب قصبات'), ('google_translate', 'Inflammation of lower respiratory tracts, bronchitis'), ('human_translate', 'bronchiolitis, bronchitis'), ('normalized', None), ('appears_in', \"['diagnosis']\"), ('google_translate_feb', 'Inflammation of lower respiratory tracts, bronchitis'), ('google_tokens_joined', 'lower respiratory tract infection, bronchitis'), ('orig_value', None), ('added', None), ('flag_ENT', None), ('flag_abdomen', None), ('flag_abdominal_pain', None), ('flag_allergy', None), ('flag_anemia', None), ('flag_animal_insect_bite', None), ('flag_back', None), ('flag_blast', None), ('flag_bleed', None), ('flag_blunt', None), ('flag_burn', None), ('flag_cancer', None), ('flag_cardiovascular', None), ('flag_chest', None), ('flag_complication', None), ('flag_comprehensive_injury', '1'), ('flag_conflict_related', None), ('flag_congenital', None), ('flag_constipation', None), ('flag_dehydration', None), ('flag_dental_complaint', None), ('flag_derm', None), ('flag_diabetes', None), ('flag_diarrhea_dysentery', None), ('flag_endocrine', None), ('flag_explosive', None), ('flag_eye', None), ('flag_facial', None), ('flag_fatigue', None), ('flag_fever', None), ('flag_follow_up', None), ('flag_fracture', None), ('flag_gi_complaint', None), ('flag_growth_delay', None), ('flag_gu', None), ('flag_gunshot', None), ('flag_gyn_women', None), ('flag_head', None), ('flag_headache', None), ('flag_history_of', None), ('flag_hyperlipidemia', None), ('flag_infection', '1'), ('flag_injury', None), ('flag_injury_neuro', None), ('flag_liver_dysfunction', None), ('flag_lower_extremity', None), ('flag_malnutrition', None), ('flag_mental_health', None), ('flag_musculoskeletal_pain', None), ('flag_nausea_vomiting', None), ('flag_neck', None), ('flag_nerve', None), ('flag_neuro_complaint', None), ('flag_neurologic', None), ('flag_orthopedic', None), ('flag_other_infection', None), ('flag_pain', None), ('flag_pelvic', None), ('flag_pregnancy', None), ('flag_renal', None), ('flag_respiratory', '1'), ('flag_shrapnel', None), ('flag_spinal', None), ('flag_spine', None), ('flag_stab', None), ('flag_stroke', None), ('flag_suspected', None), ('flag_traffic_accident', None), ('flag_trauma', None), ('flag_upper_extremity', None), ('flag_urologic', None), ('flag_vascular', None), ('flag_wound', None)])\n"
     ]
    }
   ],
   "source": [
    "# Let's test that a value we pull out of the database has a hit in the lookup table.\n",
    "test_rec = tab_raw_ar.find_one()\n",
    "diagnosis = test_rec['diagnosis']\n",
    "print(diagnosis)\n",
    "print(\"---------------------- Lookup result below\")\n",
    "print(arabic_lookup[diagnosis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following takes a while to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# The insert_many method inserts in chunks of 1000, but this specifies that we don't want\n",
    "# to start the process until we have this many records to insert.\n",
    "buffer_size = 50000\n",
    "\n",
    "flags_to_insert = []\n",
    "\n",
    "try:\n",
    "    db['full_raw_flags'].drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tab_raw_flags = db['full_raw_flags']\n",
    "\n",
    "# Insert a dummy record to create the table\n",
    "dummy_record = {\n",
    "    'file_id':None,\n",
    "    'files_sheets_id':None,\n",
    "    'sheet_id':None\n",
    "}\n",
    "\n",
    "for flag in flag_cols:\n",
    "    dummy_record[flag] = None\n",
    "    \n",
    "tab_raw_flags.insert(dummy_record)\n",
    "print(tab_raw_flags.count())\n",
    "tab_raw_flags.delete()\n",
    "\n",
    "\n",
    "# Iterate through the raw records one by one\n",
    "for rec in tab_raw_ar.find():\n",
    "    \n",
    "    # Include foreign keys that allow us to query against the flag table instead of \n",
    "    # joining with the raw data table, which is slow.\n",
    "    flag_record = {\n",
    "        'id':rec['id'],\n",
    "        'file_id':rec['a_file_id'],\n",
    "        'files_sheets_id':rec['a_files_sheets_id'],\n",
    "        'sheet_id':rec['a_sheet_id']\n",
    "    }\n",
    "    \n",
    "    # Initialize each flag_record\n",
    "    for flag in flag_cols:\n",
    "        flag_record[flag] = None\n",
    "        \n",
    "    # Scan the conflict related column for values, but do this before looking at the\n",
    "    # corresponding Arabic values so that we don't overwrite the Arabic value setting.\n",
    "    if rec['conflict_related'] is not None:\n",
    "        if rec['conflict_related'].strip() == 'كبرى' or rec['conflict_related'].strip() =='كبرى':\n",
    "            flag_record['flag_conflict_related'] = 1\n",
    "        elif rec['conflict_related'].strip() == 'لا':\n",
    "            flag_record['flag_conflict_related'] = 0\n",
    "        else:\n",
    "            flag_record['flag_conflict_related'] = None\n",
    "    else:\n",
    "        flag_record['flag_conflict_related'] = None\n",
    "        \n",
    "    # Loop through the variables for each raw data record\n",
    "    for v in variables:\n",
    "        # These are obfuscated PII cols, or the flag columns we're ignoring, so skip them\n",
    "        if 'info_' in v or 'flag_' in v or v == 'id':\n",
    "            continue\n",
    "        \n",
    "        # Get the value in the column\n",
    "        to_lookup = rec[v]\n",
    "        \n",
    "        if to_lookup is None or to_lookup == '.':\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            # We have a legit value, so look it up and grab the flags\n",
    "            try:\n",
    "                # There might be a keyerror on the info_ columns' hashed values, etc.\n",
    "                # I also manually removed some PII from arabic_values, so that might\n",
    "                # cause an occassional mismatch.\n",
    "                arabic_values_rec = arabic_lookup[to_lookup]\n",
    "                for flag in flag_cols:\n",
    "                    # Should be None if not flagged, so just check for existence\n",
    "                    if arabic_values_rec[flag]:\n",
    "                        flag_record[flag] = arabic_values_rec[flag]\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Store the record\n",
    "    flags_to_insert.append(flag_record)\n",
    "\n",
    "    # Check if we need to insert\n",
    "    if len(flags_to_insert) > buffer_size:\n",
    "        tab_raw_flags.insert_many(flags_to_insert)\n",
    "        \n",
    "        # Clear the buffer\n",
    "        flags_to_insert.clear()\n",
    "        \n",
    "# We've been through all raw records so make sure the buffer is clear\n",
    "tab_raw_flags.insert_many(flags_to_insert)\n",
    "flags_to_insert.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We no longer have use for full_raw_english, so drop it\n",
    "\n",
    "tab_eng = db['full_raw_english']\n",
    "tab_eng.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Guarantee that we've cleaned up\n",
    "\n",
    "# Fix a few minor spelling errors in the facilities table\n",
    "\n",
    "result = db.query(\"\"\"\n",
    "UPDATE facilities SET district = 'Idlib' WHERE district = 'Idleb';\n",
    "\"\"\")\n",
    "\n",
    "result = db.query(\"\"\"\n",
    "UPDATE facilities SET subdistrict = 'Idlib' WHERE subdistrict = 'Idleb';\n",
    "\"\"\")\n",
    "\n",
    "result = db.query(\"\"\"\n",
    "UPDATE facilities SET district = 'Jisr Ash Shugar' WHERE district = 'Jisr-Ash-Shugur';\n",
    "\"\"\")\n",
    "\n",
    "result = db.query(\"\"\"\n",
    "UPDATE facilities SET subdistrict = 'Jisr Ash Shugar' WHERE subdistrict = 'Jisr-Ash-Shugur';\n",
    "\"\"\")\n",
    "\n",
    "result = db.query(\"\"\"\n",
    "UPDATE facilities SET district = 'Daraa' WHERE district = \"Dar'a\";\n",
    "\"\"\")\n",
    "\n",
    "result = db.query(\"\"\"\n",
    "UPDATE facilities SET subdistrict = 'Daraa' WHERE subdistrict = \"Dar'a\";\n",
    "\"\"\")\n",
    "\n",
    "result = db.query(\"\"\"\n",
    "UPDATE facilities SET district = 'Al Mara' WHERE district = \"Al Ma'ra\";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update to location information match on facility_code\n",
    "new_data = [\n",
    "    (\"SAMS002\",\"Abu Fadel\",\"Syria\",\"Daraa\",\"Daraa\",\"Dael \"),\n",
    "    (\"SAMS010\",\"Al Ehsan\",\"Syria\",\"Daraa\",\"Daraa\",\"Kherbet Ghazala\"),\n",
    "    (\"SAM011\",\"Al Ehsan Clinic\",\"Syria\",\"Daraa\",\"Daraa\",\"Kherbet Ghazala\"),\n",
    "    (\"SAMS010\",\"Al Ehsan RH\",\"Syria\",\"Daraa\",\"Daraa\",\"Kherbet Ghazala\"),\n",
    "    (\"SAMS013\",\"Al Hara\",\"Syria\",\"Daraa\",\"As-Sanamayn\",\"As-Sanamayn\"),\n",
    "    (\"SAMS015\",\"Al Herak\",\"Syria\",\"Daraa\",\"Izra\",\"Herak\"),\n",
    "    (\"SAMS015\",\"Al Herak RH\",\"Syria\",\"Daraa\",\"Izra\",\"Herak\"),\n",
    "    (\"SAMS017\",\"Al Jeza\",\"Syria\",\"Daraa\",\"Daraa\",\"Jizeh\"),\n",
    "    (\"SAMS017\",\"Al Jeza Clinic\",\"Syria\",\"Daraa\",\"Daraa\",\"Jizeh\"),\n",
    "    (\"SAMS024\",\"Al Msaifra\",\"Syria\",\"Daraa\",\"Daraa\",\"msaifra\"),\n",
    "    (\"SAMS027\",\"Al Noor\",\"Syria\",\"Daraa\",\"Daraa\",\"Daraa\"),\n",
    "    (\"SAMS030\",\"Al Rafed\",\"Syria\",\"Quneitra\",\"Quneitra\",\"Al-Khashniyyeh\"),\n",
    "    (\"SAMS030\",\"Al Rafed RH\",\"Syria\",\"Quneitra\",\"Quneitra\",\"Al-Khashniyyeh\"),\n",
    "    (\"SAMS030\",\"Al Rafed SAMS Clinic\",\"Lebanon\",\"Quneitra\",\"Quneitra\",\"Al-Khashniyyeh\"),\n",
    "    (\"SAMS032\",\"Al Redwan\",\"Syria\",\"Daraa\",\"Izraa\",\"Jasim\"),\n",
    "    (\"SAMS032\",\"Al Redwan Clinic\",\"Syria\",\"Daraa\",\"Izraa\",\"Jasim\"),\n",
    "    (\"SAMS035\",\"Al Salam Hospital\",\"Syria\",\"Daraa\",\"Daraa\",\"Daraa\"),\n",
    "    (\"SAMS036\",\"Al Salam Midwife\",\"Syria\",\"Daraa\",\"Daraa\",\"Daraa\"),\n",
    "    (\"SAMS037\",\"Al Yadudeh\",\"Syria\",\"Daraa\",\"Daraa\",\"Mzeireb\"),\n",
    "    (\"SAMS038\",\"Al Yaman\",\"Syria\",\"Rural Damascus\",\"Rural Damascus\",\"Kafr Batna\"),\n",
    "    (\"SAMS060\",\"Ankhal\",\"Syria\",\"Daraa\",\"As-Sanamayn\",\"As-Sanamayn\"),\n",
    "    (\"SAMS062\",\"Artificial Limbs Center (Farha)\",\"Syria\",\"Rural Damascus\",\"Rural Damascus\",\"Kafr Batna\"),\n",
    "    (\"SAMS068\",\"Beer Ajam\",\"Syria\",\"Quneitra\",\"Quneitra\",\"Quneitra\"),\n",
    "    (\"SAMS073\",\"Douma Dialysis\",\"Syria\",\"Rural Damascus\",\"Duma\",\"Duma\"),\n",
    "    (\"SAMS074\",\"Douma FH\",\"Syria\",\"Rural Damascus\",\"Duma\",\"Duma\"),\n",
    "    (\"SAMS075\",\"Douma OBGYN\",\"Syria\",\"Rural Damascus\",\"Duma\",\"Duma\"),\n",
    "    (\"SAMS076\",\"East Ghouta ICU\",\"Syria\",\"Rural Damascus\",\"Rural Damascus\",\"Kafr Batna\"),\n",
    "    (\"SAMS077\",\"Eissa Ajaj\",\"Syria\",\"Daraa\",\"Daraa\",\"Daraa\"),\n",
    "    (\"SAMS079\",\"Erbin FH\",\"Syria\",\"Rural Damascus\",\"Rural Damascus\",\"Rural Damascus\"),\n",
    "    (\"SAMS087\",\"Hit Med Point\",\"Syria\",\"Daraa\",\"Daraa\",\"Ash-Shajara\"),\n",
    "    (\"SAMS090\",\"Ibta RH\",\"Syria\",\"Daraa\",\"Daraa\",\"Dael\"),\n",
    "    (\"SAMS097\",\"Ishraqat Amal PSS\",\"Syria\",\"Rural Damascus\",\"Rural Damascus\",\"Rural Damascus\"),\n",
    "    (\"SAMS101\",\"Jassim\",\"Syria\",\"Daraa\",\"Izra\",\"Jasim\"),\n",
    "    (\"SAMS102\",\"Jbt Khashab\",\"Syria\",\"Quneitra\",\"Quneitra\",\"Khan Arnaba\"),\n",
    "    (\"SAMS105\",\"Jlein Med Point\",\"Syria\",\"Daraa\",\"Daraa\",\"Mzeireb\"),\n",
    "    (\"SAMS106\",\"Jobar Med Center\",\"Syria\",\"Rural Damascus\",\"Rural Damascus\",\"Kafr Batna\"),\n",
    "    (\"SAMS142\",\"Maaraba\",\"Syria\",\"Daraa\",\"Daraa\",\"Busra Esh-Sham\"),\n",
    "    (\"SAMS143\",\"Maaraba RH\",\"Syria\",\"Daraa\",\"Daraa\",\"Busra Esh-Sham\"),\n",
    "    (\"SAMS153\",\"Muwafeq Dakhl Alla \",\"Syria\",\"Daraa\",\"Izra\",\"Tassil\"),\n",
    "    (\"SAMS154\",\"Muwafeq Dakhl Alla Clinic\",\"Syria\",\"Daraa\",\"Izra\",\"Tassil\"),\n",
    "    (\"SAMS155\",\"Nabd Horan \",\"Syria\",\"Daraa\",\"Daraa\",\"Dael \"),\n",
    "    (\"SAMS156\",\"Nabd Horan Clinic\",\"Syria\",\"Daraa\",\"Daraa\",\"Dael \"),\n",
    "    (\"SAMS159\",\"Nawa\",\"Syria\",\"Daraa\",\"Izra\",\"Nawa\"),\n",
    "    (\"SAMS160\",\"Nawa Clinic\",\"Syria\",\"Daraa\",\"Izra\",\"Nawa\"),\n",
    "    (\"SAMS161\",\"Neonatal ICU-hamoria\",\"Syria\",\"Rural Damascus\",\"Rural Damascus\",\"Kafr Batna\"),\n",
    "    (\"SAMS162\",\"Nuaima\",\"Syria\",\"Daraa\",\"Daraa\",\"Daraa\"),\n",
    "    (\"SAMS173\",\"Rawan Birth Center\",\"Syria\",\"Daraa\",\"Daraa\",\"Ash-Shajara\"),\n",
    "    (\"SAMS175\",\"Sahm Al Jolan Clinic\",\"Syria\",\"Daraa\",\"Daraa\",\"Ash-Shajara\"),\n",
    "    (\"SAMS176\",\"Saida\",\"Syria\",\"Daraa\",\"Daraa\",\"Sayda\"),\n",
    "    (\"SAMS177\",\"Saida PSS\",\"Syria\",\"Daraa\",\"Daraa\",\"Sayda\"),\n",
    "    (\"SAMS185\",\"Sham \",\"Syria\",\"Rural Damascus\",\"Rural Damascus\",\"Arbin\"),\n",
    "    (\"SAMS186\",\"Shifa\",\"Jordan\",\"Rural Damascus\",\"Duma\",\"Duma\"),\n",
    "    (\"SAMS187\",\"Shifa Mobile hospital\",\"Jordan\",\"Rural Damascus\",\"Duma\",\"Duma\"),\n",
    "    (\"SAMS194\",\"Shohadaa Horan\",\"Syria\",\"Daraa\",\"Daraa\",\"Daraa\"),\n",
    "    (\"SAMS192\",\"Tafas Clinic\",\"Syria\",\"Daraa\",\"Daraa\",\"Mzeireb\"),\n",
    "    (\"SAMS193\",\"Tafas PSS\",\"Syria\",\"Daraa\",\"Daraa\",\"Mzeireb\"),\n",
    "    (\"SAMS195\",\"Tal Shehab\",\"Syria\",\"Daraa\",\"Daraa\",\"Mzeireb\"),\n",
    "    (\"SAMS198\",\"Tassil RH\",\"Syria\",\"Daraa\",\"Izra\",\"Tassil\"),\n",
    "    (\"SAMS201\",\"Um Walad\",\"Syria\",\"Daraa\",\"Daraa\",\"Mseifra\"),\n",
    "    (\"SAMS203\",\"Wadi Al Yarmouk\",\"Syria\",\"Daraa\",\"Daraa\",\"Ash-Shajara\"),\n",
    "    (\"SAMS204\",\"Wadi Al Yarmouk FH\",\"Syria\",\"Daraa\",\"Daraa\",\"Ash-Shajara\"),\n",
    "    (\"SAMS205\",\"White Hands PSS\",\"Syria\",\"Daraa\",\"Daraa\",\"Mzeireb\"),\n",
    "    (\"SAMS208\",\"Zayzun\",\"Syria\",\"Daraa\",\"Daraa\",\"Mzeireb\"),\n",
    "    (\"SAMS209\",\"Zayzun\",\"Syria\",\"Daraa\",\"Daraa\",\"Mzeireb\")\n",
    "]\n",
    "\n",
    "def make_query_string(t, field, index):\n",
    "    s = \"UPDATE facilities SET \" + field + \" ='\" + t[index] + \"' WHERE facility_code = '\" + t[0] + \"';\"\n",
    "    return s\n",
    "\n",
    "query_queue = []\n",
    "\n",
    "for facility in new_data:\n",
    "    query_queue.append(make_query_string(facility, \"country\", 2))\n",
    "    query_queue.append(make_query_string(facility, \"governorate\", 3))\n",
    "    query_queue.append(make_query_string(facility, \"district\", 4))\n",
    "    query_queue.append(make_query_string(facility, \"subdistrict\", 5))\n",
    "    \n",
    "for query in query_queue:\n",
    "    result = db.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_queue = [\n",
    "    \"UPDATE files SET facility_id = 124 WHERE id IN (594, 599, 747, 947);\",\n",
    "    \"UPDATE files SET facility_id = 71  WHERE id IN (42);\",\n",
    "    \"UPDATE files SET facility_id = 312 WHERE id IN (627, 1023);\",\n",
    "    \"UPDATE files SET facility_id = 119 WHERE id IN (635, 796, 884, 956, 1102);\",\n",
    "    \"UPDATE files SET facility_id = 88  WHERE id IN (668, 1039, 1041, 1042);\",\n",
    "    \"UPDATE files SET facility_id = 146 WHERE id IN (719, 874, 940);\",\n",
    "    \"UPDATE files SET facility_id = 118 WHERE id IN (806, 891, 964, 1034);\",\n",
    "    \"UPDATE files SET facility_id = 92  WHERE id IN (834);\",\n",
    "    \"UPDATE files SET facility_id = 129 WHERE id IN (1061, 1120);\",\n",
    "    \"UPDATE files SET facility_id = 3   WHERE id IN (981);\"\n",
    "]\n",
    "\n",
    "for query in query_queue:\n",
    "    result = db.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export full flags table to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This used to be a part of dataset but was extracted to its own library\n",
    "# https://github.com/pudo/datafreeze\n",
    "from datafreeze import freeze\n",
    "\n",
    "# Export database table to CSV\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can change this query to export a different set of data\n",
    "result = db.query(\"\"\"\n",
    "SELECT  files.id as files_id,\n",
    "        files.year,\n",
    "        files.month,\n",
    "        files.year || '-' || files.month || '-01' AS full_date,\n",
    "        facilities.id AS facility_id,\n",
    "        facilities.facility_parent_id,\n",
    "        facilities.facilityname,\n",
    "        facilities.country,\n",
    "        facilities.governorate,\n",
    "        facilities.district,\n",
    "        facilities.subdistrict,\n",
    "        facilities.facility_type,\n",
    "        full_raw_flags.flag_abdomen,\n",
    "        full_raw_flags.flag_abdominal_pain,\n",
    "        full_raw_flags.flag_allergy,\n",
    "        full_raw_flags.flag_anemia,\n",
    "        full_raw_flags.flag_animal_insect_bite,\n",
    "        full_raw_flags.flag_back,\n",
    "        full_raw_flags.flag_blast,\n",
    "        full_raw_flags.flag_bleed,\n",
    "        full_raw_flags.flag_blunt,\n",
    "        full_raw_flags.flag_burn,\n",
    "        full_raw_flags.flag_cancer,\n",
    "        full_raw_flags.flag_cardiovascular,\n",
    "        full_raw_flags.flag_chest,\n",
    "        full_raw_flags.flag_complication,\n",
    "        full_raw_flags.flag_conflict_related,\n",
    "        full_raw_flags.flag_congenital,\n",
    "        full_raw_flags.flag_constipation,\n",
    "        full_raw_flags.flag_dehydration,\n",
    "        full_raw_flags.flag_dental_complaint,\n",
    "        full_raw_flags.flag_derm,\n",
    "        full_raw_flags.flag_diabetes,\n",
    "        full_raw_flags.flag_diarrhea_dysentery,\n",
    "        full_raw_flags.flag_endocrine,\n",
    "        full_raw_flags.flag_ENT,\n",
    "        full_raw_flags.flag_explosive,\n",
    "        full_raw_flags.flag_eye,\n",
    "        full_raw_flags.flag_facial,\n",
    "        full_raw_flags.flag_fatigue,\n",
    "        full_raw_flags.flag_fever,\n",
    "        full_raw_flags.flag_follow_up,\n",
    "        full_raw_flags.flag_fracture,\n",
    "        full_raw_flags.flag_gi_complaint,\n",
    "        full_raw_flags.flag_growth_delay,\n",
    "        full_raw_flags.flag_gu,\n",
    "        full_raw_flags.flag_gunshot,\n",
    "        full_raw_flags.flag_gyn_women,\n",
    "        full_raw_flags.flag_head,\n",
    "        full_raw_flags.flag_headache,\n",
    "        full_raw_flags.flag_history_of,\n",
    "        full_raw_flags.flag_hyperlipidemia,\n",
    "        full_raw_flags.flag_infection,\n",
    "        full_raw_flags.flag_injury,\n",
    "        full_raw_flags.flag_injury_neuro,\n",
    "        full_raw_flags.flag_liver_dysfunction,\n",
    "        full_raw_flags.flag_lower_extremity,\n",
    "        full_raw_flags.flag_malnutrition,\n",
    "        full_raw_flags.flag_mental_health,\n",
    "        full_raw_flags.flag_musculoskeletal_pain,\n",
    "        full_raw_flags.flag_nausea_vomiting,\n",
    "        full_raw_flags.flag_neck,\n",
    "        full_raw_flags.flag_nerve,\n",
    "        full_raw_flags.flag_neuro_complaint,\n",
    "        full_raw_flags.flag_neurologic,\n",
    "        full_raw_flags.flag_orthopedic,\n",
    "        full_raw_flags.flag_other_infection,\n",
    "        full_raw_flags.flag_pain,\n",
    "        full_raw_flags.flag_pelvic,\n",
    "        full_raw_flags.flag_pregnancy,\n",
    "        full_raw_flags.flag_renal,\n",
    "        full_raw_flags.flag_respiratory,\n",
    "        full_raw_flags.flag_shrapnel,\n",
    "        full_raw_flags.flag_spinal,\n",
    "        full_raw_flags.flag_spine,\n",
    "        full_raw_flags.flag_stab,\n",
    "        full_raw_flags.flag_stroke,\n",
    "        full_raw_flags.flag_suspected,\n",
    "        full_raw_flags.flag_traffic_accident,\n",
    "        full_raw_flags.flag_trauma,\n",
    "        full_raw_flags.flag_upper_extremity,\n",
    "        full_raw_flags.flag_urologic,\n",
    "        full_raw_flags.flag_vascular,\n",
    "        full_raw_flags.flag_wound,\n",
    "        full_raw_flags.flag_comprehensive_injury\n",
    "\n",
    "FROM full_raw_flags\n",
    "JOIN files on files.id = full_raw_flags.file_id\n",
    "JOIN facilities on files.facility_id = facilities.id\n",
    "\n",
    "WHERE files.facility_id IS NOT NULL \n",
    "AND files.month IS NOT NULL\n",
    "AND files.skipped = 0\n",
    "AND files.ignore = 0;\n",
    "\"\"\")\n",
    "\n",
    "# This used to be a part of dataset but was extracted to its own library\n",
    "# https://github.com/pudo/datafreeze\n",
    "freeze(result, format='csv', filename='full_raw_flags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sams_data_phase22_output_2018-04-05.sqlite'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is optional and will generate a copy of the database that will be gigabytes in size.\n",
    "shutil.copy2(\"sams_data_phase22.sqlite\",'sams_data_phase22_output_2018-04-05.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
