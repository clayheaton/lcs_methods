{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw Arabic data (from processed files) is in the template database. The purpose of this notebook is to take the raw data, scrub it of PII, consolidate it to a polymorphic table for translation, and then send the remaining Arabic data through Google Translated for a rough version of the Arabic. \n",
    "\n",
    "Note that the translation process is expensive and should only be performed a single time on a reduced set of data. Therefore, this notebook should not be run many times.\n",
    "\n",
    "Also, it's possible that the raw data will need to be reimported after correcting the variable reduction. That means that the template that is loaded into Notebook 4 would need to have the normalized values in the variables table adjusted to produce a more sane consolidation of the data. The risk in this is that collapsing to too few columns will inevitably collapse disparate data points together into the same blob of data. A reason why this might be done would be to fix a prior consolidation that is incorrect to an issue where there are fields in the data that only contain several records with values entered into them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulate the file system\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Copy dictionaries\n",
    "import copy\n",
    "\n",
    "# work with dates\n",
    "import datetime\n",
    "import arrow\n",
    "\n",
    "# For scrubbing PII\n",
    "import hashlib\n",
    "\n",
    "# Convert stored string representation of a list to a list\n",
    "import ast\n",
    "\n",
    "# Recurse through a directory tree and return file names with glob\n",
    "import glob\n",
    "\n",
    "# Decode and re-encode mangled Arabic file names\n",
    "import codecs\n",
    "\n",
    "# Connect to a SQLite database in a lazy manner.\n",
    "import sqlalchemy\n",
    "import dataset\n",
    "\n",
    "# Enables opening and reading of Excel files\n",
    "import openpyxl\n",
    "\n",
    "# Translating variables, sheet names, and workbook names from Arabic\n",
    "# This is NOT free to use.\n",
    "from google.cloud import translate\n",
    "\n",
    "# Set the environment variable for the Google Service Account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\clay\\\\Documents\\\\fxb-lcs-2b24f4f8a73a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed template clone sams_data_phase05.sqlite\n",
      "Created database from template: sams_data_phase05.sqlite\n"
     ]
    }
   ],
   "source": [
    "#If there's an existing db for this sheet, delete it\n",
    "#so that we can copy from the template for a fresh start\n",
    "\n",
    "try:\n",
    "    os.remove(\"sams_data_phase05.sqlite\")\n",
    "    print(\"Removed template clone sams_data_phase05.sqlite\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Try to preserve a copy in case there is a problem and it has to be restored\n",
    "    shutil.copy2(\"sams_data_phase05_template.sqlite\",\"sams_data_phase05.sqlite\")\n",
    "    \n",
    "    print(\"Created database from template: sams_data_phase05.sqlite\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///sams_data_phase05.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrub the PII from the data\n",
    "\n",
    "Store the scrubbed data in a new table. Look into the full HIPAA compliance on scrubbing. \n",
    "\n",
    "Fields that might need to be scrubbed begin with `info_`. For now, only scrub name fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_raw = db['full_raw_data']\n",
    "tab_raw_scrubbed = db['full_raw_scrubbed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = []\n",
    "buffer_size = 10000\n",
    "\n",
    "# Do not save this value in a source code repository!\n",
    "salt = 'REDACTED'.encode()\n",
    "\n",
    "# Note, may need to include residence fields\n",
    "# depending on their level of specificity\n",
    "# info_residence\n",
    "# info_residence_of_relative\n",
    "# info_residence_original\n",
    "\n",
    "fields = [\n",
    "    \"info_name\",\n",
    "    \"info_name_author\",\n",
    "    \"info_name_caregiver\",\n",
    "    \"info_name_facility\",\n",
    "    \"info_name_group\",\n",
    "    \"info_name_of_coach\",\n",
    "    \"info_name_processor\",\n",
    "    \"info_name_surgeon\",\n",
    "    \"info_phone_skype\"\n",
    "]\n",
    "\n",
    "for rec in tab_raw.find():\n",
    "    for pii_field in fields:\n",
    "        if rec[pii_field] is None or rec[pii_field] == '.':\n",
    "            continue\n",
    "        else:\n",
    "            # Hash the value in the field\n",
    "            h = hashlib.sha256()\n",
    "            h.update(rec[pii_field].encode())\n",
    "            h.update(salt)\n",
    "            rec[pii_field] = h.hexdigest()\n",
    "            \n",
    "    buffer.append(rec)\n",
    "    \n",
    "    if len(buffer) > buffer_size:\n",
    "        tab_raw_scrubbed.insert_many(buffer)\n",
    "        buffer.clear()\n",
    "        \n",
    "# Catch the remaining records\n",
    "tab_raw_scrubbed.insert_many(buffer)\n",
    "buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the non-scrubbed raw data\n",
    "tab_raw.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sams_data_phase06_template.sqlite'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the database over as the template for the next file.\n",
    "# This Notebook did not include manual editing of the data.\n",
    "\n",
    "# Do not rerun this cell!\n",
    "# shutil.copy2('sams_data_phase05.sqlite','sams_data_phase06_template.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
