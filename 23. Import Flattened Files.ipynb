{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulate the file system\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import arrow\n",
    "\n",
    "import hashlib\n",
    "\n",
    "# Display errors in realtime\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import re\n",
    "\n",
    "# This used to be a part of dataset but was extracted to its own library\n",
    "# https://github.com/pudo/datafreeze\n",
    "from datafreeze import freeze\n",
    "\n",
    "# Export database table to CSV\n",
    "import csv\n",
    "\n",
    "# Copy dictionaries\n",
    "import copy\n",
    "\n",
    "# Convert stored string representation of a list to a list\n",
    "import ast\n",
    "\n",
    "# Recurse through a directory tree and return file names with glob\n",
    "import glob\n",
    "\n",
    "# Decode and re-encode mangled Arabic file names\n",
    "import codecs\n",
    "\n",
    "# Connect to a SQLite database in a lazy manner.\n",
    "import dataset\n",
    "import sqlalchemy\n",
    "\n",
    "# Enables opening and reading of Excel files\n",
    "import openpyxl\n",
    "\n",
    "# Translating variables, sheet names, and workbook names from Arabic\n",
    "# This is NOT free to use.\n",
    "from google.cloud import translate\n",
    "\n",
    "# Set the environment variable for the Google Service Account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\clay\\\\Documents\\\\fxb-lcs-2b24f4f8a73a.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and map the flat files\n",
    "\n",
    "Note that the source files for this previously were loaded into the `files` table in the database but that those entries need to be ignored in favor of these entries. \n",
    "\n",
    "These files mostly are in English with standardized values and do not require translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of the files to process\n",
    "flat_files = glob.glob(\"flattened/*.xls*\",recursive=True)\n",
    "flat_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If there's an existing db for this sheet, delete it\n",
    "#so that we can copy from the template for a fresh start\n",
    "new_db_name = \"sams_data_phase23.sqlite\"\n",
    "\n",
    "try:\n",
    "    os.remove(new_db_name)\n",
    "    print(\"Removed template clone \", sams_data_phase23.sqlite)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Try to preserve a copy in case there is a problem and it has to be restored\n",
    "    shutil.copy2(\"sams_data_phase22_output_2018-04-05.sqlite\",new_db_name)\n",
    "    \n",
    "    print(\"Created database from template: \", new_db_name)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///\" + new_db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mark the older imported files as ignore\n",
    "res = db.query(\"\"\" UPDATE files SET ignore = 1 WHERE added = '2018-04-03' AND sheet_names LIKE \"%['M1%\" OR sheet_names LIKE \"%['M3%\"; \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Provide some info for future reference\n",
    "res = db.query(\"\"\" UPDATE files SET info = 'flattened and parsed separately' WHERE added = '2018-04-03' AND sheet_names LIKE \"%['M1%\" OR sheet_names LIKE \"%['M3%\"; \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_facilities = db['facilities']\n",
    "tab_files = db['files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in flat_files:\n",
    "    file_name = f.split(\"\\\\\")[1]\n",
    "    sams_id = file_name.split(\"-\")[0]\n",
    "    year = file_name.split(\"-\")[1]\n",
    "    month = file_name.split(\"-\")[2]\n",
    "    \n",
    "    facility_id = tab_facilities.find_one(facility_code=sams_id)['id']\n",
    "    \n",
    "    wb = openpyxl.load_workbook(f, read_only=True)\n",
    "    sheets = wb.sheetnames\n",
    "    \n",
    "    file_rec = {\n",
    "        \"file_name\":file_name,\n",
    "        \"ungarbled\":None,\n",
    "        \"translation\":None,\n",
    "        \"path\":f,\n",
    "        \"country\":None,\n",
    "        \"year\":year,\n",
    "        \"month\":month,\n",
    "        \"num_sheets\":len(sheets),\n",
    "        \"sheet_names\":str(sheets),\n",
    "        \"info\":\"flattened versions of previously listed files\",\n",
    "        \"problem_opening\":None,\n",
    "        \"skipped\":0,\n",
    "        \"ignore\":0,\n",
    "        \"facility_id\":facility_id,\n",
    "        \"added\":\"2018-04-07\",\n",
    "        \"processed\":0\n",
    "    }\n",
    "    \n",
    "    tab_files.insert(file_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the join info about these files and sheets\n",
    "\n",
    "new_files = [rec for rec in db.query(\"SELECT * FROM files WHERE added = '2018-04-07';\")]\n",
    "\n",
    "tab_sheets = db['sheets']\n",
    "tab_files_sheets_join = db['files_sheets']\n",
    "\n",
    "sheets_lookup = {rec['name']:rec['id'] for rec in tab_sheets.find()}\n",
    "\n",
    "sheet_set = set()\n",
    "\n",
    "for rec in new_files:\n",
    "    sheets = ast.literal_eval(rec['sheet_names'])\n",
    "    for s in sheets:\n",
    "        sheet_set.add(s)\n",
    "        \n",
    "for sheet_name in sheet_set:\n",
    "    if sheet_name not in sheets_lookup.keys():\n",
    "        new_rec = {\"name\":sheet_name,\"added\":\"2018-04-07\", \"skip\":0, \"normalized\":\"data\"}\n",
    "        tab_sheets.insert(new_rec)\n",
    "        print(new_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheets_lookup = {rec['name']:rec['id'] for rec in tab_sheets.find()}\n",
    "\n",
    "join_records = []\n",
    "\n",
    "for rec in new_files:\n",
    "    for sheet in ast.literal_eval(rec['sheet_names']):\n",
    "        sheet_id = sheets_lookup[sheet]\n",
    "        join_rec = {\n",
    "            \"file_id\":rec['id'],\n",
    "            \"sheet_id\":sheet_id,\n",
    "            \"header_start\":None,\n",
    "            \"header_end\":None,\n",
    "            \"header_values\":None,\n",
    "            \"added\":\"2018-04-07\"\n",
    "        }\n",
    "        join_records.append(join_rec)\n",
    "\n",
    "# Bulk inserts are faster than individual inserts\n",
    "tab_files_sheets_join.insert_many(join_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recs_to_process = db.query(\"\"\"\n",
    "SELECT files_sheets.id AS files_sheets_id, files_sheets.file_id, files_sheets.sheet_id, files.path AS file_path,sheets.name AS sheet_name\n",
    "FROM files_sheets\n",
    "JOIN files ON files_sheets.file_id = files.id\n",
    "JOIN sheets ON files_sheets.sheet_id = sheets.id\n",
    "WHERE sheets.skip = 0\n",
    "  AND files_sheets.added = '2018-04-07'\n",
    "ORDER BY file_id, sheet_id;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def headers_from_worksheet(workbook,worksheet_name):\n",
    "    worksheet = workbook.get_sheet_by_name(worksheet_name)\n",
    "    winning_row_values = 0\n",
    "    winning_row_number = None\n",
    "    \n",
    "    for row in range(1,21):\n",
    "        start_range = 'A' + str(row)\n",
    "        end_range = 'Z' + str(row)\n",
    "        cells = worksheet[start_range:end_range]\n",
    "        try:\n",
    "            values = [c.value for c in cells[0]]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        filled_cells = 0\n",
    "        for v in values:\n",
    "            if v is not None:\n",
    "                filled_cells += 1\n",
    "        \n",
    "        if filled_cells > winning_row_values:\n",
    "            winning_row_values = filled_cells\n",
    "            winning_row_number = row\n",
    "            \n",
    "    # Some sheets may be blank\n",
    "    if winning_row_number is None:\n",
    "        return None,None\n",
    "            \n",
    "    winning_start = 'A' + str(winning_row_number)\n",
    "    winning_end = 'Z' + str(winning_row_number)\n",
    "\n",
    "    header_cells = worksheet[winning_start:winning_end]\n",
    "    header_data = [c.value for c in header_cells[0]]\n",
    "    \n",
    "    # If we detect a datetime.datetime.object, then we probably\n",
    "    # want the previous row. Might be a better way to check this\n",
    "    \n",
    "    # What's the actual start column of the header?\n",
    "    start_idx = 0\n",
    "    determined_start = False\n",
    "    \n",
    "    for val in header_data:\n",
    "        if not determined_start and val is None:\n",
    "            start_idx += 1\n",
    "        elif val is not None:\n",
    "            determined_start = True\n",
    "            \n",
    "        if isinstance(val,datetime.datetime):\n",
    "            winning_row_number -= 1\n",
    "            \n",
    "    header_start_letter = letter_lookup[start_idx]\n",
    "            \n",
    "    winning_start = header_start_letter + str(winning_row_number)\n",
    "    winning_end = 'Z' + str(winning_row_number)\n",
    "    try:\n",
    "        header_cells = worksheet[winning_start:winning_end]\n",
    "        header_data = [c.value for c in header_cells[0]]\n",
    "    except:\n",
    "        return None,None\n",
    "    \n",
    "    end_idx = len(header_data) - 1\n",
    "    problem = ''\n",
    "#     print(end_idx)\n",
    "#     print(header_data)\n",
    "#     print(header_data[end_idx],\"\\n\")\n",
    "    \n",
    "    while header_data[end_idx] is None:\n",
    "        end_idx -= 1\n",
    "        if end_idx <= start_idx:\n",
    "            problem = ' (PROBLEM)'\n",
    "            break\n",
    "            \n",
    "    # Lookup assumes that the header starts with col A, so offset the lookup on the\n",
    "    # end letter by the start letter index and it will assign the proper letter to the\n",
    "    # end letter.\n",
    "    end_letter = letter_lookup[end_idx+start_idx]  \n",
    "    header_end = end_letter + winning_start[1:] + problem\n",
    "    header_range = (winning_start,header_end)\n",
    "    \n",
    "    # Prune the header_data to get rid of trailing None values\n",
    "    prune_by = 0\n",
    "    \n",
    "    while header_data[prune_by-1] is None:\n",
    "        prune_by -= 1\n",
    "        \n",
    "    try:\n",
    "        header_data = header_data[:prune_by]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return header_range, header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letter_lookup = ['A','B','C','D','E','F','G','H','I','J','K',\n",
    "                 'L','M','N','O','P','Q','R','S','T','U','V',\n",
    "                 'W','X','Y','Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename this ref b/c the old code did\n",
    "tab_files_sheets = db['files_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables = set()\n",
    "\n",
    "working_file_id = -1\n",
    "active_file_path = None\n",
    "active_workbook = None\n",
    "\n",
    "for rec in recs_to_process:\n",
    "    \n",
    "    # This only fires with a new file_id\n",
    "    if rec['file_id'] > working_file_id:\n",
    "        working_file_id = rec['file_id']\n",
    "        active_file_path = rec['file_path']\n",
    "        try:\n",
    "            active_workbook = openpyxl.load_workbook(active_file_path,read_only=True,guess_types=False,data_only=True)\n",
    "        except:\n",
    "            print(\"Unable to open\",active_file_path)\n",
    "            active_workbook = None\n",
    "            active_file_path = None\n",
    "            working_file_id = -1\n",
    "            \n",
    "    # Process the active file\n",
    "    sheet_name = rec['sheet_name']\n",
    "    header_range, header_data = headers_from_worksheet(active_workbook,sheet_name)\n",
    "    \n",
    "    # Unable to find a header in this sheet. Mark the record\n",
    "    if header_range is None:\n",
    "        update_rec = {\"id\":rec['files_sheets_id'],\"header_start\":\"PROBLEM\"}\n",
    "        tab_files_sheets.update(update_rec,['id'])\n",
    "        print(rec['files_sheets_id'],\"Problem workbook\",active_file_path,\"--> sheet -->\",sheet_name)\n",
    "        continue\n",
    "    else:\n",
    "        header_start = header_range[0]\n",
    "        header_end = header_range[1]\n",
    "        \n",
    "        fixed_header_data = []\n",
    "        for value in header_data:\n",
    "            if isinstance(value,datetime.datetime):\n",
    "                fixed_value = arrow.get(value).format(\"YYYY-MM-DD\")\n",
    "                fixed_header_data.append(fixed_value)\n",
    "                variables.add(fixed_value)\n",
    "            else:\n",
    "                fixed_header_data.append(value)\n",
    "                variables.add(value)\n",
    "        \n",
    "        update_rec = {\n",
    "            \"id\":rec['files_sheets_id'],\n",
    "            \"header_start\":header_start,\n",
    "            \"header_end\":header_end,\n",
    "            \"header_values\":str(fixed_header_data)\n",
    "        }\n",
    "        \n",
    "        tab_files_sheets.update(update_rec,['id'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_vars = db['variables']\n",
    "var_lookup = {rec['orig']:rec['id'] for rec in tab_vars.find()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that we create the set() variables two cells above, when looking for headers\n",
    "\n",
    "for v in variables:\n",
    "    if v not in var_lookup.keys():\n",
    "        new_var_rec = {'orig':v, 'translation':v, 'normalized': v, 'added':'2018-04-07' }\n",
    "        tab_vars.insert(new_var_rec)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fix_vars = {\n",
    "    \"injury_city\":\"info_geo_injury_city\",\n",
    "    \"injury_site\":\"injury_site\",\n",
    "    \"phone_skype\":\"info_phone_skype\",\n",
    "    \"sex\":\"info_sex\",\n",
    "    \"age\":\"info_age\",\n",
    "    \"name\":\"info_name\",\n",
    "    \"injury_state\":\"info_geo_injury_state\",\n",
    "    \"admission_date\":\"date_admission\",\n",
    "    \n",
    "}\n",
    "\n",
    "for k,v in fix_vars.items():\n",
    "    db.query(\"UPDATE variables SET normalized = '\" + v + \"' WHERE normalized = '\" + k + \"';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_lookup = {rec['orig']:rec['id'] for rec in tab_vars.find()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_vars = db['files_variables']\n",
    "tab_sheets_vars = db['sheets_variables']\n",
    "tab_files_sheets_vars = db['files_sheets_vars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_vars_set = set()\n",
    "sheets_vars_set = set()\n",
    "files_sheets_vars_set = set()\n",
    "\n",
    "recs_to_process = [rec for rec in db.query(\"SELECT * from files_sheets WHERE added = '2018-04-07';\")]\n",
    "\n",
    "for rec in recs_to_process:\n",
    "    header_values = rec['header_values']\n",
    "    if header_values is None:\n",
    "        continue\n",
    "    \n",
    "    header_values = ast.literal_eval(rec['header_values'])\n",
    "    \n",
    "    for header in header_values:\n",
    "        if header is None:\n",
    "            continue\n",
    "        try:\n",
    "            var_id = var_lookup[str(header)]    \n",
    "            file_id = rec['file_id']\n",
    "            sheet_id = rec['sheet_id']\n",
    "            files_sheets_id = rec['id']\n",
    "        except:\n",
    "            print(\"problem with\",header)\n",
    "            continue\n",
    "        \n",
    "        files_vars_set.add((file_id,var_id))\n",
    "        files_sheets_vars_set.add((files_sheets_id,var_id))\n",
    "        sheets_vars_set.add((sheet_id,var_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_vars_recs = []\n",
    "\n",
    "for rec_tuple in files_vars_set:\n",
    "    rec = {\"file_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_files_vars_recs.append(rec)\n",
    "    \n",
    "tab_files_vars.insert_many(tab_files_vars_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_sheets_vars_recs = []\n",
    "\n",
    "for rec_tuple in sheets_vars_set:\n",
    "    rec = {\"sheet_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_sheets_vars_recs.append(rec)\n",
    "    \n",
    "tab_sheets_vars.insert_many(tab_sheets_vars_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_files_sheets_vars_recs = []\n",
    "\n",
    "for rec_tuple in files_sheets_vars_set:\n",
    "    rec = {\"files_sheets_id\":rec_tuple[0],\"var_id\":rec_tuple[1]}\n",
    "    tab_files_sheets_vars_recs.append(rec)\n",
    "    \n",
    "tab_files_sheets_vars.insert_many(tab_files_sheets_vars_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the flat file data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheets_to_process = db.query(\"\"\"\n",
    "SELECT files_sheets.id AS files_sheets_id, \n",
    "       files_sheets.file_id, \n",
    "       files_sheets.sheet_id, \n",
    "       files_sheets.header_start, \n",
    "       files_sheets.header_end,\n",
    "       files_sheets.header_values,\n",
    "       files.path AS file_path,\n",
    "       sheets.name AS sheet_name\n",
    "FROM files_sheets\n",
    "JOIN files ON files_sheets.file_id = files.id\n",
    "JOIN sheets ON files_sheets.sheet_id = sheets.id\n",
    "WHERE sheets.skip = 0\n",
    "  AND files_sheets.added = '2018-04-07'\n",
    "\n",
    "AND files.ignore = 0\n",
    "AND files_sheets.header_start IS NOT NULL\n",
    "AND files_sheets.header_start <> 'PROBLEM'\n",
    "AND files_sheets.header_end NOT LIKE '%PROBLEM%'\n",
    "ORDER BY file_id, files_sheets_id;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_all = db['full_raw_scrubbed']\n",
    "tab_vars = db['variables']\n",
    "tab_files_sheets = db['files_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to create some columns...\n",
    "raw_columns = [k for k in tab_all.find_one().keys()]\n",
    "flag_cols = [c for c in raw_columns if 'flag_' in c]\n",
    "col_names_we_need = sorted(list(set([r['normalized'] for r in tab_vars.find()])))\n",
    "missing_col_names = set(col_names_we_need) - set(raw_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the new columns\n",
    "for c in missing_col_names:\n",
    "    tab_all.create_column(c, sqlalchemy.String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename var_lookup to work with code below\n",
    "\n",
    "# Create an in-memory lookup table for variables\n",
    "var_lookup = {}\n",
    "for r in tab_vars.find():\n",
    "    var_lookup[r['orig']] = r['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only reopen files when necessary\n",
    "working_file_id = -1\n",
    "active_file_path = None\n",
    "active_workbook = None\n",
    "\n",
    "\n",
    "for rec in sheets_to_process:\n",
    "    import_status = \"\"\n",
    "    \n",
    "    if rec['file_id'] > working_file_id:\n",
    "        working_file_id = rec['file_id']\n",
    "        active_file_path = rec['file_path']\n",
    "        try:\n",
    "            active_workbook = openpyxl.load_workbook(active_file_path,read_only=True,guess_types=False,data_only=True)\n",
    "        except:\n",
    "            import_status = \"Unable to open file\"\n",
    "            import_status = \"imported\"\n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])\n",
    "            print(\"Unable to open\",active_file_path)\n",
    "            \n",
    "            active_workbook = None\n",
    "            active_file_path = None\n",
    "            working_file_id = -1\n",
    "            continue\n",
    "            \n",
    "    # Process the active file\n",
    "    sheet_name = rec['sheet_name']\n",
    "    header_start = rec['header_start']\n",
    "    header_end = rec['header_end']\n",
    "    \n",
    "    # Unable to find a header in this sheet. Mark the record\n",
    "    if header_start is None or header_end is None or \"PROBLEM\" in header_start or \"PROBLEM\" in header_end:\n",
    "        import_status = \"skipped\"\n",
    "        continue\n",
    "    else:\n",
    "        worksheet = active_workbook.get_sheet_by_name(sheet_name)\n",
    "        last_row = worksheet.max_row\n",
    "        \n",
    "        # Sometimes worksheet.max_row doesn't return a value\n",
    "        if last_row == None:\n",
    "            import_status = \"imported: last row None\"\n",
    "            last_row = 10000\n",
    "        \n",
    "        data_start = header_start[0] + str(int(header_start[1:])+1)\n",
    "        data_end = header_end[0] + str(last_row)\n",
    "        data_range_string = data_start + \":\" + data_end\n",
    "        \n",
    "        # These are stored as a list converted to a string. Convert back to a list for enumeration\n",
    "        header_values = ast.literal_eval(rec['header_values'])\n",
    "        sheet_data = []\n",
    "        \n",
    "        try:\n",
    "            for datarow in worksheet[data_range_string]:\n",
    "                record = {}\n",
    "                for idx,cell in enumerate(datarow):\n",
    "                    header_val = header_values[idx]\n",
    "\n",
    "                    # Get the normalized value\n",
    "                    header_val = var_lookup[header_val]\n",
    "\n",
    "                    cell_value = cell.value                \n",
    "\n",
    "                    try:\n",
    "                        cell_value = cell_value.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    # Cannot write datetime objects to the database\n",
    "                    # Unless they are first converted to strings\n",
    "\n",
    "                    if isinstance(cell_value,datetime.datetime):\n",
    "                        cell_value = openpyxl.utils.datetime.datetime_to_W3CDTF(cell_value)\n",
    "                    elif isinstance(cell_value,datetime.time):\n",
    "                        cell_value = str(cell_value)\n",
    "                    elif cell_value is not None:\n",
    "                        cell_value = str(cell_value)\n",
    "\n",
    "                    # There's already a value in the field and it should be a string.\n",
    "                    # If it is our representation of none, replace it\n",
    "                    if header_val in record.keys():\n",
    "                        if record[header_val] == '.' or record[header_val] is None:\n",
    "                            record[header_val] = cell_value\n",
    "                        elif cell_value is not None:\n",
    "                            record[header_val] = record[header_val] + \", \" + cell_value\n",
    "                    else:\n",
    "\n",
    "                        # Blank strings instead of NULL will help us know which fields were available for the record\n",
    "                        if cell_value is None:\n",
    "                            record[header_val] = '.'\n",
    "                        else:\n",
    "                            record[header_val] = cell_value\n",
    "\n",
    "                sheet_data.append(record)\n",
    "\n",
    "            # Remove from sheet_data blank rows\n",
    "            rich_sheet_data = []\n",
    "            for staged in sheet_data:\n",
    "                working_copy = copy.deepcopy(staged)\n",
    "                try:\n",
    "                    # Put \"passover columns\" here. They will be removed from the record\n",
    "                    # before it is evaluated as \"empty.\" Number is a good example because there are sheets\n",
    "                    # where somebody dragged numbers down a column in preparation for a lot of data but \n",
    "                    # never actually used all of the numbered rows\n",
    "                    del working_copy['number']\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                if all((x == None or x == '.' or x == '') for x in list(working_copy.values())):\n",
    "                    continue\n",
    "                else:\n",
    "                    staged['a_file_id'] = rec['file_id']\n",
    "                    staged['a_files_sheets_id'] = rec['files_sheets_id']\n",
    "                    staged['a_sheet_id'] = rec['sheet_id']\n",
    "                    staged['added'] = '2018-04-07'\n",
    "\n",
    "                    rich_sheet_data.append(staged)\n",
    "\n",
    "            # Try to perform a bulk insert\n",
    "            tab_all.insert_many(rich_sheet_data)\n",
    "\n",
    "            # Update the status of the worksheet\n",
    "            import_status = \"imported\"\n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])\n",
    "            \n",
    "        except Exception as ex:\n",
    "            \n",
    "            print(\"\\n--------------------------------------------------------------------------\")\n",
    "            print(ex)\n",
    "            print(\"Failure\")\n",
    "            print(\"file_id\",rec['file_id'],\"files_sheets_id\",rec['files_sheets_id'],\"sheet_id\",rec['sheet_id'])\n",
    "            print(active_file_path,sheet_name)\n",
    "            print(\"Header range:\",header_start,header_end)\n",
    "            print(\"Data range:\",data_start,data_end)\n",
    "            print(\"data_range_string\",data_range_string)\n",
    "            \n",
    "            tab_files_sheets.update({\"id\":rec['files_sheets_id'],\"import_status\":import_status},[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrub PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_update = {}\n",
    "\n",
    "# Do not save this value in a source code repository!\n",
    "salt = 'REDACTED'.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"info_name\",\n",
    "    \"info_name_author\",\n",
    "    \"info_name_caregiver\",\n",
    "    \"info_name_facility\",\n",
    "    \"info_name_group\",\n",
    "    \"info_name_of_coach\",\n",
    "    \"info_name_processor\",\n",
    "    \"info_name_surgeon\",\n",
    "    \"info_phone_skype\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rec in tab_all.find(added='2018-04-07'):\n",
    "    for pii_field in fields:\n",
    "        if rec[pii_field] is None or rec[pii_field] == '.':\n",
    "            continue\n",
    "        else:\n",
    "            # Hash the value in the field\n",
    "            h = hashlib.sha256()\n",
    "            h.update(rec[pii_field].encode())\n",
    "            h.update(salt)\n",
    "            \n",
    "            if rec['id'] not in to_update.keys():\n",
    "                to_update[rec['id']] = {'id':rec['id']}\n",
    "\n",
    "            to_update[rec['id']][pii_field] = h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in to_update.keys():\n",
    "    tab_all.update(to_update[k],['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to arabic_values\n",
    "\n",
    "NOTE: these are English values but we are putting them in the table nonetheless. However, we do not need to tokenize or translate them.\n",
    "\n",
    "Be careful here because arabic_values no longer auto-increments the id, so it has to be set manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_raw = db['full_raw_scrubbed']\n",
    "tab_arabic = db['arabic_values']\n",
    "tab_vars = db['variables']\n",
    "\n",
    "column_names = db.query(\"SELECT DISTINCT(normalized) FROM variables;\")\n",
    "column_names = sorted([r['normalized'] for r in column_names])\n",
    "\n",
    "# We don't want to work with the values in the fields that have been hashed,\n",
    "# so remove them from the list of variables to query.\n",
    "fields = [\n",
    "    \"info_name\",\n",
    "    \"info_name_author\",\n",
    "    \"info_name_caregiver\",\n",
    "    \"info_name_facility\",\n",
    "    \"info_name_group\",\n",
    "    \"info_name_of_coach\",\n",
    "    \"info_name_processor\",\n",
    "    \"info_name_surgeon\",\n",
    "    \"info_phone_skype\",\n",
    "    \"date\",\n",
    "    \"date_first_exam\",\n",
    "    \"death_date\",\n",
    "    \"date_admission\"\n",
    "]\n",
    "column_names = [e for e in column_names if e not in fields]\n",
    "\n",
    "arabic_lookup = set([r['arabic'] for r in tab_arabic.find()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_id_results = db.query(\"SELECT max(id) FROM arabic_values;\")\n",
    "for r in max_id_results:\n",
    "    max_id = int(r['max(id)'])\n",
    "\n",
    "current_id = max_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = []\n",
    "for col in column_names:\n",
    "    col_values = db.query(\"\"\"\n",
    "        SELECT DISTINCT([\"\"\" + col + \"\"\"]) \n",
    "        FROM full_raw_scrubbed \n",
    "        WHERE added = '2018-04-07'\n",
    "        AND [\"\"\" + col + \"\"\"] IS NOT NULL\n",
    "        AND [\"\"\" + col + \"\"\"] <> '.'\n",
    "        AND [\"\"\" + col + \"\"\"] <> '';\n",
    "        \"\"\")\n",
    "    col_values = [r[col] for r in col_values]\n",
    "\n",
    "    # Create a table of unique Arabic values\n",
    "    for v in col_values:\n",
    "        if v in arabic_lookup:\n",
    "            continue\n",
    "        # Skip numbers\n",
    "        if v.replace(\",\",\".\").replace('.','',1).isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            r = {\"id\":current_id,\"arabic\":v,\"added\":'2018-04-07'}\n",
    "            current_id += 1\n",
    "            buffer.append(r)\n",
    "            arabic_lookup.add(v)\n",
    "            \n",
    "# 367621\n",
    "\n",
    "tab_arabic.insert_many(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set human_translate = to Arabic for the records just loaded.\n",
    "\n",
    "update_recs = []\n",
    "\n",
    "for rec in tab_arabic.find(added='2018-04-07'):\n",
    "    ur = {'id':rec['id'],'human_translate':rec['arabic']}\n",
    "    update_recs.append(ur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in update_recs:\n",
    "    tab_arabic.update(r,['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flag Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a reference to the arabic_values table\n",
    "tab_arabic_values = db['arabic_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    db['full_raw_flags'].drop()\n",
    "    print(\"Dropped full_raw_flags\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    db['full_raw_flags_reduced'].drop()\n",
    "    print(\"Dropped full_raw_flags_reduced\").drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# One SQLite limitation is you cannot drop columns, so you have to create a new table and then rename it.\n",
    "preserve_fields = [k for k in tab_arabic_values.find_one().keys() if 'flag_' not in k]\n",
    "\n",
    "# We don't use result but assigning it skips printing some garbage below\n",
    "result = db.query(\"\"\"\n",
    "CREATE TABLE new_arabic_values AS \n",
    "    SELECT \"\"\" + \",\".join(preserve_fields) + \"\"\" \n",
    "    FROM arabic_values;\n",
    "\"\"\")\n",
    "\n",
    "# Drop the original arabic_values table\n",
    "tab_arabic_values.drop()\n",
    "\n",
    "# Rename new_arabic_values to arabic_values & now we have a table with no flag columns\n",
    "result = db.query(\"\"\"\n",
    "ALTER TABLE new_arabic_values RENAME TO arabic_values;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now because we futzed with the arabic_values table, we have to create a new reference to the database\n",
    "# and to our arabic_values table. The db object stores some schema information that isn't updated with\n",
    "# our direct query calls above.\n",
    "\n",
    "del db\n",
    "del tab_arabic_values\n",
    "\n",
    "db = dataset.connect(\"sqlite:///\" + new_db_name)\n",
    "tab_arabic_values = db['arabic_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now create an in-memory representation of the arabic_values table\n",
    "# and store it in variable `data`\n",
    "data = [x for x in tab_arabic_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update this if you want to change what flags you are making on the dataset.\n",
    "# The logic for creating them is in the following cell.\n",
    "\n",
    "# Require and flag term\n",
    "flag_terms = [\n",
    "    \"blunt\",\n",
    "    \"explosive\",\n",
    "    \"blast\",\n",
    "    \"stab\",\n",
    "    \"upper extremity\",\n",
    "    \"lower extremity\",\n",
    "    \"neck\",\n",
    "    \"chest\",\n",
    "    \"back\",\n",
    "    \"spinal\",\n",
    "    \"neurologic\",\n",
    "    \"nerve\",\n",
    "    \"vascular\",\n",
    "    \"orthopedic\",\n",
    "    \"fracture\",\n",
    "    \"suspected\",\n",
    "    \"follow-up\",\n",
    "    \"complication\",\n",
    "    \"history of\",\n",
    "    \"traffic accident\"\n",
    "]\n",
    "\n",
    "# require all terms - not in use at the moment\n",
    "multiple_flag_terms = [\n",
    "#     (\"burn\",\"fracture\")\n",
    "]\n",
    "\n",
    "# require any of the terms but name the flag after the first\n",
    "synonym_flag_terms = [\n",
    "    (\"allergy\", \"allergic\"),\n",
    "    (\"anemia\", \"thalassemia\"),\n",
    "    (\"cancer\", \"bcc\", \"leukemia\", \"lymphoma\", \"malignancy\", \"malignant\", \"scc\"),\n",
    "    (\"cardiovascular\",\" asd \",\" vsd \",\"cholesterol\",\"hypercholesterolemia\",\"hyperlipidemia\",\"hypertriglyceridemia\",\"triglycerides\",\"blood pressure\",\" bp \",\"high blood pressure\",\"hypertension\",\"acute coronary syndrome\",\"angina\",\"arrhythmia\",\"atrial fibrillation\",\" avr \",\"cardiac ischemia\",\"chest pain\",\"clot\",\"clotting\",\"coronary atery\",\"coronary heart disease\",\"coronary ischemia\",\"dvt\",\"endocarditis\",\"heart attack\",\"heart disease\",\"heart failure\",\"heart valve\",\"hf\",\"hypotension\",\"ihd\",\" mi \",\"mitral valve prolapse\",\"mvr\",\"myocardial hypoperfusion\",\"myocardial infarction\",\"palpitations\",\"pericarditis\",\"pulmonary embolism\",\"pvd\",\"svt\",\"thromboembolism\",\"thrombophlebitis\",\"thrombosis\",\"vasculitis\"),\n",
    "    (\"congenital\", \"asd\", \"vsd\"),\n",
    "    (\"dehydration\", \"dehydration\", \"hypovolemic shock\"),\n",
    "    (\"dental complaint\", \"dental\", \"gingivitis\", \" gum \", \"odonitis\", \"teeth\", \"tooth\", \"toothache\"),\n",
    "    (\"derm\", \"acne\",\"alopecia\",\"blisters\",\"cellulitis\",\"dermatitis\",\"dermoid\",\"dry skin\",\"eczema\",\"folliculitis\",\"hair loss\",\"inflammatory papules\",\"intertrigo\",\"itch\",\"lice\",\"pruritis\",\"psoriasis\",\"rash\",\"ringworm\",\"scabies\",\"skin disease\",\"skin disorder\",\"skin eruption\",\"skin infection\",\"skin lesion\",\"tinea\",\"warts\"),\n",
    "    (\"diabetes\",\"diabetic\",\"DKA\",\"glucose\",\"hyperglycemia\",\"hypoglycemia\",\"sugar\"),\n",
    "    (\"endocrine\",\"hyperthyroid\",\"hyperthyroidism\",\"hypocalcemia\",\"hypothyroid\",\"hypothyroidism\",\"parathyroid\",\"thyroid\",\" TSH \"),\n",
    "    (\"infection\",\"conjunctivitis\",\"eye discharge\",\"eye infection\",\"keratoconjunctivitis\",\"ophthalmic infection\"),\n",
    "    (\"pain\", \"corneal inflammation\", \"eye sensitivity\", \"keratitis\", \"pain in the eye\"),\n",
    "    (\"fatigue\", \"exhaustion\", \"tired\", \"tiredness\"),\n",
    "    (\"fever\", \"hyperthermia\", \"temperature\"),\n",
    "    (\"constipation\", \"intestinal stasis\"),\n",
    "    (\"shrapnel\", \"fragments\",\"sliver\",\"splinter\"),\n",
    "    (\"musculoskeletal pain\",\"ankylosing spondylitis\",\"arthralgia\",\"Arthritis\",\"back pain\",\"bruise\",\"bruising\",\"chondritis\",\"contusion\",\"costochondritis\",\"disc herniation\",\"disc herniation\",\"discitis\",\"elbow pain\",\"extremity pain\",\"gout\",\"inflammation of the shoulder\",\"joint\",\"knee degeneration\",\"knee inflammation\",\"knee pain\",\"loin pain\",\"low back pain\",\"lumbar pain\",\"musclar pain\",\"Muscle spasm\",\"muscular pain\",\"myalgia\",\"myositis\",\"neck pain\",\"osteoarthritis\",\"osteomyelitis\",\"osteomylitis\",\"plantar fasciitis\",\"polyarthritis\",\"rheumatism\",\"sacroiliitis\",\"spine degeneration\",\"sprain\",\"strain\",\"synovitis\",\"tendinitis\",\"tendonitis\",\"tendonopathy\",\"tmj\"),\n",
    "    (\"headache\", \"head pain\"),\n",
    "    (\"stroke\",\"cerebral accident\",\"cerebral hemorrhage\",\"cerebral infarction\",\"cerebral ischemia\",\"cerebrovascular accident\",\" cva \"),\n",
    "    (\"gunshot\", \" shot \"),\n",
    "    \n",
    "    # Prior flags, preserved\n",
    "    (\"facial\",\"face\"),\n",
    "    (\"pelvic\",\"pelvis\"),\n",
    "    (\"head\",\"eye\",\"ear\",\"face\",\"brain\",\"scalp\",\"mouth\",\"nose\"),\n",
    "    (\"spine\",\"spinal\"),\n",
    "    (\"abdomen\",\"abdominal\")\n",
    "]\n",
    "\n",
    "# require the first term and the absence of the remaining terms\n",
    "# name the flag after the first term.\n",
    "complex_flag_terms = [\n",
    "    (\"urologic\",\"neurologic\"),\n",
    "    (\"burn\",\"heartburn\"),\n",
    "    (\"trauma\", \"psychological trauma\")\n",
    "]\n",
    "\n",
    "# Look for any of the terms in terms_to_find but only apply if terms in terms_to_avoid are absent.\n",
    "# Check human or google translation (ht, gt)\n",
    "\n",
    "complex_set_flag_terms = [\n",
    "    {\n",
    "        \"flag_name\": \"hyperlipidemia\",\n",
    "        \"terms_to_find\": [\"blood pressure\", \"bp\", \"high blood pressure\", \"hypertension\"],\n",
    "        \"terms_to_avoid\": [\"hypotension\"],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"ENT\",\n",
    "        \"terms_to_find\": [\"adenoiditis\",\"ear congestion\",\"ear discharge\",\"ear infection\",\"ear inflammation\",\"eustachian tube infection\",\"mucositis\",\"mumps\",\"nasal congestion\",\"nose congestion\",\"otitis\",\"otorrhea\",\"pharyngitis\",\"throat ache\",\"tonsillitis\",\"tonsils enlargement\",\"cerumen impaction\",\"dysphagia\",\"earache\",\"epistaxis\",\"hearing impairment\",\"hearing loss\",\"nasal obstruction\",\"pain in the ear\",\"pharyngeal pain\",\"pharynx pain\",\"swallowing pain\",\"vestibulitis\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"infection\",\n",
    "        \"terms_to_find\": [\"adenoiditis\",\"ear congestion\",\"ear discharge\",\"ear infection\",\"ear inflammation\",\"eustachian tube infection\",\"laryngitis\",\"mucositis\",\"mumps\",\"nasal congestion\",\"nose congestion\",\"otitis\",\"otorrhea\",\"pharyngitis\",\"rhinitis\",\"sinusitis\",\"throat ache\",\"tonsillitis\",\"tonsils enlargement\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"eye\",\n",
    "        \"terms_to_find\": [\"conjunctivitis\",\"eye discharge\",\"eye infection\",\"keratoconjunctivitis\",\"ophthalmic infection\",\"corneal inflammation\",\"eye sensitivity\",\"keratitis\",\"pain in the eye\",\"blepharitis\",\"cataract\",\"eye redness\",\"eyelid\",\"eye-redness\",\"glaucoma\",\"left eye\",\"my eye\",\"npdr\",\"pterygium\",\"pupil\",\"redness of the eye\",\"retinal\",\"retinopathy\",\"right eye\",\"swelling of the eye\",\"uveitis\",\"vision\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"gi_complaint\",\n",
    "        \"terms_to_find\": [\"abdominal injury\",\"apendicitis\",\"appendicitis\",\"belly pain\",\"bile duct obstruction\",\"bile stones\",\"cholecystitis\",\"colic\",\"colitis\",\"colon spasm\",\"Crohn\",\"duodenal ulcer\",\"enteritis\",\"epigastric pain\",\"flank pain\",\"gallbladder inflammation\",\"gastric pain\",\"gastric ulcer\",\"gastritis\",\"gastroenteritis\",\"gastrointestinal infection\",\"hiatal hernia\",\"ibd\",\"ibs\",\"indigestion\",\"inflammation of the stomach\",\"intestinal pain\",\"intestinal ulcer\",\"pain in the stomach\",\"pancreatitis\",\"peptic ulcer\",\"peritoneal inflammation\",\"peritonitis\",\"sore stomach\",\"stomach hurts\",\"stomach pain\",\"Digestive bleed\",\"Gastric bleeding\",\"Gastric hemorrhage\",\"Gastrointestinal bleeding\",\"hemorrhoids\",\"Ulcer of the colon\",\"Constipation\",\"intestinal stasis\",\"diarrhea\",\"dysentery\",\"food poisoning\",\"giardia\",\"typhoid\",\"cirrhosis\",\"hapatitis\",\"hep a\",\"hep b\",\"hep c\",\"hepatic\",\"jaundice\",\"nausea\",\"vomiting\",\"vomitting\",\"anal fissure\",\"bloating\",\"celiac disease\",\"esophageal reflux\",\"gastroesophageal reflux\",\"gerd\",\"heartburn\",\"inguinal hernia\",\"malabsorption\",\"umbilical fistula\",\"umbilical hernia\"],\n",
    "        \"terms_to_avoid\": [\"renal colic\"],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"abdominal_pain\",\n",
    "        \"terms_to_find\": [\"abdominal injury\",\"apendicitis\",\"appendicitis\",\"belly pain\",\"bile duct obstruction\",\"bile stones\",\"cholecystitis\",\"colic\",\"colitis\",\"colon spasm\",\"Crohn\",\"duodenal ulcer\",\"enteritis\",\"epigastric pain\",\"flank pain\",\"gallbladder inflammation\",\"gastric pain\",\"gastric ulcer\",\"gastritis\",\"gastroenteritis\",\"gastrointestinal infection\",\"hiatal hernia\",\"ibd\",\"ibs\",\"indigestion\",\"inflammation of the stomach\",\"intestinal pain\",\"intestinal ulcer\",\"pain in the stomach\",\"pancreatitis\",\"peptic ulcer\",\"peritoneal inflammation\",\"peritonitis\",\"sore stomach\",\"stomach hurts\",\"stomach pain\"],\n",
    "        \"terms_to_avoid\": [\"renal colic\"],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"bleed\",\n",
    "        \"terms_to_find\": [\"Digestive bleed\",\"Gastric bleeding\",\"Gastric hemorrhage\",\"Gastrointestinal bleeding\",\"hemorrhoids\",\"Ulcer of the colon\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"diarrhea_dysentery\",\n",
    "        \"terms_to_find\": [\"diarrhea\",\"dysentery\",\"food poisoning\",\"giardia\",\"typhoid\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"liver_dysfunction\",\n",
    "        \"terms_to_find\": [\"cirrhosis\",\"hapatitis\",\"hep a\",\"hep b\",\"hep c\",\"hepatic\",\"jaundice\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"nausea_vomiting\",\n",
    "        \"terms_to_find\": [\"nausea\",\"vomiting\",\"vomitting\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"gu\",\n",
    "        \"terms_to_find\": [\"cystitis\",\"dysuria\",\"epididymitis\",\"genital infection\",\"herpes\",\"orchitis\",\"sexually transmitted infection\",\"urethritis\",\"urinary infection\",\"Urinary tract infection\",\"urogenital infection\",\"UTI\",\"bladder\",\"hematuria\",\"incontinence\",\"pelvic mass\",\"urinary disorder\",\"urinary retention\",\"urinary symptoms\",\"varicocele\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"infection\",\n",
    "        \"terms_to_find\": [\"cystitis\",\"dysuria\",\"epididymitis\",\"genital infection\",\"herpes\",\"orchitis\",\"sexually transmitted infection\",\"urethritis\",\"urinary infection\",\"Urinary tract infection\",\"urogenital infection\",\"UTI\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"gyn_women\",\n",
    "        \"terms_to_find\": [\"breast\",\"endometriosis\",\"fibroids\",\"gynecological\",\"hot flashes\",\"irregular cycle\",\"mastitis\",\"menopause\",\"menstrual\",\"ovarian\",\"ovary\",\"ovulation\",\"reproductive health\",\"uterine\",\"uterus\",\"vagina\",\"vaginal\",\"vaginitis\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"injury\",\n",
    "        \"terms_to_find\": [\"bite\",\"sting\",\"stinging\",\"cut\",\"wound\",\"injury\",\"blast\",\"burn\",\"fracture\",\"gunshot\",\"shot\",\"hemiplegia\",\"paralysis\",\"paraplegia\",\"quadriplegia\",\"fragments\",\"shrapnel\",\"sliver\",\"splinter\",\"traffic accident\",\"abrasion\",\"bruise\",\"bruising\",\"Concussion\",\"contusion\",\"falling\",\"knee rupture\",\"splint\",\"trauma\"],\n",
    "        \"terms_to_avoid\": [\"psychological trauma\",\"heartburn\"],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"injury\",\n",
    "        \"terms_to_find\": [\"ulcer\"],\n",
    "        \"terms_to_avoid\": [\"gastric\", \"stomach\", \"peptic\", \"intestinal\", \"duodenal\"],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"injury_neuro\",\n",
    "        \"terms_to_find\": [\"hemiplegia\",\"paralysis\",\"paraplegia\",\"quadriplegia\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"malnutrition\",\n",
    "        \"terms_to_find\": [\"delayed growth\",\"growth delay\",\"growth retardation\",\"short stature\",\"malnutrition\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"growth_delay\",\n",
    "        \"terms_to_find\": [\"delayed growth\",\"growth delay\",\"growth retardation\",\"short stature\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\",\"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"mental_health\",\n",
    "        \"terms_to_find\": [\"anxiety\",\"bipolar\",\"mental illness\",\"personality disorder\",\"post traumatic syndrome\",\"post-traumatic syndrome\",\"psychiatric\",\"psychological\",\"ptsd\",\"schizophrenia\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"mental_health\",\n",
    "        \"terms_to_find\": [\"depression\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"neuro_complaint\",\n",
    "        \"terms_to_find\": [\"head pain\",\"headache\",\"cerebral accident\",\"cerebral hemorrhage\",\"cerebral infarction\",\"cerebral ischemia\",\"cerebrovascular accident\",\"cva\",\"stroke\",\"benign paroxysmal postitional vertigo\",\"brachial plexus\",\"brain infection\",\"brain tumor\",\"cauda equina\",\"cerebral palsy\",\"cervical root\",\"convulsion\",\"convulsions\",\"dementia\",\"dizziness\",\"encephalitis\",\"epilepsy\",\"epileptic\",\"foot drop\",\"hand drop\",\"meningitis\",\"meningocele\",\"migraine\",\"nerve\",\"neuritis\",\"neurodegenerative\",\"neurological\",\"neuropathy\",\"numbness\",\"nystagmus\",\"polyneuritis\",\"sciatica\",\"seizure\",\"subarachnoid hemorrhage\",\"TIA\",\"tinnitus\",\"Vertigo\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"other_infection\",\n",
    "        \"terms_to_find\": [\"mediterranean fever\",\"mf\",\"abcess\",\"abscess\",\"sepsis\",\"septic shock\",\"bacteremia\",\"brucellosis\",\"chickenpox\",\"diphtheria\",\"finger infection\",\"foot infection\",\"fungal\",\"hand foot\",\"hand infection\",\"hand mouth\",\"hand-foot\",\"hookworm\",\"infection of blood\",\"intestinal worms\",\"leprosy\",\"lymphadenitis\",\"lymphadenopathy\",\"measles\",\"nemotodes\",\"omphalitis\",\"parasite\",\"pinworm\",\"rheumatic fever\",\"rubella\",\"scarlet fever\",\"thrush\",\"toe infection\",\"worms\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"other_infection\",\n",
    "        \"terms_to_find\": [\"leishmania\",\"leishmaniasis\"],\n",
    "        \"terms_to_avoid\": [\"excluding leishmaniasis\", \"excluding leishmania\", \"except leishmaniasis\"],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"pregnancy\",\n",
    "        \"terms_to_find\": [\"abortion\",\"antenatal\",\"birth\",\"caesarean section\",\"csection\",\"delivery\",\"gestation\",\"miscarriage\",\"placenta\",\"postnatal\",\"postpartum\",\"pregnancy\",\"pregnant\",\"prenatal\"],\n",
    "        \"terms_to_avoid\": [\"not pregnant\"],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"renal\",\n",
    "        \"terms_to_find\": [\"hydronephrosis\",\"kidney cysts\",\"kidney failure\",\"kidney stone\",\"nephritis\",\"nephrolithiasis\",\"nephropathy\",\"pyelonephritis\",\"renal calculi\",\"renal calculus\",\"renal failure\",\"renal impairment\",\"renal insufficiency\",\"renal stones\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"respiratory\",\n",
    "        \"terms_to_find\": [\"laryngitis\",\"rhinitis\",\"sinusitis\",\"bronchiolitis\",\"bronchitis\",\"cold\",\"congestion\",\"cough\",\"croup\",\"flu\",\"grippe\",\"influenza\",\"penumonia\",\"pneumonia\",\"pneumonitis\",\"pulmonary infection\",\"respiratory infection\",\"respiratory tract infection\",\"rhinorrhea\",\"running nose\",\"runny nose\",\"tuberculosis\",\"urti\",\"asthma\",\"bronchospasm\",\"COPD\",\"difficulty breathing\",\"dyspnea\",\"emphysema\",\"hemoptysis\",\"lung disease\",\"nebulization\",\"nebulizing\",\"pulmonary disease\",\"pulmonary fibrosis\",\"shortness of breath\",\"sneezing\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"infection\",\n",
    "        \"terms_to_find\": [\"bronchiolitis\",\"bronchitis\",\"cold\",\"congestion\",\"cough\",\"croup\",\"flu\",\"grippe\",\"influenza\",\"penumonia\",\"pneumonia\",\"pneumonitis\",\"pulmonary infection\",\"respiratory infection\",\"respiratory tract infection\",\"rhinorrhea\",\"running nose\",\"runny nose\",\"tuberculosis\",\"urti\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"wound\",\n",
    "        \"terms_to_find\": [\"dressing change\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    },\n",
    "    {\n",
    "        \"flag_name\": \"animal_insect_bite\",\n",
    "        \"terms_to_find\": [\"bite\",\"sting\",\"stinging\"],\n",
    "        \"terms_to_avoid\": [],\n",
    "        \"check\": [\"ht\", \"gt\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the rows we change here\n",
    "# so that we can update the table\n",
    "\n",
    "update_data = []\n",
    "\n",
    "# Iterate through the in-memory representation\n",
    "for rec in data:\n",
    "    # Create a placeholder update record\n",
    "    update_rec = {'id':rec['id']}\n",
    "    \n",
    "    # A flag we'll use to determine whether the record needs to be updated\n",
    "    update_record = False\n",
    "    \n",
    "    # Get the human_translate value from the record\n",
    "    ht = rec['human_translate']\n",
    "    \n",
    "    # If it is not None, then convert it to lowercase\n",
    "    if ht:\n",
    "        ht = ht.lower()\n",
    "    \n",
    "    # Get the google_translate value and convert it to lowercase\n",
    "    # We are not currently using this to generate flags so it is commented out\n",
    "    # but you could substitute it in below or write additional code if you want\n",
    "    # to use it for flag generation\n",
    "    gt = rec['google_translate_feb']\n",
    "    if gt:\n",
    "        gt = gt.lower()\n",
    "    \n",
    "    # Look at google_tokens_joined field\n",
    "    gtj = rec['google_tokens_joined']\n",
    "    if gtj:\n",
    "        gtj = gtj.lower()\n",
    "    \n",
    "    \n",
    "    # Walk through the different flag types from above and check whether the \n",
    "    # human_translate value matches for that flag. If so, create the update record\n",
    "    # for that flag and then mark our update boolean indicator true so that we know\n",
    "    # to update the appropriate record in the database. All records that will be updated\n",
    "    # have their update record put into the update_data list.\n",
    "    for term in flag_terms:\n",
    "        if (ht and term in ht) or (gt and term in gt) or (gtj and term in gtj):\n",
    "            update_rec[\"flag_\" + \"_\".join(term.replace(\"-\",\"_\").split())] = 1\n",
    "            update_record = True\n",
    "\n",
    "    for tup in multiple_flag_terms:\n",
    "        if (ht and all(x in ht for x in tup)) or (gt and all(x in gt for x in tup)) or (gtj and all(x in gtj for x in tup)):\n",
    "            update_rec[\"flag_\" + \"_and_\".join(tup)] = 1\n",
    "            update_record = True\n",
    "\n",
    "    for tup in synonym_flag_terms:\n",
    "        if (ht and any(x in ht for x in tup)) or (gt and any(x in gt for x in tup)) or (gtj and any(x in gtj for x in tup)):\n",
    "            update_rec[\"flag_\" + \"_\".join(tup[0].split())] = 1\n",
    "            update_record = True\n",
    "\n",
    "    for tup in complex_flag_terms:\n",
    "        if (ht and tup[0] in ht and not any(x in ht for x in tup[1:])) or (gt and tup[0] in gt and not any(x in gt for x in tup[1:])) or (gtj and tup[0] in gtj and not any(x in gtj for x in tup[1:])):\n",
    "            update_rec[\"flag_\" + tup[0].replace(\" \",\"_\").replace(\"-\",\"_\")] = 1\n",
    "            update_record = True\n",
    "\n",
    "    # complex_set_flag_terms\n",
    "    for rule in complex_set_flag_terms:\n",
    "        flag_name = \"flag_\" + \"_\".join(rule['flag_name'].split())\n",
    "\n",
    "        # Continue because we already set this flag\n",
    "        if flag_name in update_rec.keys():\n",
    "            if update_rec[flag_name] == 1:\n",
    "                continue\n",
    "\n",
    "        if \"ht\" in rule['check']:\n",
    "            if ht and any(x in ht for x in rule[\"terms_to_find\"]) and not any(x in ht for x in rule[\"terms_to_avoid\"]):\n",
    "                update_rec[flag_name] = 1\n",
    "                update_record = True\n",
    "                # We set the flag so stop searching\n",
    "                continue\n",
    "\n",
    "        if \"gt\" in rule['check']:\n",
    "            if gt and any(x in gt for x in rule[\"terms_to_find\"]) and not any(x in gt for x in rule[\"terms_to_avoid\"]):\n",
    "                update_rec[flag_name] = 1\n",
    "                update_record = True\n",
    "                # We set the flag so stop searching\n",
    "                continue\n",
    "\n",
    "            if gtj and any(x in gtj for x in rule[\"terms_to_find\"]) and not any(x in gtj for x in rule[\"terms_to_avoid\"]):\n",
    "                update_rec[flag_name] = 1\n",
    "                update_record = True\n",
    "                # We set the flag so stop searching\n",
    "                continue\n",
    "            \n",
    "    # Handle war-related separately. This very likely can be improved upon\n",
    "    if ht and 'war-related injury' in ht and 'not war-related injury' not in ht:\n",
    "        update_rec['flag_conflict_related'] = 1\n",
    "        update_record = True\n",
    "    \n",
    "    # If we created any flags, update_record is true so put this record in the list \n",
    "    # of records to update.\n",
    "    if update_record:\n",
    "        # Create comprehensive injury flag per Ranya's request\n",
    "        keys = update_rec.keys()\n",
    "        if ('flag_injury' in keys and update_rec['flag_injury'] == 1) or ('flag_wound' in keys and update_rec['flag_wound'] == 1):\n",
    "            update_rec['flag_comprehensive_injury'] = 1\n",
    "        else:\n",
    "            update_rec['flag_comprehensive_injury'] = 0\n",
    "                \n",
    "        update_data.append(update_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many records are we going to update in the arabic_values table?\n",
    "len(update_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What do the update records look like? \n",
    "update_data[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the arabic_values table with the update_records' data\n",
    "# 1. Create the columns we need\n",
    "# 2. Bulk update for each column\n",
    "\n",
    "flag_cols = set()\n",
    "for rec in update_data:\n",
    "    for k in rec.keys():\n",
    "        if k != 'id':\n",
    "            flag_cols.add(k)\n",
    "flag_cols = sorted(list(flag_cols))\n",
    "\n",
    "# The trick here is to get the id from a record in arabic values and update that\n",
    "# record with a None value for each of these flags - that will cause dataset to generate the columns\n",
    "ref_rec = tab_arabic_values.find_one()\n",
    "ref_rec_update = {'id':ref_rec['id']}\n",
    "for col in flag_cols:\n",
    "    ref_rec_update[col] = None\n",
    "tab_arabic_values.update(ref_rec_update, ['id'])\n",
    "\n",
    "# At this point maybe open DB Browser for SQLite to make sure the columns were created.\n",
    "# The 1 that prints below is the number of records updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now iterate through the flag cols and create a list of each record that needs to set the value for each\n",
    "# flag column and then bulk update. It is orders of magnitude faster to do it this way than one by one.\n",
    "\n",
    "# Note - this is generating and executing some super gnarly long SQL queries with tons of ID numbers\n",
    "\n",
    "for col in flag_cols:\n",
    "    recs_to_update = []\n",
    "    for rec in update_data:\n",
    "        if col in rec.keys():\n",
    "            recs_to_update.append(rec['id'])\n",
    "    recs_to_update = sorted(recs_to_update)\n",
    "\n",
    "    db.query(\"\"\"\n",
    "    UPDATE arabic_values\n",
    "    SET \"\"\" + col + \"\"\" = 1 \n",
    "    WHERE id IN (\"\"\" + \",\".join([str(a) for a in recs_to_update]) +\"\"\");\n",
    "    \"\"\")\n",
    "    \n",
    "# After this runs, check in the database against to make sure the flags were properly applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a new db connection again in case the schema has changed.\n",
    "# This probably isn't necessary but is a safety measure.\n",
    "\n",
    "try:\n",
    "    del db\n",
    "    del tab_arabic_values\n",
    "except:\n",
    "    pass\n",
    "\n",
    "db = dataset.connect(\"sqlite:///\" + new_db_name)\n",
    "tab_arabic_values = db['arabic_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a reference to the raw Arabic data table\n",
    "tab_raw_ar = db['full_raw_scrubbed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the list of variables used in full_raw_scrubbed and full_raw_english\n",
    "rec_raw = tab_raw_ar.find_one()\n",
    "variables = list(rec_raw.keys())\n",
    "print(\",\".join(variables))\n",
    "\n",
    "# Due to previous work, there are flag columns in the full_raw_scrubbed table, but we will ignore them\n",
    "# because they aren't used in this flag-generation methodology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the in-memory arabic_values lookup\n",
    "# This time, since we created the flags, they'll be in the records\n",
    "\n",
    "arabic_lookup = {}\n",
    "arabic_values = [x for x in tab_arabic_values.find()]\n",
    "\n",
    "for v in arabic_values:\n",
    "    arabic_lookup[v['arabic']] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's test that a value we pull out of the database has a hit in the lookup table.\n",
    "test_rec = tab_raw_ar.find_one()\n",
    "diagnosis = test_rec['diagnosis']\n",
    "print(diagnosis)\n",
    "print(\"---------------------- Lookup result below\")\n",
    "print(arabic_lookup[diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The insert_many method inserts in chunks of 1000, but this specifies that we don't want\n",
    "# to start the process until we have this many records to insert.\n",
    "buffer_size = 50000\n",
    "\n",
    "flags_to_insert = []\n",
    "\n",
    "try:\n",
    "    db['full_raw_flags'].drop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tab_raw_flags = db['full_raw_flags']\n",
    "\n",
    "# Insert a dummy record to create the table\n",
    "dummy_record = {\n",
    "    'file_id':None,\n",
    "    'files_sheets_id':None,\n",
    "    'sheet_id':None\n",
    "}\n",
    "\n",
    "for flag in flag_cols:\n",
    "    dummy_record[flag] = None\n",
    "    \n",
    "tab_raw_flags.insert(dummy_record)\n",
    "print(tab_raw_flags.count())\n",
    "tab_raw_flags.delete()\n",
    "\n",
    "\n",
    "# Iterate through the raw records one by one\n",
    "for rec in tab_raw_ar.find():\n",
    "    \n",
    "    # Include foreign keys that allow us to query against the flag table instead of \n",
    "    # joining with the raw data table, which is slow.\n",
    "    flag_record = {\n",
    "        'id':rec['id'],\n",
    "        'file_id':rec['a_file_id'],\n",
    "        'files_sheets_id':rec['a_files_sheets_id'],\n",
    "        'sheet_id':rec['a_sheet_id']\n",
    "    }\n",
    "    \n",
    "    # Initialize each flag_record\n",
    "    for flag in flag_cols:\n",
    "        flag_record[flag] = None\n",
    "        \n",
    "    # Scan the conflict related column for values, but do this before looking at the\n",
    "    # corresponding Arabic values so that we don't overwrite the Arabic value setting.\n",
    "    if rec['conflict_related'] is not None:\n",
    "        if rec['conflict_related'].strip() == 'كبرى' or rec['conflict_related'].strip() =='كبرى':\n",
    "            flag_record['flag_conflict_related'] = 1\n",
    "        elif rec['conflict_related'].strip() == 'لا':\n",
    "            flag_record['flag_conflict_related'] = 0\n",
    "        else:\n",
    "            flag_record['flag_conflict_related'] = None\n",
    "    else:\n",
    "        flag_record['flag_conflict_related'] = None\n",
    "        \n",
    "    # Loop through the variables for each raw data record\n",
    "    for v in variables:\n",
    "        # These are obfuscated PII cols, or the flag columns we're ignoring, so skip them\n",
    "        if 'info_' in v or 'flag_' in v or v == 'id':\n",
    "            continue\n",
    "        \n",
    "        # Get the value in the column\n",
    "        to_lookup = rec[v]\n",
    "        \n",
    "        if to_lookup is None or to_lookup == '.':\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            # We have a legit value, so look it up and grab the flags\n",
    "            try:\n",
    "                # There might be a keyerror on the info_ columns' hashed values, etc.\n",
    "                # I also manually removed some PII from arabic_values, so that might\n",
    "                # cause an occassional mismatch.\n",
    "                arabic_values_rec = arabic_lookup[to_lookup]\n",
    "                for flag in flag_cols:\n",
    "                    # Should be None if not flagged, so just check for existence\n",
    "                    if arabic_values_rec[flag]:\n",
    "                        flag_record[flag] = arabic_values_rec[flag]\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Store the record\n",
    "    flags_to_insert.append(flag_record)\n",
    "\n",
    "    # Check if we need to insert\n",
    "    if len(flags_to_insert) > buffer_size:\n",
    "        tab_raw_flags.insert_many(flags_to_insert)\n",
    "        \n",
    "        # Clear the buffer\n",
    "        flags_to_insert.clear()\n",
    "        \n",
    "# We've been through all raw records so make sure the buffer is clear\n",
    "tab_raw_flags.insert_many(flags_to_insert)\n",
    "flags_to_insert.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ran VACUUM; to try to reduce the DB size..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export full flags table to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can change this query to export a different set of data\n",
    "result = db.query(\"\"\"\n",
    "SELECT  files.id as files_id,\n",
    "        files.year,\n",
    "        files.month,\n",
    "        files.year || '-' || files.month || '-01' AS full_date,\n",
    "        facilities.id AS facility_id,\n",
    "        facilities.facility_parent_id,\n",
    "        facilities.facilityname,\n",
    "        facilities.country,\n",
    "        facilities.governorate,\n",
    "        facilities.district,\n",
    "        facilities.subdistrict,\n",
    "        facilities.facility_type,\n",
    "        full_raw_flags.flag_abdomen,\n",
    "        full_raw_flags.flag_abdominal_pain,\n",
    "        full_raw_flags.flag_allergy,\n",
    "        full_raw_flags.flag_anemia,\n",
    "        full_raw_flags.flag_animal_insect_bite,\n",
    "        full_raw_flags.flag_back,\n",
    "        full_raw_flags.flag_blast,\n",
    "        full_raw_flags.flag_bleed,\n",
    "        full_raw_flags.flag_blunt,\n",
    "        full_raw_flags.flag_burn,\n",
    "        full_raw_flags.flag_cancer,\n",
    "        full_raw_flags.flag_cardiovascular,\n",
    "        full_raw_flags.flag_chest,\n",
    "        full_raw_flags.flag_complication,\n",
    "        full_raw_flags.flag_conflict_related,\n",
    "        full_raw_flags.flag_congenital,\n",
    "        full_raw_flags.flag_constipation,\n",
    "        full_raw_flags.flag_dehydration,\n",
    "        full_raw_flags.flag_dental_complaint,\n",
    "        full_raw_flags.flag_derm,\n",
    "        full_raw_flags.flag_diabetes,\n",
    "        full_raw_flags.flag_diarrhea_dysentery,\n",
    "        full_raw_flags.flag_endocrine,\n",
    "        full_raw_flags.flag_ENT,\n",
    "        full_raw_flags.flag_explosive,\n",
    "        full_raw_flags.flag_eye,\n",
    "        full_raw_flags.flag_facial,\n",
    "        full_raw_flags.flag_fatigue,\n",
    "        full_raw_flags.flag_fever,\n",
    "        full_raw_flags.flag_follow_up,\n",
    "        full_raw_flags.flag_fracture,\n",
    "        full_raw_flags.flag_gi_complaint,\n",
    "        full_raw_flags.flag_growth_delay,\n",
    "        full_raw_flags.flag_gu,\n",
    "        full_raw_flags.flag_gunshot,\n",
    "        full_raw_flags.flag_gyn_women,\n",
    "        full_raw_flags.flag_head,\n",
    "        full_raw_flags.flag_headache,\n",
    "        full_raw_flags.flag_history_of,\n",
    "        full_raw_flags.flag_hyperlipidemia,\n",
    "        full_raw_flags.flag_infection,\n",
    "        full_raw_flags.flag_injury,\n",
    "        full_raw_flags.flag_injury_neuro,\n",
    "        full_raw_flags.flag_liver_dysfunction,\n",
    "        full_raw_flags.flag_lower_extremity,\n",
    "        full_raw_flags.flag_malnutrition,\n",
    "        full_raw_flags.flag_mental_health,\n",
    "        full_raw_flags.flag_musculoskeletal_pain,\n",
    "        full_raw_flags.flag_nausea_vomiting,\n",
    "        full_raw_flags.flag_neck,\n",
    "        full_raw_flags.flag_nerve,\n",
    "        full_raw_flags.flag_neuro_complaint,\n",
    "        full_raw_flags.flag_neurologic,\n",
    "        full_raw_flags.flag_orthopedic,\n",
    "        full_raw_flags.flag_other_infection,\n",
    "        full_raw_flags.flag_pain,\n",
    "        full_raw_flags.flag_pelvic,\n",
    "        full_raw_flags.flag_pregnancy,\n",
    "        full_raw_flags.flag_renal,\n",
    "        full_raw_flags.flag_respiratory,\n",
    "        full_raw_flags.flag_shrapnel,\n",
    "        full_raw_flags.flag_spinal,\n",
    "        full_raw_flags.flag_spine,\n",
    "        full_raw_flags.flag_stab,\n",
    "        full_raw_flags.flag_stroke,\n",
    "        full_raw_flags.flag_suspected,\n",
    "        full_raw_flags.flag_traffic_accident,\n",
    "        full_raw_flags.flag_trauma,\n",
    "        full_raw_flags.flag_upper_extremity,\n",
    "        full_raw_flags.flag_urologic,\n",
    "        full_raw_flags.flag_vascular,\n",
    "        full_raw_flags.flag_wound,\n",
    "        full_raw_flags.flag_comprehensive_injury\n",
    "\n",
    "FROM full_raw_flags\n",
    "JOIN files on files.id = full_raw_flags.file_id\n",
    "JOIN facilities on files.facility_id = facilities.id\n",
    "\n",
    "WHERE files.facility_id IS NOT NULL \n",
    "AND files.month IS NOT NULL\n",
    "AND files.skipped = 0\n",
    "AND files.ignore = 0;\n",
    "\"\"\")\n",
    "\n",
    "# This used to be a part of dataset but was extracted to its own library\n",
    "# https://github.com/pudo/datafreeze\n",
    "freeze(result, format='csv', filename='full_raw_flags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is optional and will generate a copy of the database that will be gigabytes in size.\n",
    "shutil.copy2(new_db_name,'sams_data_phase23_output_2018-04-07.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
