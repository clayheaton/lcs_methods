{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to perform an initial load of all Excel data provided by SAMS into a SQLite database for consolidation, cleaning, and translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulate the file system\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Convert stored string representation of a list to a list\n",
    "import ast\n",
    "\n",
    "# Recurse through a directory tree and return file names with glob\n",
    "import glob\n",
    "\n",
    "# Decode and re-encode mangled Arabic file names\n",
    "import codecs\n",
    "\n",
    "# Connect to a SQLite database in a lazy manner.\n",
    "import dataset\n",
    "\n",
    "# Enables opening and reading of Excel files\n",
    "import openpyxl\n",
    "\n",
    "# Translating variables, sheet names, and workbook names from Arabic\n",
    "# This is NOT free to use.\n",
    "from google.cloud import translate\n",
    "\n",
    "# Set the environment variable for the Google Service Account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\clay\\\\Documents\\\\fxb-lcs-2b24f4f8a73a.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is meant to be run from top to bottom, as a way to reproduce the data. The next cell deletes the existing database so that it can be recreated. Do not run the next cell if you do not want to delete the database. The script takes a long time to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to preserve a copy\n",
    "    shutil.copy2(\"sams_data.sqlite\",\"sams_data.sqlite.bak\")\n",
    "    print(\"Backed up to sams_data.sqlite.bak\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.remove(\"sams_data.sqlite\")\n",
    "    print(\"Removed sams_data.sqlite\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the database and create references to the tables we'll use to import file names and create links to tab names. The idea is that tabs with the same or similar names likely contain data that should be consolidated into the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///sams_data.sqlite\")\n",
    "tab_files = db['files']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create database references for the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of Excel files and print out how many there are to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1138"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(\"data/**/*.xls*\",recursive=True)\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the file names, loading the files with `openpyxl` and creating records for them. Use the Google Translate API to translate Arabic file names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the file names are encoded using the incorrect Codepage. This provides a fixed value in the 'ungarbled' column of the files table. Solved with help from [the question I asked on Stack Overflow](https://stackoverflow.com/questions/45625029/how-do-i-convert-file-names-in-iso8859-6-to-utf-8/45628279#45628279).\n",
    "\n",
    "\n",
    "Run this command in the proper python context to be able to use the API:\n",
    "\n",
    "`gcloud auth application-default login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate_client = translate.Client()\n",
    "target_lang = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in file_list:\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(f,read_only=True)\n",
    "        problem = False\n",
    "        sheets = wb.sheetnames\n",
    "        num_sheets = len(sheets)\n",
    "    except:\n",
    "        print(\"Unable to load\",f)\n",
    "        problem = True\n",
    "        sheets = []\n",
    "        num_sheets = 0\n",
    "        \n",
    "    path = f\n",
    "    filename = f.split(\"\\\\\")[-1]\n",
    "    country = f.split(\"\\\\\")[1]\n",
    "    year = f.split(\"\\\\\")[2]\n",
    "    skipped = False\n",
    "    ignore = False\n",
    "    \n",
    "    ungarbled = None\n",
    "    translation = None\n",
    "    \n",
    "    if ('Ω' in filename or\n",
    "        'π' in filename or\n",
    "        'ó' in filename or\n",
    "        'Θ' in filename):\n",
    "        ungarbled = filename.encode('cp437').decode('cp720')\n",
    "        translation = translate_client.translate(ungarbled,\n",
    "                                                 target_language=target_lang)\n",
    "        translation = translation['translatedText']\n",
    "    \n",
    "    file_rec = {\n",
    "        \"file_name\":filename,\n",
    "        \"ungarbled\":ungarbled,\n",
    "        \"translation\":translation,\n",
    "        \"path\":path,\n",
    "        \"country\":country,\n",
    "        \"year\":year,\n",
    "        \"num_sheets\":num_sheets,\n",
    "        \"sheet_names\":str(sheets),\n",
    "        \"info\":\"\",\n",
    "        \"problem_opening\":problem,\n",
    "        \"skipped\":skipped,\n",
    "        \"ignore\":ignore\n",
    "    }\n",
    "    \n",
    "    # Insert the file record into the database\n",
    "    tab_files.insert(file_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Create db references for the sheets\n",
    "\n",
    "This is needed because the worksheet names likely consolidate down to fewer schema than would be apparent through their current naming scheme. The English names will be used to generate table names.\n",
    "\n",
    "\n",
    "Considerations:\n",
    "\n",
    "- Sheet names need to be translated\n",
    "- Sheets with the same name should share the same schema\n",
    "- Variables for a sheet schema should be attached to sheet names\n",
    "- Aggregate sheets are more difficult to import. Handle them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most of these are reference or aggregate sheets that\n",
    "# will be handled later\n",
    "\n",
    "sheet_names_to_skip = [\n",
    "    \"TOTAL\",\n",
    "    \"Name\",\n",
    "    \"Code\",\n",
    "    \"Sheet\",\n",
    "    \"Monthly\",\n",
    "    \"Injured Info\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_sheets = db['sheets']\n",
    "tab_files_sheets_join = db['files_sheets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of the unique sheet names so that we can translate them and mark some of them to be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_set = set()\n",
    "\n",
    "for rec in tab_files.find():\n",
    "    sheets = ast.literal_eval(rec['sheet_names'])\n",
    "    for s in sheets:\n",
    "        sheet_set.add(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create records for each unique sheet name and insert them into the sheets table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet in sheet_set:\n",
    "    rec = {\"name\":sheet}\n",
    "    translation = translate_client.translate(sheet,\n",
    "                                             target_language=target_lang)\n",
    "    translation = translation['translatedText']\n",
    "    rec[\"translation\"] = translation\n",
    "    rec[\"normalized\"] = \"\"\n",
    "    skip = False\n",
    "\n",
    "    if any(skipname in sheet for skipname in sheet_names_to_skip):\n",
    "        skip = True\n",
    "    rec[\"skip\"] = skip\n",
    "    tab_sheets.insert(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, join the files to the sheets in a join table for future reference. That table will be processed later to provide additional metadata about whether a particular sheet from a file was processed or imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an in-memory reference for the sheets. While this could be done in an earlier step, it's being done later in the event that the join records need to be recreated with additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheet_ref = {}\n",
    "for rec in tab_sheets.find():\n",
    "    sheet_ref[rec['name']] = rec['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete existing join records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_files_sheets_join.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rebuild the records in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_records = []\n",
    "\n",
    "for rec in tab_files.find():\n",
    "    for sheet in ast.literal_eval(rec['sheet_names']):\n",
    "        sheet_id = sheet_ref[sheet]\n",
    "        join_rec = {\n",
    "            \"file_id\":rec['id'],\n",
    "            \"sheet_id\":sheet_id,\n",
    "            \"header_start\":None,\n",
    "            \"header_end\":None,\n",
    "            \"header_values\":None\n",
    "        }\n",
    "        join_records.append(join_rec)\n",
    "\n",
    "# Bulk inserts are faster than individual inserts\n",
    "tab_files_sheets_join.insert_many(join_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this workbook\n",
    "\n",
    "To minimize the chance of accidently deleting data and requiring additional calls to the Google Translate API, this process is going to be cut into a series of notebooks, each of which works with a different database as its primary starting point.\n",
    "\n",
    "\n",
    "### Copy the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sams_data_phase02_template.sqlite'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy2(\"sams_data.sqlite\",\"sams_data_phase02_template.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
