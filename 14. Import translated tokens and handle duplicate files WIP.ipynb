{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manipulate the file system\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Copy dictionaries\n",
    "import copy\n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# Display errors in realtime\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# work with dates\n",
    "import datetime\n",
    "import arrow\n",
    "import time\n",
    "\n",
    "# For scrubbing PII\n",
    "import hashlib\n",
    "\n",
    "# Convert stored string representation of a list to a list\n",
    "import ast\n",
    "\n",
    "# Recurse through a directory tree and return file names with glob\n",
    "import glob\n",
    "\n",
    "# Decode and re-encode mangled Arabic file names\n",
    "import codecs\n",
    "\n",
    "# Connect to a SQLite database in a lazy manner.\n",
    "import sqlalchemy\n",
    "import dataset\n",
    "\n",
    "# Enables opening and reading of Excel files\n",
    "import openpyxl\n",
    "\n",
    "# Import Pandas for easy importing of an excel file\n",
    "import csv\n",
    "\n",
    "# Translating variables, sheet names, and workbook names from Arabic\n",
    "# This is NOT free to use.\n",
    "from google.cloud import translate\n",
    "\n",
    "# Set the environment variable for the Google Service Account\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'C:\\\\Users\\\\clay\\\\Documents\\\\fxb-lcs-2b24f4f8a73a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed template clone sams_data_phase14.sqlite\n",
      "Created database from template: sams_data_phase14.sqlite\n"
     ]
    }
   ],
   "source": [
    "#If there's an existing db for this sheet, delete it\n",
    "#so that we can copy from the template for a fresh start\n",
    "\n",
    "try:\n",
    "    os.remove(\"sams_data_phase14.sqlite\")\n",
    "    print(\"Removed template clone sams_data_phase14.sqlite\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Try to preserve a copy in case there is a problem and it has to be restored\n",
    "    shutil.copy2(\"sams_data_phase14_template.sqlite\",\"sams_data_phase14.sqlite\")\n",
    "    \n",
    "    print(\"Created database from template: sams_data_phase14.sqlite\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Translation file from Houssom\n",
    "\n",
    "translation_file = \"split_arabic_phrases_01-DEC_2017.csv\"\n",
    "data = []\n",
    "with open(translation_file,'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    header = next(csvreader)\n",
    "    header[0] = 'id'\n",
    "    for row in csvreader:\n",
    "        rowdict = zip(header,row)\n",
    "        data.append(dict(rowdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert some values to integers and use None for empty strings to save database space\n",
    "\n",
    "clean_data = []\n",
    "for r in data:\n",
    "    try:\n",
    "        r['id'] = int(r['id'])\n",
    "        r['occurrences'] = int(r['occurrences'])\n",
    "        for key in r.keys():\n",
    "            try:\n",
    "                if r[key].strip() == '':\n",
    "                    r[key] = None\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            del r['Number of Characters']\n",
    "        except:\n",
    "            pass\n",
    "        clean_data.append(r)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///sams_data_phase14.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the arabic_tokens table with the translation data, which includes\n",
    "# all of the original arabic_tokens data along with additional metadata from Excel\n",
    "tab_arabic_tokens = db['arabic_tokens']\n",
    "tab_arabic_tokens.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_arabic_tokens = db['arabic_tokens']\n",
    "tab_arabic_tokens.insert_many(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a review of the data, the best approach here is probably to take the terms that were split out of the original Arabic Values and join them back together, comma separated, into the \"Human Translation\" column of the Arabic Values table. Given that those map back to the raw data, it will give an approximation of how well we can provide a human translated glimpse of the raw source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_arabic_values_tokens = db['arabic_values_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_lookup = {}\n",
    "\n",
    "for record in tab_arabic_values_tokens.find():\n",
    "    if record['arabic_values_id'] not in join_lookup.keys():\n",
    "        join_lookup[record['arabic_values_id']] = set()\n",
    "    join_lookup[record['arabic_values_id']].add(record['token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are errant commas in the import file and this attempts to remove them,\n",
    "# though it is possible is also is removing commas that belong. \n",
    "token_lookup = {}\n",
    "\n",
    "for record in tab_arabic_tokens.find():\n",
    "    translation = record['translation']\n",
    "    \n",
    "    # Nothing to join back\n",
    "    if translation is None:\n",
    "        continue\n",
    "    else:\n",
    "        translation = [x.lower().strip() for x in translation.split(\",\") if x.strip() != '']\n",
    "        \n",
    "    token_lookup[record['id']] = translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_values_to_update = []\n",
    "for key in join_lookup.keys():\n",
    "    update_rec = {'id':key}\n",
    "    all_vals = []\n",
    "    for token_rec in sorted(list(join_lookup[key])):\n",
    "        try:\n",
    "            all_vals += token_lookup[token_rec]\n",
    "        except:\n",
    "            pass\n",
    "    if len(all_vals) == 0:\n",
    "        continue\n",
    "    \n",
    "    hum_trans = \", \".join(all_vals)\n",
    "    update_rec['human_translate'] = hum_trans\n",
    "    ar_values_to_update.append(update_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_arabic_values = db['arabic_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human_translate': 'urinalysis test, cbc, esr, laboratory tests, vitamin b complex, mebeverine, alt',\n",
       " 'id': 167781}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at our update records:\n",
    "ar_values_to_update[1030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for rec in ar_values_to_update:\n",
    "    tab_arabic_values.update(rec,['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this process, I noticed that some `col_none` values represent duplicate files and data that was not deidentified. I'm awaiting word of what steps I should take to handle the errors. \n",
    "\n",
    "Files with ids: 769 and 747 are duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This represents an attempt to detect duplicate files, but due to slight differences\n",
    "# in the files, it doesn't see them as duplicates. Hence, the code was not used, but is here\n",
    "# for future reference.\n",
    "\n",
    "# https://docs.python.org/3/library/filecmp.html\n",
    "# import filecmp\n",
    "# f1 = r\"data\\turkey\\2015\\Feb\\Bab Al Hawa\\Patient log.xlsx\"\n",
    "# f2 = r\"data\\turkey\\2015\\Jan\\Bab Al Hawa DU\\Patient log.xlsx\"\n",
    "\n",
    "# filecmp.cmp(f1,f2,shallow=False)\n",
    "\n",
    "# import os\n",
    "# os.stat(f1)\n",
    "# os.stat(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove data from file with id 747 because it is duplicate of 769. The data for 769 was recorded for January and the data for 747 was recorded for February of the same year, so it makes sense to get rid of the more recent duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the files table to mark the file as skipped\n",
    "update_rec = {'id':747, 'ignore':1, 'info':\"duplicate of file id 769\"}\n",
    "tab_files = db['files']\n",
    "tab_files.update(update_rec,['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.persistence.util.ResultIter at 0x150a0281f98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are other 'dangling' pieces of data from this duplicate file, but they only should \n",
    "# affect the raw data tables. The rest is metadata.\n",
    "db.query(\"\"\"\n",
    "DELETE FROM full_raw_scrubbed WHERE a_file_id = 747;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataset.persistence.util.ResultIter at 0x150a0270390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.query(\"\"\"\n",
    "DELETE FROM full_raw_english WHERE a_file_id = 747;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next: obscure the `col_none` values in the full data for those with PII - they weren't caught the first time.\n",
    "\n",
    "# Don't save in a source code repository\n",
    "salt = 'REDACTED'.encode()\n",
    "\n",
    "# Arabic \n",
    "tab_raw_ar = db['full_raw_scrubbed']\n",
    "update_records_ar = []\n",
    "\n",
    "for rec in tab_raw_ar.find(a_file_id=769):\n",
    "    new_rec = {'id':rec['id'], 'col_none':rec['col_none']}\n",
    "    \n",
    "    h = hashlib.sha256()\n",
    "    h.update(new_rec['col_none'].encode())\n",
    "    h.update(salt)\n",
    "    new_rec['col_none'] = h.hexdigest()\n",
    "    update_records_ar.append(new_rec)\n",
    "    \n",
    "for update_rec in update_records_ar:\n",
    "    tab_raw_ar.update(update_rec,['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have to apply the hash values from the Arabic table to the English table for consistency\n",
    "tab_raw_en = db['full_raw_english']\n",
    "\n",
    "for update_rec in update_records_ar:\n",
    "    tab_raw_en.update(update_rec, ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sams_data_phase15_template.sqlite'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the database over as the template for the next file.\n",
    "# This Notebook did not include manual editing of the data.\n",
    "\n",
    "# Do not rerun this cell!\n",
    "shutil.copy2('sams_data_phase14.sqlite','sams_data_phase15_template.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
